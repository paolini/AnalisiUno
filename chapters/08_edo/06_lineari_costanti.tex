\section{equazioni lineari di ordine $n$ a coefficienti costanti}

Una equazione differenziale ordinaria di ordine $n$ a coefficienti costanti è una equazione del tipo:
\mymark{***}
\begin{equation}\label{eq:edo_lineare_omogenea_coeff_costanti_non_omogenea}
  a_n\cdot u^{(n)}(x) + a_{n-1} \cdot u^{(n-1)}(x) + \dots + a_1 \cdot u'(x) + a_0 \cdot u(x) = g(x) 
%  \sum_{k=0}^n a_k \cdot u^{(k)}(x) = g(x)
\end{equation}
con $a_0, a_1, \dots a_n \in \RR$ (sono i coefficienti costanti) $a_n\neq 0$ e $g(x)$ una funzione continua
(se fosse $a_n=0$ non è un problema: abbiamo una equazione di ordine inferiore).
Tale equazione può essere scritta in forma normale e rientra 
nella casistica generale che abbiamo considerato nel capitolo precedente.
In particolare 
il teorema~\ref{th:edo_lineare_ordine_n}
ci dice che lo spazio delle soluzioni è uno spazio lineare affine di dimensione $n$.

Se $g=0$ diremo che l'equazione è lineare omogenea (sempre a coefficienti costanti)
\index{equazione!differenziale!lineare!a coefficienti costanti}
\begin{equation}\label{eq:edo_lineare_omogenea_coeff_costanti}
  a_n \cdot u^{(n)}(x) + a_{n-1} \cdot u^{(n-1)}(x) + \dots + a_1 \cdot u'(x) + a_0 \cdot u(x) = 0 
%   \sum_{k=0}^n a_k \cdot u^{(k)}(x) = 0.
\end{equation}
In questo caso l'insieme delle soluzioni è uno spazio vettoriale di dimensione $n$.
Osserviamo che il teorema di esistenza e unicità globale garantisce che le soluzioni 
dell'equazione differenziale lineare a coefficienti costanti siano funzioni di 
classe $C^n$ definite su tutto $\RR$. 
Ma i coefficienti costanti sono di classe $C^\infty$ dunque la maggiore regolarità delle soluzioni
(teorema~\ref{th:maggiore_regolarita}) 
ci dice, in questo caso, che le soluzioni dovranno essere funzioni di classe $C^\infty$.

Per motivi puramente algebrici sarà utile considerare le funzioni a valori complessi.
Ricordiamo che se $u(x)$ è una funzione a valori complessi,
allora si può scrivere $u(x) = f(x) + i g(x)$ con $f$ e $g$ funzioni a valori reali.
Si avrà allora $u'(x) = f'(x) + ig'(x)$.
Denotiamo con $D \colon C^\infty(\RR, \CC)\to C^\infty(\RR, \CC)$ l'operatore derivata: $D u = u'$.

Osserviamo esplicitamente che anche quando $\lambda \in \CC$ risulta,
come nel caso $\lambda\in \RR$
\begin{equation}\label{eq:499375}
  D e^{\lambda x} = \lambda e^{\lambda x}.
\end{equation}
Si potrebbe fare la verifica diretta riconducendosi alle funzioni $\sin$ e $\cos$
tramite le formule di Eulero ma questa
è in realtà una proprietà fondamentale dell'esponenziale complesso
che discende direttamente dal teorema~\ref{th:exp_complesso} (ci basta $h,x\in \RR$ ma in realtà 
questo è vero anche se $h,x\in \CC$):
\[
 \lim_{h\to 0} \frac{e^{\lambda(x+h)}-e^{\lambda x}}{h}
 = \lim_{h\to 0} e^{\lambda x}\frac{e^{\lambda h}-1}{h}
 = \lambda e^{\lambda x} \lim_{h\to 0}\frac{e^{\lambda h}-1}{\lambda h}
 = \lambda e^{\lambda x}.
\]

Se $P\in \RR[x]$ è un polinomio di grado $n$
possiamo valutare $P$ sull'operatore
derivata $D\colon C^\infty(\RR,\CC)\to C^\infty(\RR,\CC)$.
Questo ha senso perché sullo spazio $C^\infty(\RR,\CC)$
sono definite le operazioni di spazio vettoriale reale
(somma e prodotto per scalare) in più l'operazione
di moltiplicazione $D\cdot D$ e quindi la potenza
$D^k$ va intesa come composizione dell'operatore
lineare con se stesso.
In pratica l'operatore
$P(D)\colon C^\infty(\RR,\CC) \to C^\infty(\RR,\CC)$
risulta definito come segue
\[
  P(D)[u] = \sum_{k=0}^n a_k D^k u = \sum_{k=0}^n a_k u^{(k)}.
\]
Si osservi che $D^0 = I$ è l'identità ma tenendo valida
la convenzione $D^0 = 1$ ci capiterà di scrivere
$\lambda$ al posto di $\lambda I$ quando $\lambda\in \RR$.

L'equazione lineare omogenea a coefficienti costanti~\eqref{eq:edo_lineare_omogenea_coeff_costanti}
si può scrivere più espressivamente nella forma
\[
  P(D)[u] = 0
\]
Visto che $a_n\neq 0$ si avrà $\deg P = n$.

\subsection{equazioni omogenee}

\begin{theorem}
\mymark{***}
Sia $P$ un polinomio e sia $\lambda\in \CC$ una radice di $P$ con molteplicità $m$.
Allora se $p(x)$ è un polinomio (a coefficienti complessi) di grado inferiore a $m$,
la funzione $u(x) = p(x) e^{\lambda x}$ è soluzione (complessa) dell'equazione differenziale
\[
   P(D) [u] = 0.
\]
\end{theorem}
%
\begin{proof}
\mymark{***}
Se il polinomio $P(t)$ ha una radice $\lambda$ con molteplicità $m$ significa che $P(t)$ è divisibile per $(t-\lambda)^m$ cioè esiste un polinomio $R(t)$ tale che
\[
  P(t) = R(t)\cdot (t-\lambda)^m.
\]

Ma allora possiamo decomporre anche l'equazione differenziale:
\[
 P(D) [u] = R(D)[(D - \lambda)^m u].
\]
Se $u(x) = p(x) e^{\lambda x}$ si ha,
utilizzando~\eqref{eq:499375},
\begin{align*}
  (D-\lambda) u(x) &= Du(x) -\lambda u(x) =
  p'(x) e^{\lambda x} + p(x) \lambda e^{\lambda x} - \lambda p(x) e^{\lambda x} \\
  &= p'(x) e^{\lambda x},
\end{align*}
da cui
\[
  (D-\lambda)^m u(x) = p^{(m)}(x) \cdot e^{\lambda x}.
\]
Visto che la derivata di un polinomio è un polinomio di grado inferiore,
se il polinomio $p$ ha grado inferiore a $m$ risulta che $p^{(m)}=0$.
Dunque, come volevamo dimostrare,
\[
 P(D) [u] = R(D) [(D-\lambda)^m u] = R(D) [0] = 0.
\]
\end{proof}

\begin{theorem}[indipendenza delle soluzioni fondamentali complesse]
\mymark{***}
Il sottoinsieme $\B$ di $C^\infty(\RR, \CC)$ dato dalle funzioni $u(x)$ della forma
\[
   u(x) = x^m e^{\lambda x}
\]
al variare di $m\in \NN$, e $\lambda \in \CC$ è un sottoinsieme linearmente indipendente
dello spazio vettoriale $C^\infty(\RR,\CC)$ sul campo $\CC$.
\end{theorem}
%
\begin{proof}
Si tratta di mostrare che se una combinazione lineare finita di tali funzioni
è identicamente nulla, allora tutti i coefficienti sono nulli.

Supponiamo per assurdo che esistano
$u_1, \dots, u_N\in \B$
\[
  u_k(x) = x^{m_k} e^{\lambda_k x}, \qquad k=1, \dots, N
\]
tali che
\[
\sum_{k=1}^N c_k x^{m_k} e^{\lambda_k x} = 0
  \qquad \forall x \in \RR
\]
per una qualche scelta di coefficienti $c_k \in \CC$ non tutti nulli.
Sommando assieme i monomi che moltiplicano gli esponenziali con lo stesso
coefficiente, potremo riscrivere la relazione precedente nella forma:
\begin{equation}\label{eq:4656978}
  \sum_{j=1}^M P_j(x) e^{\lambda_j x} = 0 \qquad \forall x\in \RR
\end{equation}
con i coefficienti $\lambda_j$ tutti diversi tra loro e con
$P_j$ polinomi non nulli.

Abbiamo già osservato che $(D-\lambda)P(x)e^{\lambda x}= P'(x)e^{\lambda x}$
mentre se $\lambda\neq \mu$ si ha $(D-\lambda)P(x) e^{\mu x} = Q(x) e^{\mu x}$
con $Q$ polinomio dello stesso grado di $P$ in quanto:
\[
(D-\lambda) P(x) e^{\mu x} = (P'(x) + (\mu-\lambda) P(x)) e^{\mu x}.  
\]
Posto $d_j = \deg P_j$ applichiamo all'equazione~\eqref{eq:4656978}
l'operatore $(D-\lambda_1)^{d_1}$.
Quello che si ottiene è:
\begin{equation}\label{eq:4656979}
  k_1 e^{\lambda_1 x} + \sum_{j=2}^M Q_j(x) e^{\lambda_j x} = 0 \qquad \forall x\in \RR
\end{equation}
dove $k_1$ è la derivata $d_1$-esima del polinomio $P_1$.
Visto che $P_1$ è un polinomio di grado $d_1$ la sua derivata
$d_1$-esima è un polinomio di grado $0$ non nullo,
dunque è una costante $k_1 \neq 0$.
I polinomi $Q_j$ hanno invece lo stesso grado dei $P_j$ perché abbiamo visto
che se $\lambda\neq \mu$ il grado del polinomio non cambia.

Ora applichiamo in sequenza all'equazione~\eqref{eq:4656979} gli operatori
$(D-\lambda_j)^{d_j+1}$ per $j=2,\dots, M$.
Applicando tali operatori il coefficiente $k_1$ cambierà, ma rimarrà comunque
diverso da zero (chiamiamolo $k\neq 0$) mentre tutti gli altri polinomi
verranno annullati, perché i rispettivi polinomi vengono derivati una volta in
più del loro grado. Si otterrà quindi:
\[
  k e^{\lambda_1 x} = 0 \qquad \forall x\in \RR.
\]
Ma questo è assurdo perché per $x=0$ si ottiene $k=0$, che abbiamo escluso.
\end{proof}

I risultati precedenti ci permettono di determinare tutte le soluzioni complesse delle equazioni 
lineari omogenee a coefficienti costanti. Infatti per ogni radice complessa $\lambda$ del polinomio 
associato $P(t) = a_n t^n + \dots + a_1 t + a_0$ se $m$ è la molteplicità di $\lambda$ 
le funzioni 
\[
  e^{\lambda x}, x e^{\lambda x}, \dots, x^{m-1} e^{\lambda x}  
\]
sono $m$ soluzioni indipendenti, e al variare di $\lambda$ tra le radici di $P$ otteniamo 
quindi in totale $n$ soluzioni indipendenti. Visto che 
sappiamo che lo spazio delle soluzioni ha dimensione $n$ le combinazioni lineari di queste 
soluzioni ci danno l'insieme di tutte le soluzioni.

Se tutte le radici del polinomio $P$ sono reali, le funzioni $e^{\lambda x}$ sono effettivamente 
soluzioni reali e, grazie al seguente lemma, possiamo affermare che tutte le soluzioni reali 
si ottengono facendo combinazioni lineari a coefficienti reali di queste soluzioni.

\begin{lemma}\label{lm:combinazioni_reali}
Siano $u_1(x), \dots, u_n(x)$ funzioni indipendenti a valori reali
e sia 
\[
  h(x) = c_1 \cdot u_1(x) + \dots + c_n \cdot u_n(x)
\]
una combinazione lineare a coefficienti complessi.
Allora $h(x)$ ha valori reali se e solo se i coefficienti $c_1, \dots, c_n$ 
sono anch'essi reali
\end{lemma}
%
\begin{proof}
E' ovvio che se i coefficienti $c_1,\dots,c_n$ sono reali e le funzioni $u_1, \dots, u_n$ sono reali,
facendone una combinazione lineare si ottiene una funzione reale.

Per dimostrare il viceversa è invece necessario sapere che le funzioni sono anche indipendenti. Infatti 
se $h(x)$ è reale possiamo scrivere $\bar h(x) - h(x) = 0$ cioè, visto che il coniugio
attraversa la somma e il prodotto e visto che $\overline u_k(x) = u_k(x)$, si ha 
\[
 (\bar c_1 - c_1) u_1(x) + \dots + (\bar c_n-c_n) u_n(x)=0.
\]
Ma la combinazione lineare nulla si può ottenere solo con tutti i coefficienti nulli 
(è la definizione di indipendenza lineare) dunque $\bar c_k = c_k$ per ogni $k$ ovvero 
i coefficienti sono reali.
\end{proof}

\begin{example}
Risolvere l'equazione
\[
  u^{(5)} - 2 u''' + u' = 0.  
\]
\end{example}
%
\begin{proof}[Svolgimento.]
Il polinomio associato all'equazione è $P(\lambda) = \lambda^5 - 2\lambda^3 + \lambda$ 
e può essere fattorizzato come segue:
\[
  \lambda^5 - 2\lambda^3 + \lambda
  = \lambda(\lambda^4-2\lambda^2+1)
  = \lambda(\lambda^2-1)^2
  = \lambda(\lambda-1)^2(\lambda+1)^2.
\]
Dunque abbiamo tre radici distinte $\lambda_1=0$, $\lambda_2=1$, $\lambda_3=-1$ con molteplicità 
$m_1=1$, $m_2=2$, $m_3=2$.
Una base di soluzioni è quindi data dalle cinque funzioni indipendenti
\[
 1, e^x, xe^x, e^{-x}, xe^{-x}.
\]
Tutte le soluzioni si scrivono quindi nella forma
\[
  u(x) = c_1 + (c_2 + c_3 x)e^x + (c_4 + c_5 x) e^{-x}  
\]
con $c_1, \dots, c_5$ costanti arbitrarie.
\end{proof}

Se però le radici del polinomio sono complesse noi siamo comunque interessati ad identificare 
le soluzini reali.
Per fare ciò osserviamo che se $\lambda = \alpha + i \beta $ è una radice complessa di un polinomio $P$ a coefficienti 
reali allora anche $\bar \lambda =\alpha - i \beta$ è una radice complessa con la stessa molteplicità di $\lambda$
(si veda la dimostrazione del teorema~\ref{th:fattorizzazione_polinomio_reale}). 
Ma allora osserviamo che lo spazio generato dalle combinazioni lineari a coefficienti complessi
di due soluzioni corrispondenti ai due coefficienti coniugati:
\begin{align*}
  c_1 x^k e^{\lambda x} + c_2  x^k e^{\bar\lambda x}
  &= c_1 x^k e^{\alpha x+i \beta x} + c_2 x^k e^{\alpha x - i \beta x}\\
  &= x^k e^{\alpha x}\enclose{(c_1+c_2) \cos \beta x + i (c_1-c_2) \sin \beta x}
\end{align*}
è anche una combinazione lineare a coefficienti complessi delle due funzioni reali:
\begin{equation}\label{eq:489571798}
  x^k e^{\alpha x} \cos(\beta x), \qquad
  x^k e^{\alpha x} \sin(\beta x)
\end{equation}
Dunque, per il lemma~\ref{lm:combinazioni_reali}, 
le soluzioni reali possono essere scritte come combinazioni lineari a coefficienti 
costanti di queste ultime funzioni.

In pratica il metodo per determinare le soluzioni è quindi molto semplice. 
Ogni radice reale $\lambda$ del polinomio $P$ associato all'equazione dà origine 
alle seguenti soluzioni indipendenti:
\[
  e^{\lambda x}, x e^{\lambda x}, \dots, x^{m-1} e^{\lambda x}
\]
dove $m$ è la molteplicità di $\lambda$ come radice del polinomio.
Per ogni coppia di radici complesse $\lambda = \alpha \pm i \beta$ con molteplicità $m$ 
avremo invece le seguenti soluzioni indipendenti:
\begin{gather*}
  e^{\alpha x}\cos(\beta x), x e^{\alpha x}\cos(\beta x), \dots x^{m-1} e^{\alpha x}\cos(\beta x),\\
  e^{\alpha x}\sin(\beta x), x e^{\alpha x}\sin(\beta x), \dots x^{m-1} e^{\alpha x}\sin(\beta x).
\end{gather*}
In totale abbiamo quindi sempre $n=\deg P$ soluzioni indipendenti.

\begin{example}
Si determinino tutte le soluzioni dell'equazione differenziale
\[
  u^{(5)}(x) + 2 u'''(x) + u'(x) = 0.
\]
\end{example}
%
\begin{proof}[Svolgimento]
L'equazione può essere scritta nella forma
\[
  P(D) [u] = 0
\]
con $P(\lambda) = \lambda^5 + 2 \lambda^3+\lambda$. 
Possiamo fattorizzare il polinomio $P$ nel campo complesso:
\[
\lambda^5 + 2\lambda^3 +\lambda = \lambda(\lambda^2+1)^2 = \lambda (\lambda+i)^2(\lambda-i)^2.
\]
Il polinomio ha una radice $\lambda_0=0$ con molteplicità uno e due radici complesse coniugate 
$\lambda_1 = i$, $\lambda_2=-i$ con molteplicità due.
Risulta quindi che le seguenti funzioni devono essere soluzioni reali indipendenti dell'equazione:
\begin{equation}\label{eq:4995566}
 1, \qquad
 \cos x, \qquad
 x \cos x, \qquad
 \sin x, \qquad
 x \sin x
\end{equation}
Ogni soluzione reale si potrà dunque scrivere nella forma:
\begin{equation}\label{eq:499355456}
  u(x) = c_1 + c_2 \cos x + c_3 \sin x + c_4 x \cos x + c_5 x \sin x
\end{equation}
con $c_1, c_2, \dots, c_5 \in \CC$ opportuni coefficienti reali.
\end{proof}

% Formalizzando in generale
% il metodo utilizzato nell'esercizio precedente possiamo
% dare il seguente enunciato.
% 
% \begin{theorem}[soluzioni dell'equazione lineare omogenea a coefficienti costanti]
% \mymark{**}%
% Se $P(t)$ è un polinomio a coefficienti reali.
% Ogni soluzione
% $u\colon \RR\to \RR$ dell'equazione differenziale
% \[
%   P(D) [u] = 0
% \]
% si scrive nella forma
% \begin{equation}
% \label{eq:4725549774}
%   u(x) = \sum_{k=1}^N \sum_{j=1}^{n_k} c_{kj} x^j e^{\lambda_k x}
%         + \sum_{k=1}^M \sum_{j=1}^{m_l} x^j e^{\alpha_k x} (a_{kj} \cos(\beta_k x) + b_{kj}\sin(\beta_k x))
% \end{equation}
% dove $\lambda_1, \dots, \lambda_N$ sono le radici reali del polinomio $P(t)$, $n_1, \dots, n_N$ sono le rispettive molteplicità,
% $\alpha_k \pm i \beta_k$ sono le radici complesse coniugate (non reali) del polinomio $P$ con $k=1,\dots, M$ ognuna con molteplicità $m_k$ e infine $c_{kj}, a_{kj}, b_{kj}\in \RR$ sono costanti arbitrarie.
% \end{theorem}
% %
% \begin{proof}
% Sappiamo che le funzioni complesse
% \begin{equation}\label{eq:739647}
%   x^j e^{\lambda_k x}, \qquad
%   x^j e^{(\alpha_k+i\beta_k)x}
% \end{equation}
% sono soluzioni dell'equazione se $j$ è inferiore alla molteplicità della corrispondente radice reale $\lambda_k$ o complessa $\alpha_k + i \beta_k$.
% Dunque
% \[
%   u_{k,j}(x) = x^j e^{\lambda_k x}
% \]
% è soluzione se $j<n_k$.
% Per le radici complesse applichiamo la formula di Eulero:
% \begin{align}\label{eq:49544467}
% x^j e^{\alpha_k x}\cos (\beta_k x) &=
%   \frac 1 2 x^j e^{(\alpha_k +i \beta_k)x}
%   + \frac 1 2 x^j e^{(\alpha_k - i \beta_k)x}\\
% x^j e^{\alpha_k x}\sin (\beta_k x) &=
%     \frac 1 {2i} x^j e^{(\alpha_k +i \beta_k)x}
%     - \frac 1 {2i} x^j e^{(\alpha_k - i \beta_k)x}
% \end{align}
% dunque anche le funzioni
% \[
%   v_{kj}(x) = x^j e^{\alpha_k x}\cos x, \qquad
%   w_{kj}(x) = x^j e^{\alpha_k x}\sin x
% \]
% essendo combinazione lineare di soluzioni, sono anch'esse
% soluzioni.
% Queste soluzioni sono inoltre indipendenti in quanto
% le funzioni in \eqref{eq:739647} lo sono (per il teorema precedente) e possono essere ricondotte alle \eqref{eq:49544467}
% tramite la formula di Eulero.
% 
% Abbiamo dunque trovato un insieme formato da
% \[
%   n = \sum_{k=1}^N n_k + 2 \sum_{k=1}^M m_k
% \]
% soluzioni reali indipendenti. Il numero $n$ è pari alla somma delle molteplicità delle radici 
% (reali e complesse) del polinomio $P$ e dunque, per il teorema fondamentale dell'algebra, 
% coincide con il grado di $P$. Sappiamo allora che lo spazio delle soluzioni di $P(D)[u]=0$ ha 
% dimensione $n$ e quindi abbiamo trovato una base di tutte le soluzioni reali. 
% Significa che ogni soluzione si scrive nella forma~\eqref{eq:4725549774}.
% \end{proof}
% 
\begin{remark}[fase delle soluzioni oscillanti]
Quando si vanno a scrivere le combinazioni lineari delle funzioni 
\eqref{eq:489571798}
può essere utile osservare
che ogni combinazione lineare di seni e coseni
con la stessa pulsazione $\beta$ è una singola onda
eventualmente sfasata di un angolo $\theta$:
\[
  a \cos (\beta x) + b \sin (\beta x)
  =
  \rho \cos(\beta x - \theta)
\]
dove $\rho = \abs{a+ib} = \sqrt{a^2+b^2}$ e 
$\theta=\arg(a+ib)$. 
Infatti
posto $a=\rho\cos \theta$, $b=\rho \sin \theta$ si ha,
grazie alla formula di addizione:
\[
   a \cos (\beta x) + b \sin (\beta x)
   = \rho\enclose{\cos \theta \cos(\beta x) + \sin \theta \sin (\beta x)}
   = \rho \cos(\beta x - \theta).
\]
\end{remark}

\subsection{equazioni lineari non omogenee}

Il teorema~\ref{th:edo_lineare_ordine_n}
ci dice che le soluzioni di una 
 equazione lineare non omogenea (non importa se a coefficienti costanti o meno)
\[
  L[u]=g
\]
si scrivono tutte nella forma $u = u_* + \ker L$ dove 
$u_*$ è una qualunque soluzione fissata della non omogenea 
e $\ker L$ è l'insieme delle soluzioni dell'equazione omogenea 
associata 
\[
  L[u] =0. 
\]
Allora, supponendo di sapere risolvere l'equazione omogenea 
(cosa che sappiamo fare se i coefficienti sono costanti),
per trovare tutte le soluzioni della non omogenea sarà sufficiente 
trovare una sola soluzione particolare $u_*$.

Nel seguito vedremo due metodi per determinare una soluzione particolare 
quando abbiamo una equazione non omogenea a coefficienti costanti. 
Il primo metodo (di similarità) si applica quando il termine noto $g(x)$ assume una forma specifica,
il secondo metodo (della variazione delle costanti) si applica in generale, ma potrebbe risultare 
più laborioso del primo.

\begin{theorem}[metodo di similarità]
\mymark{***}
Consideriamo l'equazione non omogenea complessa
\begin{equation}\label{eq:3976734956}
 P(D) [u] = q(x) e^{\lambda x}.
\end{equation}
con $P$ e $q$ polinomi a coefficienti complessi.
Sia $m$ la molteplicità di $\lambda$ come radice di $P$ ($m=0$ se $\lambda$ non è radice di $P$).
Allora una soluzione particolare di~\eqref{eq:3976734956} può essere scritta nella forma
\begin{equation}
\label{eq:945396}
u_*(x) = p(x) x^m e^{\lambda x}
\end{equation}
con $p$ un polinomio (incognito) di grado uguale al grado di $q$.
\end{theorem}
%
\begin{proof}
Se $u_*(x) = x^m p(x) e^{\lambda x}$ vogliamo capire come agisce l'operatore $P(D)$ su $u_*$, in particolare
dato un qualunque polinomio $q$ vogliamo capire
se esiste un polinomio $p$, dello stesso grado di $q$, tale che applicando $P(D)$ ad $u_*(x)$ si ottiene $q(x) e^{\lambda x}$.

Fattorizziamo il polinomio $P(z)$:
\[
  P(z) = (z-\lambda_1)^{m_1}\dots (z-\lambda_n)^{m_n}
\]
dove $\lambda_1, \dots, \lambda_n\in \CC$ sono le radici distinte di $P(z)$ e $m_1,\dots,m_n\in \NN$ sono le relative molteplicità. Dunque l'operatore $P(D)$ può essere fattorizzato nella forma corrispondente:
\[
   P(D) = (D-\lambda_1)^{m_1} \dots (D-\lambda_n)^{m_n}.
\]

Chiamiamo $V^n_m$ lo spazio vettoriale dei polinomi della forma $x^m p(x)$ con $\deg p<n$. 
Una base di $V^n_m$ sono i monomi $x^m, x^{m+1}, \dots, x^{m+n-1}$ e dunque $V^n_m$ è uno 
spazio vettoriale di dimensione $n$.

La dimostrazione del teorema si basa sull'ossevazione di come agisce l'operatore $(D-\lambda_k)^{m_k}$ 
sulle funzioni del tipo $u(x) = p(x) e^{\lambda x}$ con $p\in V^n_m$. In generale se $p$ è un 
polinomio si ha
\[
 (D-\lambda_k) [p(x)\cdot e^{\lambda x}]
 = \Enclose{(\lambda -\lambda_k)p(x) + p'(x)} \cdot e^{\lambda x}.
\]
Dobbiamo allora distinguere due casi. Se $\lambda\neq \lambda_k$ allora la funzione $p(x) e^{\lambda x}$ viene trasformata nella funzione $q(x) e^{\lambda x}$ dove $q$ è un polinomio dello stesso grado di $p$, in quanto il termine di grado massimo di $p$ viene moltiplicato per $\lambda-\lambda_k$ mentre il polinomio $p'$ ha il grado inferiore al grado di $p$ e quindi non influenza il termine di grado massimo. Se chiamiamo $T_k\colon V^n_0 \to V^n_0$ l'operatore lineare che manda il polinomio $p(x)$ nel polinomio $q(x)=(\lambda-\lambda_k) p(x) + p'(x)$ osserviamo che $T_k$ è iniettivo in quanto, visto che $T_k$ preserva il grado del polinomio, l'unico polinomio che può andare a zero è il polinomio nullo. Dunque, per il teorema del rango, $T_k$ è bigettivo e di conseguenza $T_k^{m_k} \colon V^n_0 \to V^n_0$ è bigettivo.

Se invece $\lambda = \lambda_k$ osserviamo ora che l'operatore $T_k$ 
non è altro che la derivata: $q(x) = p'(x)$. 
E se $m=m_k$ è la molteplicità di $\lambda$ come radice di $P$, 
l'operatore $T_k^{m_k}$ non è altro che la derivata $m$-esima. 
Tale operatore non è invertibile su $V^n_0$ in quanto manda a zero tutti 
i polinomi di grado inferiore a $m$. Ma se partiamo da un polinomio in $V^n_m$ 
allora l'operatore $T_k^m\colon V^n_m \to V^n_0$ diventa invertibile: 
infatti in $V^n_m$ ci sono polinomi della forma 
$a_m x^m + a_{m+1}x^{m+1} + \dots + a_{n+m}x^{n+m}$
e facendone la derivata $m$-esima si può ottenere il polinomio nullo solamente 
se tutti i coefficienti sono nulli.
Risulta quindi che, se $\lambda = \lambda_k$, 
l'operatore $T_k^m\colon V^n_m \to V^n_0$ è invertibile.

Siamo arrivati alla conclusione. Se prendiamo un polinomio $p(x)$ in $V^n_m$ e applichiamo $P(D)$ 
alla corrispondente funzione $u(x) =p(x) e^{\lambda x}$
possiamo applicare sequenzialmente gli operatori $(D-\lambda_k)^{m_k}$. 
Questi agiscono sulla funzione $u$ modificando il polinomio $p$.
Se $\lambda$ è radice di $P$ al primo passaggio applichiamo l'operatore $(D-\lambda_k)^{m_k}$ 
con $\lambda_k=\lambda$ e $m_k=m$ cosicché il polinomio $p\in V^n_m$ viene trasformato, 
in maniera biunivoca, in un polinomio di $V^n_0$. 
Dopodiché applico tutti gli altri operatori $(D-\lambda_k)^{m_k}$ con $\lambda_k \neq \lambda$ 
trasformando il polinomio di $V^n_0$ in un altro polinomio di $V^n_0$ ma sempre in maniera biunivoca. 
Ne risulta che alla fine ottengo una mappa biunivoca tra $V^n_m \to V^n_0$ e qualunque sia 
$q\in V^n_0$ sappiamo esistere un $u\in V^n_m$ che viene mandato in $q$.
\end{proof}

\begin{remark}
Si noti che nella dimostrazione precedente abbiamo investigato la
struttura dell'operatore differenziale $D$ a coefficienti
costanti.
Quello che abbiamo fatto è in realtà il processo di
decomposizione di Jordan di un operatore lineare.
L'operatore $D$
ha come autovettori le funzioni $e^{\lambda_k x}$ con $\lambda_k$
radici del polinomio $P$.
Si osserva però che le funzioni della forma
$x^j e^{\lambda_k x}$ sono \emph{autovettori generalizzati}
in quanto iterando l'operatore $D-\lambda_k$ vengono mandati in un autovettore
e sono quindi una base dell'autospazio relativo all'autovettore
$\lambda_k$.

La base formata dalle funzione della forma $\frac{x^j}{j!} e^{\lambda_j x}$ è
una \emph{base a ventaglio} e la matrice che rappresenta l'operatore
$D$ risulta essere una matrice a blocchi nella forma di Jordan: gli
autovalori stanno sulla diagonale e una fila di $1$ si trova al di sopra della
diagonale.

Il nucleo dell'operatore $P(D)$ si decompone come somma
diretta dei nuclei degli operatori $(D-\lambda_k)^{m_k}$.
\end{remark}

\begin{definition}[wronksiano]
Siano $u_1, \dots, u_n$ funzioni di classe $C^{n-1}$.
La matrice
\begin{equation}\label{eq:wronksiano}
  W(x) =
    \begin{pmatrix}
    u_1(x) & u_2(x) & \dots & u_n(x) \\
    u_1'(x) & u_2'(x) & \dots & u_n'(x) \\
    \vdots & & & \vdots\\
    u_1^{(n-1)}(x) & u_2^{(n-1)}(x) & \dots & u_n^{(n-1)}(x)
    \end{pmatrix}
\end{equation}
è chiamata \emph{matrice wronksiana}.
\mymargin{matrice e}%
\index{matrice e}%
\index{matrice!wronksiana}%
\index{wronksiano}%
Il determinante di tale matrice
$w(x) = \det W(x)$ è chiamato
\emph{determinante wronksiano}%
\mymargin{determinante wronksiano}\index{determinante!wronksiano}.
\end{definition}

\begin{theorem}%
\label{th:wronksiano}%
Sia $I\subset \RR$ un intervallo aperto non vuoto e siano $a_j\in C^0(I,\CC)$.
Siano $u_1, \dots, u_n$ funzioni di classe $C^n$ soluzioni
dell'equazione lineare omogenea in forma normale:
\begin{equation}\label{eq:092784}
  u^{(n)} = \sum_{j=1}^n a_j(x) u^{(j-1)}(x).
\end{equation}
Sia $W(x)$ la matrice wronksiana~\eqref{eq:wronksiano}.
Allora le seguenti proprietà sono equivalenti:
\begin{enumerate}
\item \label{item:4736201} le funzioni $u_1,\dots, u_n$ sono linearmente indipendenti;
\item \label{item:4736202} per ogni $x\in I$ si ha $\det W(x)\neq 0$;
\item \label{item:4736203} esiste $x\in I$ tale che $\det W(x)\neq 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
Ovviamente \ref{item:4736202} implica \ref{item:4736203}.

Dimostriamo che \ref{item:4736203} implica \ref{item:4736201}.
Passando alla implicazione contropositiva dobbiamo mostrare che
se $u_1,\dots, u_n$ sono linearmente dipendenti allora per ogni $x\in I$ si ha $\det W(x)=0$. Ma se le funzioni sono dipendenti significa che esiste $\vec \lambda \neq 0$ tale che
\begin{equation}\label{eq:3675893}
\sum_{k=1}^n \lambda_k u_k(x) = 0\qquad \forall x \in I.
\end{equation}
Derivando si ottiene, per ogni $j=0,\dots, n-1$:
\[
\sum_{k=1}^n \lambda_k u_k^{(j)}(x) = 0\qquad \forall x \in I
\]
e quindi
\[
  W(x) \, \vec \lambda = 0 \qquad \forall x\in I.
\]
Ma allora la matrice $W(x)$ non è invertibile e quindi $\det W(x)=0$, come volevamo dimostrare.

Dimostriamo infine che \ref{item:4736201} implica \ref{item:4736202}
cioè che se $v_1, \dots, v_n$ sono indipendenti allora $\det W(x) \neq 0$ per ogni $x\in I$.
Fissato $x\in I$ consideriamo il funzionale $J(v) = (v(x),v'(x), \dots, v^{(n-1)}(x))$ cosicché si ha
\[
  W(x) = \enclose{J(u_1), \dots, J(u_n)}.
\]
In base al teorema~\ref{th:edo_lineare_ordine_n} ci ricordiamo che $J$ risulta essere un isomorfismo di spazi vettoriali, dunque se $v_1,\dots,v_n$ sono indipendenti anche $J(v_1),\dots, J(v_n)$ dovranno essere indipendenti.
Ma visto che le colonne della matrice $W(x)$ sono indipendenti significa che $\det W(x)\neq 0$, come volevamo dimostrare.
\end{proof}

\begin{theorem}[metodo della variazione delle costanti arbitrarie]
\mymark{***}%
Si consideri una generica equazione differenziale lineare non omogenea di ordine $n$ in forma normale:
\begin{equation}\label{eq:395467435}
  u^{(n)}(x) = \sum_{k=0}^{n-1} a_k(x) u^{(k)}(x) + b(x)
\end{equation}
dove $a_0, \dots, a_{n-1}, b\in C^0(A,\CC)$ sono funzioni definite su un aperto $A\subset \RR$
e si cerca una soluzione $u\in C^n(A,\CC)$.

Se $v_1, \dots, v_n\in C^n(A,\CC)$ sono soluzioni indipendenti dell'equazione omogenea associata:
\begin{equation*}
  u^{(n)}(x) = \sum_{k=0}^{n-1} a_k(x) u^{(k)}(x)
\end{equation*}
allora
esistono delle funzioni $c_k\in C^1(A,\CC)$
per $k=0,1, \dots, n-1$
tali che
\begin{equation}\label{eq:02154676}
  \begin{cases}
    c'_1 v_1 + \dots + c'_n v_n = 0 \\
    c'_1 v_1' + \dots + c'_n v_n' = 0 \\
    \vdots\\
    c'_1 v_1^{(n-2)} + \dots + c'_n v_n^{(n-2)} = 0 \\
    c'_1 v_1^{(n-1)} + \dots + c'_n v_n^{(n-1)} = b(x).
  \end{cases}
\end{equation}
E in tal caso la funzione
\[
 u(x) = \sum_{k=1}^n c_k(x) v_k(x)
\]
risolve l'equazione non omogenea~\eqref{eq:395467435}.
\end{theorem}
%
\begin{proof}
Osserviamo innanzitutto che se esistono le funzioni $c_k$ che
soddisfano il sistema~\eqref{eq:02154676} allora la funzione $u = c_1
\cdot v_1 + \dots + c_n \cdot v_n$ risolve l'equazione non
omogenea. Infatti dalla formula di derivazione del prodotto si ha:
\[
  u' = \sum_{k=1}^n c_k v_k' + \sum_{k=1}^n c_k' v_k
     = \sum_{k=1}^n c_k v_k'
\]
essendo il secondo addendo nullo per ipotesi~\eqref{eq:02154676}.
Ma allora si può proseguire con le derivate:
\[
  u'' = \sum_{k=1}^n c_k v_k'' + \sum_{k=1}^n c_k' v_k'
      = \sum_{k=1}^n c_k v_k''
\]
fino alla derivata $(n-1)$-esima:
\[
  u^{(n-1)} = \sum_{k=1}^n c_k v_k^{(n-1)}.
\]
Per l'ultima derivata si ha infine:
\[
  u^{(n)} = \sum_{k=1}^n c_k v_k^{(n)} + \sum_{k=1}^n c_k' v_k^{(n-1)}
          = \sum_{k=1}^n c_k v_k^{(n)} + b.
\]
Ma allora si osserva che si ha
\begin{align*}
 u^{(n)} + \sum_{j=0}^{n-1} a_j u^{(j)}
 &= \sum_{k=1}^n c_k v_k^{(n)} + b + \sum_{j=0}^{n-1} a_j \sum_{k=1}^n c_k v_k^{(j)} \\
 &= \sum_{k=1}^n c_k \Enclose{v_k^{(n)} + \sum_{j=0}^{n-1} a_j v_k^{(j)}} + b = b
\end{align*}
 in quanto le funzioni $v_j$, per ipotesi, risolvono l'equazione omogenea.

 Rimane quindi solo da verificare che esistono funzioni $c_k\in
 C^1(A,\CC)$ che soddisfano il sistema~\eqref{eq:02154676}.
 Si osserva che il sistema~\eqref{eq:02154676} si scrive nella forma
 \[
   W(x) \vec c'(x) = \vec b(x)
 \]
 dove $W(x)$ è la matrice wronksiana $W(x)$ associata
 alle soluzioni $v_1, \dots, v_n$, $\vec c(x) = (c_1(x), \dots,
 c_n(x))$ e $\vec b(x) = (0, \dots, 0, b(x))$.
 Siccome $v_1, \dots, v_n$ per ipotesi sono indipendenti,
 per il teorema~\ref{th:wronksiano}
 sappiamo che
 $\det W(x)\neq 0$ per ogni $x$ e quindi il sistema $W(x) \vec d(x) =
 \vec b(x)$
 ammette una unica soluzione $d(x)$ per ogni $x\in I$.
 Visto che $W(x)$ e $\vec b(x)$ hanno coefficienti continui,
 anche la soluzione $\vec d(x) = W(x)^{-1} \vec b(x)$ dovrà essere continua (che i coefficienti della matrice $W(x)^{-1}$ siano continui lo si vede ad esempio scrivendo $W(x)^{-1}$ tramite la matrice dei cofattori e sfruttando il fatto che il determinante è una funzione continua)
 e quindi, fissato un punto $x_0\in I$,
 potremo definire
 \[
   c_k(x) = \int_{x_0}^x d_k(t)\, dt
 \]
 per determinare le funzioni $c_k$.
\end{proof}

\begin{exercise}
Trovare tutte le soluzioni dell'equazione lineare non omogenea a coefficienti
costanti:
\[
  u''(x) + u(x) = \frac{1}{\cos x}.    
\]
\end{exercise}
\begin{proof}[Svolgimento.]
Il polinomio associato è $P(\lambda) = \lambda^2 + 1$ che 
ha due radici complesse coniugate $\pm i$.
Due soluzioni indipendenti dell'equazione omogenea $u''+u=0$ sono dunque 
(come noto):
\[
 u_1(x) = \cos x, \qquad u_2(x) = \sin x.
\]
La soluzione generale dell'omogenea è dunque della forma:
\[
  u(x) = A \cos x + B \sin x.  
\]

Per trovare una soluzione particolare della equazione non omogenea 
utilizziamo il metodo della variazione delle costanti, cioè
cerchiamo una soluzione della forma:
\[
  u_*(x) = A(x) \cos x + B(x) \sin x.
\]
I coefficienti $A(x)$ e $B(x)$ si determinano risolvendo il sistema 
\[
\begin{cases}
  A'(x) \cdot \cos x &+ B'(x)\cdot \sin x = 0, \\
  A'(x) \cdot (-\sin x) &+ B'(x)\cdot \cos x = \frac{1}{\cos x}.
\end{cases}  
\]
Si trova quindi 
\[
 \begin{cases}
  A'(x) = -\frac{\sin x}{\cos x} \cdot B'(x) \\
  B'(x) = 1
 \end{cases}  
\]
da cui, integrando, si trova
\[
  A(x) = \ln\abs{\cos x}, \qquad B(x) = x
\]
e quindi 
\[
 u_*(x) = \cos x\cdot \ln \abs{\cos x } + x\sin x.  
\]

Tutte le soluzioni dell'equazione data si scrivono quindi come somma della 
soluzione particolare della non omogenea più la soluzione generale della 
omogenea:
\[
 u(x) = a \cos x + b \sin x + \cos x \cdot \ln \abs{\cos x} + x \sin x.  
\]
\end{proof}
