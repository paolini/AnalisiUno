\chapter{Equazioni differenziali}

\section{classificazione}

Le equazioni differenziali sono una classe di \emph{equazioni funzionali}
\mynote{equazioni funzionali}
\index{equazione!funzionale}
ovvero
equazioni in cui l'incognita non è un numero (come accade nelle equazioni algebriche) ma è una funzione. La funzione incognita $u$, sarà quindi funzione di una variabile indipendente $u=u(t)$. Ad esempio $u$ potrebbe essere la traiettoria di un proiettile che è quindi una posizione nello spazio in funzione del tempo $t$. Se nelle equazioni algebriche il nome di gran lunga più utilizzato per l'incognita è $x$, nelle equazioni funzionali a seconda dei contesti le convenzioni possono cambiare in maniera drastica. Si dovrà utilizzare un nome per la funzione e un nome per la sua variabile indipendente: $x=x(t)$, $y=y(x)$, $y=y(t)$, $u=u(x)$ sono alcune delle scelte più utilizzate.
Se l'incognita è una funzione le operazioni che possono comparire nell'equazioni sono, oltre le usuali operazioni algebriche che agiscono sui singoli valori dell'equazione, anche operatori che agiscono sulla funzione in sé. Se l'equazione funzionale oltre alle operazioni algebriche comprende anche l'operatore derivata, si dirà che è una \mynote{equazione differenziale}%
\index{equazione!differenziale}
\emph{equazione differenziale}.
Di contro ci potranno ad esempio essere equazioni che coinvolgono l'operatore integrale e si chiameranno
\emph{equazioni integrali}.

Ci sono quindi diversi concetti che possono aiutare a classificare le equazioni differenziali. Se la funzione incognita è funzione di una singola variabile reale si dirà che l'equazione è una
\emph{equazione differenziale ordinaria}
\mynote{ODE}
\index{equazione!differenziale!ordinaria}
(abbreviato
\emph{EDO} \index{EDO}
in italiano,
\emph{ODE} \index{ODE} per gli anglosassoni).
Di contro se la funzione incognita è funzione di più variabili l'equazione si chiamerà
\emph{equazione differenziale alle derivate parziale}
\mynote{PDE}
\index{equazione!differenziale!alle derivate parziali}
 (abbreviato \emph{EDP} \index{EDP} in italiano e \emph{PDE}
 \index{PDE} in lingua inglese).
In questo corso, centrato sulle funzioni di una variabile, tratteremo quindi solamente le equazioni differenziali ordinarie.
L'\emph{ordine}
\mynote{ordine}
\index{ordine!equazione differenziale}
\index{equazione!differenziale!ordine}
dell'equazione differenziale è il numero massimo di derivate successive che vengono applicate alla funzione incognita.
Tipicamente la funzione $u$ sarà definita su un intervallo $I$ della retta reale: $u\colon I \to \RR$ e la forma più generale di equazione differenziale ordinaria di ordine $n$ si potrà dunque scrivere come:
\begin{equation}\label{eq:3784643}
  F(t,u(t),u'(t),u''(t), \dots, u^{(n)}(t)) = 0.
\end{equation}
con $F\colon \Omega \to \RR$ una funzione data, definita su un insieme $\Omega \subset I\times \RR^{n+1}$.
Una funzione $u\colon I \to \RR$ si dice essere una \emph{soluzione dell'equazione differenziale} \eqref{eq:3784643} se $u$ è derivabile almeno $n$ volte in ogni punto $t\in I$ e se \eqref{eq:3784643} è soddisfatta
per ogni $t\in I$.

Usualmente si tende a semplificare la notazione evitando di scrivere sempre esplicitamente il punto $t$ in cui viene calcolata la funzione. Sarà quindi usuale
scrivere l'equazione \eqref{eq:3784643}
nella forma abbreviata:
\[
  F(t,u,u',u'', \dots, u^{(n)}) = 0
\]
rendendo anche più evidente il fatto che l'incognita è $u$,  l'intera funzione, e non un singolo valore $u(t)$.


Se ad esempio scegliamo $n=2$ e $F(t,u,v,z)= z+\sin u + v$ otteniamo l'equazione differenziale:
\[
 u''(t) + \sin u(t) + u'(t) = 0
\]
che, in opportune unità di misura, è l'equazione del moto di un pendolo smorzato, dove $t$ rappresenta il tempo e $u$ la misura dell'angolo di inclinazione del pendolo rispetto alla verticale.
Osserviamo che nell'esempio precedente la funzione $F$ non dipende direttamente dalla variabile $t$. Equazioni con questa proprietà si dicono
\emph{equazioni autonome}
\mynote{equazioni autonome}
\index{equazione!differenziale!autonoma}
ed è immediato osservare che se $u(t)$ è soluzione anche una sua traslazione temporale $v(t) = u(t-t_0)$ è soluzione dell'equazione (il moto del pendolo non dipende dall'ora in cui si svolge).

Una equazione scritta nella forma \eqref{eq:3784643} si dice \emph{equazione in forma implicita}
\mynote{equazione in forma implicita}
\index{equazione!differenziale!in forma implicita}
e per analogia con le equazioni algebriche (si pensi all'equazione  $u^2(t) + t^2 = 1$) ci si aspetta che le soluzioni di tale equazioni siano meglio rappresentate da curve piuttosto che da grafici di funzione.
Risulta in effetti che la teoria delle equazioni differenziali si applica con molta maggiore efficacia alle \emph{equazioni in forma normale}
\mynote{equazioni in forma normale}
\index{equazione!differenziale!in forma normale}
che sono le equazioni differenziali di ordine $n$ che possono essere scritte
esplicitando la dipendenza dalla derivata di ordine massimo:
\begin{equation}\label{eq:366793}
 u^{(n)}(t) = f(t, u(t), u'(t), \dots, u^{n-1}(t))
\end{equation}
dove $f\colon \Omega \to \RR$ è una funzione definita su $\Omega\subset I\times \RR^n$.
L'equazione del pendolo si può scrivere in questa forma,
scegliendo $f(t,u,v) = -\sin u - v$.

Più in generale potremmo considerare
\emph{sistemi di equazioni differenziali}
\mynote{sistemi}%
\index{sistemi di equazioni differenziali}%
\index{equazione!differenziale!sistema}%
in più incognite.
Possiamo rappresentare un sistema di $k$ equazioni ordinarie in $m$ incognite nella forma:
\begin{equation}\label{eq:375456}
  \vec F(t, \vec u(t), \vec u'(t), \dots, \vec u^{n}(t)) = 0
\end{equation}
dove $\vec u$ è una funzione $\vec u \colon I \to \RR^m$ le cui componenti sono le $m$ funzioni incognite:
\[
  \vec u(t) = (u_1(t), \dots, u_m(t))
\]
mentre la funzione $\vec F \colon I \times (\RR^m)^{n+1} \to \RR^k$ è stavolta una funzione a valori vettoriali $\vec F = (F_1, \dots, F_k)$ cosicché l'equazione vettoriale
\eqref{eq:375456} è effettivamente equivalente ad un sistema di $k$ equazioni:
\[
\begin{cases}
F_1(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
F_2(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
\quad\vdots \\
F_k(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
\end{cases}
\]
Vedremo che per le equazioni ordinarie del primo ordine è naturale, come accade per le equazioni algebriche, avere lo stesso numero di equazioni e di incognite dunque è tipico avere singole equazioni scalari del primo ordine
(cioè in cui l'incognita è una funzione a valori nel campo
degli scalari $\RR$) o sistemi di $k=m$ equazioni del primo ordine con incognita una funzione vettoriale $\vec u$ (cioè una funzione a valori nello spazio vettoriale $\RR^m$) ovvero con $m$ incognite $u_1,\dots, u_m$ funzioni scalari.

Una importante osservazione è il fatto generale che una equazione differenziale di ordine $n$ può essere ricondotta ad un sistema di $n+1$ equazioni differenziali del primo ordine.
Basta infatti considerare come incognita il vettore (chiamato \emph{Jet})
\[
  \vec u = (u, u', u'', \dots, u^{(n)})
\]
comprendente tutte le derivate della funzione scalare $u$.
L'equazione \eqref{eq:3784643}, di ordine $n$, risulta infatti equivalente al sistema di $n+1$ equazioni del primo ordine nella variabile $\vec u = (u_1, \dots, u_{n+1})$
\[
  \begin{cases}
    F(t, u_1(t), u_2(t), \dots, u_{n+1}(t)) = 0\\
    u_2(t) = u_1'(t)\\
    u_3(t) = u_2'(t)\\
    \quad \vdots \\
    u_{n+1}(t) = u_n'(t)
  \end{cases}
\]
Nel caso, più interessante,
delle equazioni in forma normale consideriamo un vettore
$\vec u = (u_1,\dots, u_n)$
con $n$ componenti corrispondenti alle derivate di $u$:
\[
  \vec u = (u, u', \dots, u^{(n-1)})
\]
cosicché \eqref{eq:366793}, l'equazione normale di ordine $n$
diventa un sistema di $n$ equazioni
del primo ordine
in $n$ incognite
\[
  \begin{cases}
  u_n'(t) = f(t,u_1(t), u_2(t), \dots, u_n(t)) \\
  u_1'(t) = u_2(t)\\
  u_2'(t) = u_3(t)\\
  \quad \vdots\\
  u_{n-1}'(t) = u_n(t)
  \end{cases}
\]
ovvero una equazione differenziale vettoriale
\[
  \vec u'(t) = \vec f(t, \vec u(t))
\]
avendo definito $\vec f = (f_1, \dots, f_n)$
come
\begin{gather*}
  f_1(t, y_1,\dots, y_n) = y_2 \\
  f_1(t, y_1,\dots, y_n) = y_3 \\
  \quad \vdots \\
  f_{n-1}(t, y_1,\dots, y_n) = y_n \\
  f_n(t, y_1,\dots, y_n) = f(t,y_1, \dots, y_n)
\end{gather*}

Nell'esempio del pendolo si avrà come incognita una funzione vettoriale $\vec u(t) = (u(t),u'(t))$ le cui componenti sono posizione e velocità angolare. Il codominio di tale funzione si chiama \emph{spazio delle fasi}. L'equazione (essendo autonoma tralasciamo la dipendenza da $t$) si scriverà nella forma $\vec u' = \vec f(\vec u)$ con $\vec f(t,x) = (x, -\sin(t) - x)$.

Un caso molto particolare ma decisamente importante è quello in cui la funzione $F$ (per le equazioni in forma implicita) o la funzione $f$ (per le equazioni in forma normale) sono funzioni lineari
per ogni $t$
rispetto alla variabile $u$.
In tal caso diremo che l'equazione è
\mynote{equazioni lineari omogenee}%
\index{equazione!differenziale!lineare omogenea}%
\emph{lineare omogenea}.
Più precisamente si avrà
\[
F(t,u(t),u'(t), \dots, u^{(n)}(t)) = A_t(u(t), u'(t), \dots, u^{(n)}(t))
\]
con $A_t\colon \RR^{n+1}\to \RR$
operatore lineare per ogni $t$ ovvero $A_t$ si rappresenta tramite un vettore i cui coefficienti sono funzioni della variabile $t$:
\[
  A_t(v) = \sum_{k=0}^n a_k(t) v_k
\]
e l'equazione differenziale si scrive nella forma
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = 0.
\]
Nel caso in cui i coefficienti $a_k(t)$ non dipendano da $t$ (cioè siano funzioni costanti) diremo che l'equazione è
lineare
\mynote{coefficienti costanti}%
\index{equazione!differenziale!lineare a coefficienti costanti}%
\emph{a coefficienti costanti}.

E' facile osservare che l'insieme delle soluzioni di una equazione lineare omogenea è uno spazio vettoriale: $u=0$ è sempre soluzione, se $u$ è una soluzione e $\lambda \in \RR$ anche $\lambda u$ è soluzione e se $u$ e $v$ sono due soluzioni anche $u+v$ è soluzione
\mymargin{principio di sovrapposizione}
\index{principio di sovrapposizione}
(principio di sovrapposizione).

In effetti
se le funzioni $u$ sono definite su un intervallo $I$ possiamo
identificare $F$ con un funzionale $L\colon \RR^I \to \RR^I$ definito da
\[
  L(u)(t) = F(t,u(t), u'(t), \dots, u^{(n)}(t))
\]
Se l'equazione differenziale è lineare allora $L$ è un operatore lineare sullo spazio vettoriale $\RR^I$ e lo spazio delle soluzioni dell'equazione differenziale non è altro che il $\ker L$
ed è noto che $\ker L$ è un sottospazio vettoriale.

Nel caso in cui la funzione $F$ (o la corrispondente $f$)
sia affine si dirà che l'equazione differenziale
è una equazione
\mynote{equazione lineari}%
\index{equazione!differenziale!lineare}%
\emph{lineare (non omogenea)}.
L'equazione non omogenea avrà la forma:
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = g(t).
\]
Se $v_0$ e $v_1$ sono due soluzioni di questa equazione è chiaro che la differenza $u=v_1 - v_0$ è soluzione dell'equazione omogenea
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = 0.
\]
Dunque quest'ultima si chiama equazione omogenea associata alla non omogenea e se $v_0$ è una soluzione particolare (qualunque) dell'equazione non omogenea ogni soluzione $v$ dell'equazione non omogenea si scrive nella forma
\[
  v = v_0 + u
\]
con $u$ soluzione dell'omogenea associata. Per trovare tutte le soluzioni di una equazione non omogenea è dunque sufficiente trovare una soluzione particolare e tutte le soluzioni della equazione omogenea associata.

L'equazione del pendolo non è lineare
ma quando l'angolo $u$ è piccolo (cioè per \emph{piccole oscillazioni}) si ha $\sin u \sim u$.
Facendo questa \emph{linearizzazione} si ottiene l'equazione
\[
  u''(t)  + u(t) + u'(t) = 0.
\]
Questa è una equazione lineare omogenea. Se sul pendolo agisce una forza esterna (pendolo forzato) l'equazione diventa
\[
  u''(t) + u(t) + u'(t) = g(t)
\]
dove $g(t)$ rappresenta l'entità di una forza esterna variabile nel tempo. Questa equazione è lineare non omogenea.

Come nel caso generale le equazioni lineari di ordine $n$ si riconducono a sistemi lineari di $n$ equazioni del primo ordine.

\section[funzioni vettoriali e di più variabili]{alcuni risultati preparatori sulle funzioni vettoriali e di più variabili}

Ci sarà utile estendere la definizione delle classi di regolarità $C^k$ alle funzioni vettoriali di più variabili.

Se $\Omega\subset \RR^m$ è un insieme aperto e $f\colon \Omega \to \RR$ è una funzione, vogliamo definire le \emph{derivate parziali} di $f$. Se $\vec x \in \Omega$ si ha $\vec x = (x_1,\dots, x_m)$ e quindi $f(\vec x) = f(x_1,\dots, x_m)$. Se facciamo variare una sola componente $x_j$ mantenendo fisse tutte le altre componenti, otteniamo una funzione di una variabile $x_j \mapsto g(x_j) = f(x_1,\dots, x_j, \dots, x_m)$.
Definiamo dunque la \emph{derivata parziale}
\mynote{derivata parziale}
\index{derivata!parziale}
di $f$ rispetto alla variabile $x_j$ come la derivata della funzione $g(x_j)$. Denotiamo tale derivata con il simbolo:
\[
  \frac{\partial f}{\partial x_j}.
\]
Le derivate parziali si calcolano dunque come le usuali derivate per le funzioni di una variabile, solamente bisogna considerare costanti tutte le variabili tranne quella coinvolta nella derivazione. Ad esempio se $f(x,v) = -\sin(x) - v$ (come nell'esempio fatto nell'introduzione) si ha
\begin{align*}
  \frac{\partial f}{\partial x} &= -\cos(x)\\
  \frac{\partial f}{\partial v} &= -1.
\end{align*}

Se $\vec f\colon \Omega \subset \RR^m \to \RR^n$ è una funzione di più variabili a valori vettoriali potremo scrivere $\vec f = (f_1, \dots, f_n)$ e considerare le derivate parziali di ogni componente:
\[
  \frac{\partial \vec f}{\partial x_j}
  = \enclose{\frac{\partial f_1}{\partial x_j}, \dots, \frac{\partial f_n}{x_j}}.
\]

\begin{definition}
Sia $\Omega\subset \RR^m$ un aperto. Denoteremo con $C^0(\Omega,\RR^m)$
lo spazio vettoriale di tutte le funzioni
continue $f\colon \Omega \to \RR^m$.

Denotiamo con $C^1(\Omega,\RR^m)$ lo spazio vettoriale
delle funzioni in $C^0(\Omega,\RR^m)$ tali che ognuna delle loro derivate parziali $\frac{\partial f}{\partial x_j}$ è a sua volta una funzione continua (cioè in $C^0(\Omega,\RR^m)$).
Ricorsivamente si definisce $C^k(\Omega,\RR^m)$ per $k>1$,
come lo spazio delle funzioni $\Omega\to\RR^m$ tali che tutte le loro derivate parziali stanno in $C^{k-1}(\Omega,\RR^m)$.
\end{definition}

\begin{definition}[prodotto scalare canonico]
\mymargin{prodotto scalare}
\index{prodotto scalare canonico}
\index{$\cdot$}
Se $\vec v, \vec w\in \RR^n$ definiamo
\[
  \vec v \cdot \vec w = \sum_{k=1}^n v_k w_k.
\]
\end{definition}

\begin{theorem}[disuguaglianza di Cauchy-Schwarz]
\mymark{***}
Siano $\vec v, \vec w \in \RR^m$. Allora
\begin{equation}\label{eq:cauchy_schwarz}
   \vec v \cdot \vec w \le \abs{\vec v}\cdot \abs{\vec w}.
\end{equation}
\end{theorem}
%
\begin{proof}
\mymark{***}
Si ha
\[
 0 \le \abs{\vec v-\vec w}^2 = (\vec v-\vec w)\cdot(\vec v-\vec w) = \abs{\vec v}^2 - 2 \vec v\cdot\vec w + \abs{\vec w}^2
\]
da cui la disuguaglianza (disuguaglianza di Young):
\[
  \vec v \cdot \vec w \le \frac{\abs{\vec v}^2 + \abs{\vec w}^2}{2}.
\]
Nel caso particolare $\abs{\vec v}=\abs{\vec w}=1$ (in tal caso si dice che $\vec v$ e $\vec w$ sono versori) si ottiene
\[
  \vec v \cdot \vec w \le 1.
\]
Applichiamo dunque la precedente disuguaglianza ai versori $\vec v/\abs{\vec v}$ e $\vec w/\abs{\vec w}$ per ottenere
\[
  \frac{\vec v \cdot \vec w}{\abs{\vec v}\cdot \abs{\vec w}} =
  \enclose{\frac{\vec v}{\abs{\vec v}},\frac{\vec w}{\abs{\vec w}}} \le 1.
\]
Se $\abs{\vec v}\neq 0$ e $\abs{\vec w}\neq 0$ moltiplicando ambo i membri per $\abs{\vec v}\cdot\abs{\vec w}$ si ottiene~\eqref{eq:cauchy_schwarz}. In caso contrario ambo i membri della disuguaglianza~\eqref{eq:cauchy_schwarz} sono nulli e la disuguaglianza è banalmente verificata.
\end{proof}

\begin{definition}
Sia $\vec f\colon [a,b]\to \RR^m$, $\vec f(x) = (f_1(x), \dots, f_m(x))$.
Diremo che $\vec f$ è integrabile su $[a,b]$ se ogni
$f_k\colon[a,b]\to \RR$ è integrabile su $[a,b]$ e in tal caso porremo:
\[
  \int_a^b \vec f(x)\, dx = \enclose{\int_a^b f_1(x)\, dx, \dots, \int_a^b f_m(x)\, dx}
\]
cosicché per ogni $k=1,\dots, m$ si ha:
\[
  \enclose{\int_a^b \vec f(x)\, dx}_k = \int_a^b f_k(x)\, dx.
\]
Come al solito si pone inoltre $\int_b^a \vec f = - \int_a^b \vec f$.
\end{definition}

\begin{theorem}\label{teo:tipo_jensen}
\mymark{*}
Sia $\vec f\colon [a,b]\to \RR^m$ integrabile. Allora si ha
\[
  \abs{\int_a^b \vec f(x)\, dx} \le \int_a^b \abs{\vec f(x)}\, dx.
\]
\end{theorem}
%
\begin{proof}
Posto
\[
 \vec v = \int_a^b \vec f(x)\, dx
\]
si ha, usando la linearità dell'integrale
e sfruttando la disuguaglianza di Cauchy-Schwarz
\begin{align*}
  \abs{\vec v}^2
  &= \vec v \cdot \vec v
   = \sum_{k=1}^n v_k \int_a^b f_k(x)\, dx
   = \int_a^b \sum_{k=1}^m v_k f_k(x)\, dx \\
  &= \int_a^b \enclose{\vec v, \vec f(x)}\, dx
  \le \int_a^b \abs{\vec v}\cdot \abs{\vec f(x)}\, dx
  = \abs{\vec v} \int_a^b \abs{\vec f(x)}\, dx.
\end{align*}
Se $\abs{\vec v}\neq 0$ possiamo dividere ambo i membri per $\abs{\vec v}$ e ottenere la disuguaglianza cercata.
Altrimenti se $\abs{\vec v}=0$ la disuguaglianza è certamente soddisfatta in quanto il lato destro non può essere negativo.
\end{proof}

%%%%%
%%%%%
%%%%%
%%%%%
\section{Il problema di Cauchy}
%%%%%
%%%%%
%%%%%
%%%%%

Il problema di Cauchy consiste nel
trovare una soluzione di un sistema di $n$ equazioni differenziali ordinarie del primo ordine in $n$ incognite
con un dato iniziale fissato. Cioè dato $x_0\in \RR$,
e $\vec y_0 \in \RR^n$ si cerca un intervallo $I\subset \RR$ con $x_0\in I$ e una funzione $\vec u \colon I\to \RR^n$
che sia derivabile e che soddisfi:
\begin{equation}\label{eq:problema_cauchy}
\begin{cases}
 \vec u'(x) = \vec f(x, \vec u(x)), \qquad \forall x\in I\\
 \vec u(x_0) = \vec y_0.
\end{cases}
\end{equation}

\begin{theorem}[Cauchy-Lipschitz, esistenza e unicità]
\label{th:cauchy_lipschitz}
\mymark{***}
Sia $\Omega$ un aperto di $\RR\times \RR^n$ e sia $\vec f\colon \Omega \to \RR^n$, una funzione continua e tale che esista $L>0$ per cui vale
\[
  \abs{\vec f(x,\vec y_1) - \vec f(x,\vec y_2)} \le L \abs{\vec y_1 - \vec y_2}
  \]
per ogni $x\in \RR$, $\vec y_1, \vec y_2\in \RR^n$
con $(x,\vec y_1),(x,\vec y_2)\in\Omega$
(diremo che $f(x,\vec y)$ è Lipschitziana in $\vec y$ uniformemente rispetto a $x$).

Dato $(x_0,\vec y_0)\in \Omega$
esiste $\delta_0>0$ tale che per ogni $\delta < \delta_0$
posto $I_\delta = [x_0-\delta, x_0+\delta]$ esiste una unica funzione $\vec u\colon I_\delta \to \RR^n$ tale che $(x,\vec u(x))\in \Omega$ per ogni $x\in I_\delta$, $u$ è derivabile
e soddisfa il problema di Cauchy~\eqref{eq:problema_cauchy}.
\end{theorem}
%
\begin{proof}
\mymark{***}
Innanzitutto vogliamo trasformare il problema differenziale in un problema integrale. Se $\vec u$ è una funzione derivabile soluzione di \eqref{eq:problema_cauchy}
allora è chiaro che $\vec u'(x) = \vec f(x,\vec u(x))$ è continua in quanto composizione di funzioni continue e dunque $\vec u$ è di classe $C^1$. Possiamo dunque integrare tra $x_0$ e $x$ i due lati dell'equazione differenziale per ottenere:
\[
  \vec u(x) - \vec u(x_0) = \int_{x_0}^x \vec f(s,\vec u(s))\, ds
\]
e dunque se $\vec u$ risolve il problema di Cauchy allora $\vec u$ soddisfa anche la seguente equazione integrale:
\begin{equation}\label{eq:cauchy_integrale}
  \vec u(x) = \vec y_0 + \int_{x_0}^x \vec f(s,\vec u(s))\, ds.
\end{equation}
Viceversa se $\vec u$ è continua e soddisfa \eqref{eq:cauchy_integrale} allora si ha ovviamente
$\vec u(x_0) = \vec y_0$ e, passando alle derivate, si scopre che $\vec u$ è derivabile e soddisfa l'equazione differenziale $\vec u'(x) = \vec f(x,\vec u(x))$.
Dunque trovare una soluzione $C^1$ del problema di Cauchy~\eqref{eq:problema_cauchy} è equivalente a trovare una soluzione $C^0$ del problema integrale~\eqref{eq:cauchy_integrale}.
Ci dedicheremo dunque a questo secondo problema.

Denotiamo con
\[
  C_{\alpha,\beta} =
  \{(x,\vec y)\in \RR\times\RR^n\colon \abs{x-x_0}\le \alpha, \abs{\vec y -\vec y_0}\le \beta\}
\]
il cilindro centrato in $(x_0, \vec y_0)$ con asse parallelo all'asse delle $x$,
di altezza $2\alpha$ e raggio $\beta$.
Visto che $\Omega$ è aperto esiste una palla aperta centrata in $(x_0,\vec y_0)$ e contenuta in $\Omega$.
Tale palla dovrà anche contenere un cilindro $C_{\alpha,\beta}$ per un qualche $\alpha>0$ e $\beta>0$.
Il cilindro $C_{\alpha,\beta}$ è chiuso e limitato in $\Omega$ e dunque $\vec f$, essendo continua, è limitata su $C_{\alpha,\beta}$ per il teorema di Weierstrass.
Sia $M>0$ tale che $\abs{\vec f(x,\vec y)}\le M$ per ogni $(x,\vec y) \in C_{\alpha,\beta}$ e definiamo
\begin{equation}\label{eq:395109}
 \delta_0 = \min\{\alpha, \frac{\beta}{M}, \frac{1}{L}\}.
\end{equation}
Il perché $\delta_0$ venga definito in questo modo si capirà nel prosieguo della dimostrazione.
Consideriamo un qualunque $\delta<\delta_0$,
poniamo $I=[x_0-\delta,x_0+\delta]$ e
\[
X=\{\vec u\in C^0(I,\RR^n)\colon \Abs{\vec u-\vec y_0}_\infty
\le \beta \}.
\]
Sappiamo che $C^0(I,\RR^n)$ è uno spazio metrico completo e $X$ è chiuso quindi anch'esso è completo. Possiamo allora considerare l'operatore $T\colon X \to C^0(I, \RR^n)$:
\[
  T(\vec u)(x) = \vec y_0 + \int_{x_0}^x \vec f(s, \vec u(s))\, ds.
\]
Vogliamo innanzitutto verificare che $X$ è invariante, cioè che $T(X)\subset X$. Ma se $\vec u \in X$ e se $x\in I$ si ha
$(x,\vec u(x)) \in C_{\delta,\beta} \subset C_{\alpha,\beta}$
e dunque (usando anche~\eqref{eq:395109} e il teorema~\ref{teo:tipo_jensen})
\[
  \abs{\vec T(u)(x)-\vec y_0}
  = \abs{\int_{x_0}^x \vec f(s,\vec u(s))\, ds}
  \le \int_{x_0}^x M\, ds = \abs{x-x_0} M \le \delta M
  \le \beta.
\]
Dunque $\Abs{T(\vec u)-\vec y_0}\le \beta$ e $T(\vec u)\in X$.
Abbiamo dunque che $T\colon X \to X$. Verifichiamo ora che $T$ è una contrazione. Si ha:
\begin{align*}
  \abs{T(\vec u_1)(x)-T(\vec u_2)(x)}
   &\le \abs{\int_{x_0}^x\abs{\vec f(s,\vec u_1(s))-\vec f(s,\vec u_2(s))}\, ds}\\
   &\le \abs{\int_{x_0}^x L\abs{\vec u_1(s)-\vec u_2(s)}\, ds}\\
   &\le L \abs{x-x_0}\Abs{\vec u_1 - \vec u_2}_\infty
   \le L \delta \Abs{\vec u_1 - \vec u_2}_\infty
\end{align*}
ovvero, facendo l'estremo superiore al variare di $x\in I$:
\[
 \Abs{T(\vec u_1)-T(\vec u_2)}_\infty \le L \delta \Abs{\vec u_1 - \vec u_2}_\infty \le \frac{1}{2}\Abs{\vec u_1- \vec u_2}.
\]
Da~\eqref{eq:395109} risulta $L\delta < L \delta_0 \le 1$
dunque $T\colon X \to X$ è una contrazione su uno spazio metrico completo. Per il teorema di punto fisso di Banach-Caccioppoli
sappiamo quindi che esiste una unica funzione $\vec u \in X$ ovvero $\vec u \colon I \to \RR^n$ che soddisfa $T(\vec u)=\vec u$ ovvero~\eqref{eq:cauchy_integrale} ovvero~\eqref{eq:problema_cauchy}.
\end{proof}

\begin{proposition}
\mymark{***}
Il teorema precedente si applica in particolare se
$\vec f \in C^1(\Omega, \RR^n)$.
\end{proposition}
%
\begin{proof}
\mymark{***}
Se prendiamo un cilindro $K=C_{\alpha,\beta}\subset \Omega$ centrato in $(x_0,\vec y_0)$, sappiamo che ogni derivata parziale $\partial \vec f / \partial y_j$ è continua ed è quindi limitata (per il teorema di Weierstrass) su $K$. Sia $L_j$ il massimo di $\abs{\partial \vec f / \partial u_j}$ su $K$ e sia $L$ il massimo degli $L_j$. Allora presi $\vec y, \vec z \in \RR^n$ con $(x,\vec y),(x,\vec z)\in K$ si può scomporre l'incremento vettoriale $\vec z - \vec y$ lungo le $n$ direzioni coordinate e applicare il teorema di Lagrange lungo ogni direzione.
Per ogni $k = 1,\dots, n$ si ha quindi:
\begin{align*}
\MoveEqLeft
\abs{f_k(x,\vec z)-f_k(x,\vec y)}\\
&\le \sum_{j=1}^n \abs{f_k(x,z_1, \dots, z_j, y_{j+1}, \dots, y_n) - f_k(x,z_1, \dots, z_{j-1}, y_j, \dots, y_n)}\\
&\le \sum_{j=1}^n L_j \abs{z_j-y_j}
\le \sum_{j=1}^n L_j \abs{z-y}
\le n L \abs{z-y}
\end{align*}
da cui
\begin{align*}
  \abs{\vec f(x,\vec z) - \vec f(x,\vec y)}
  &= \sqrt{\sum_{k=1}^n \abs{f_k(x,\vec z) - f_k(x,\vec y)}^2} \\
  &\le \sqrt{\sum_{k=1}^n n^2 L^2 \abs{z-y}^2} \\
  &\le n\sqrt{n} L \abs{z-y}.
\end{align*}
Abbiamo quindi mostrato che per ogni $(x_0,\vec y_0)\in \Omega$ esiste un intorno del punto $(x_0,\vec y_0)$ in cui la funzione $\vec f$ soddisfa le ipotesi del teorema~\ref{th:cauchy_lipschitz}.
\end{proof}

\begin{comment}

\begin{theorem}[maggiore regolarità delle soluzioni]
Sia $\vec u(x)$ una soluzione del sistema di
equazioni differenziali:
\[
  \vec u'(x) = \vec f(x, \vec u(x)).
\]
Se $\vec f\colon \Omega\subset \RR\times\RR^n \to\RR^n$ è una funzione di classe $C^k$, con $k\in \NN$ allora la soluzione $\vec u(x)$ è di classe $C^{k+1}$.
In particolare se $\vec f \in C^\infty$ anche $u\in C^\infty$.
\end{theorem}
%
\begin{proof}
Se $\vec u$ soddisfa l'equazione differenziale significa quanto meno che $\vec u$ è derivabile. Dunque è continua e derivabile e la derivata è
\[
  \vec u'(x) = \vec f(x,\vec u(x)).
\]
Ma il lato destro è composizione di funzioni continue (in quanto al minimo $\vec f \in C^0$ e $u\in C^0$) dunque la derivata di $u$ è di classe $C^0$. Significa che come minimo $\vec u$ è di classe $C^1$. Se ora $k\ge 1$ possiamo ripetere il procedimento e osservare che $\vec u'(x)$ è uguale ad una composizione di funzioni $C^1$, dunque è di classe $C^1$. Ma allora $\vec u$ è di classe $C^2$.
Il procedimento può essere dunque iterato fino ad arrivare a dedurre che $\vec u$ è di classe $C^k$. A quel punto la funzione $f(x,\vec u(x))$ è di classe $C^k$ e quindi $\vec u'$ è di classe $C^k$ e $\vec u$ è di classe $C^{k+1}$. Non si può iterare ulteriormente perché ora componendo una funzione $C^k$ con una funzione $C^{k+1}$ quello che ottengo è di nuovo $C^k$.

Dire che $f\in C^\infty$ significa che $f\in C^k$ per ogni $k$. Dunque $\vec u$ risulta essere di classe $C^{k+1}$ per ogni $k$: dunque diremo che $\vec u$ è di classe $C^\infty$.
\end{proof}
\end{comment}

\begin{definition}[estensione di una soluzione, soluzione massimale]
Sia $I\subset \RR$ un intervallo non vuoto e sia $\vec u \colon I \to \RR^n$ una soluzione dell'equazione differenziale in forma normale del primo ordine:
\begin{equation}\label{eq:edo_normale_ordine_uno}
  \vec u'(x) = \vec f(x,\vec u(x))
\end{equation}
dove $\vec f\colon \Omega \subset \RR \times \RR^n \to \RR^n$
 e $\Omega$ è un aperto di $\RR \times \RR^n$.

 Se $J\supset I$, $J \neq I$ è un intervallo e se $\vec v\colon J \to \RR^n$ risolve la stessa equazione~\eqref{eq:edo_normale_ordine_uno}
 e se $\vec v(x)=\vec u(x)$ per ogni $x\in I$, diremo che $\vec v$ è una \myemph{estensione della soluzione} $u(x)$.

 Se la soluzione $\vec u$ non ammette estensioni, diremo che $\vec u$ è una soluzione definita su un intervallo massimale o, più semplicemente, diremo che $\vec u$ è una \myemph{soluzione massimale}.
\end{definition}

\begin{proposition}[caratterizzazione delle soluzioni massimali]
\label{prop:edo_massimale}
Supponiamo che $\vec f\colon \Omega\subset \RR\times\RR^n\to\RR^n$
soddisfi le ipotesi del teorema di Cauchy-Lipschitz.
Allora una soluzione $\vec u$ di \eqref{eq:edo_normale_ordine_uno}
è definita su un intervallo massimale $I$ se e solo se
$I$ è aperto e per ogni $K$ compatto $K\subset \Omega$
e per ogni $x_0 \in I$ esistono $x_1,x_2\in I$, $x_1 < x_0 < x_2$ tali che $\vec u(x_1)\not \in K$ e $\vec u(x_2) \not \in K$.
Significa cioè che il grafico di $\vec u$ cioè la curva $(x,\vec u(x))$ esce da qualunque compatto fissato $K\subset \Omega$ sia facendo crescere $x$ verso destra che facendo calare $x$ verso sinistra.
In altre parole il grafico della soluzione massimale tende ad arrivare sulla frontiera di $\Omega$.
\end{proposition}
%
\begin{proof}
\emph{Prima implicazione}.
Supponiamo che $\vec u\colon I \to \RR^n$ sia una soluzione di~\eqref{eq:edo_normale_ordine_uno} definita su un intervallo massimale $I$. Dovrà essere $(x,u(x))\in \Omega$ per ogni $x\in I$.

Mostriamo innanzitutto
che $I$ è un intervallo aperto a destra: se non lo fosse si avrebbe che $x_0=\sup I \in I$. Allora $(x_0,u(x_0))\in \Omega$ e per il teorema di esistenza e unicità locale
possiamo trovare un intorno $J$ di $x_0$ e una funzione
$\vec v\colon J\to \RR^n$ che risolve~\eqref{eq:edo_normale_ordine_uno}. Per l'unicità delle soluzioni $\vec u$ e $\vec v$ devono coincidere su $I\cap J$ e dunque facendo l'unione dei due grafici ottengo una estensione di $\vec u$ a tutto l'intervall $I\cup J$ che è strettamente più grande di $I$. Dunque $\vec u$ non poteva essere una soluzione massimale.

Mostriamo ora che la curva $(x,\vec u(x))$ esce da ogni compatto $K\subset \Omega$ sia da destra che da sinistra. Supponiamo per assurdo che esista un compatto $K$ tale che $(x,\vec u(x))\in K$ per ogni $x\in I$.
Il grafico $(x,\vec u(x))$ è limitato per $x\in I$ dunque certamente $I$ deve essere limitato. Inoltre abbiamo visto che $I$ è aperto e quindi $I=(a,b)$ con $a,b\in \RR$, $a<b$.

Allora per ogni $x\in I=(a,b)$ si ha
\[
  \abs{\vec u'(x)} = \abs{\vec f(x, \vec u(x))}
   \le \sup_K \abs{\vec f}.
\]
Visto che $\vec f$ è continua (in quanto soddisfa le ipotesi del teorema di esistenza e unicità) la funzione $\abs{\vec f}$ ha massimo su $K$ per il teorema di Weierstrass e quindi esiste $M\ge 0$ tale che $\abs{\vec u'(x)} \le M$ per ogni $x\in (a,b)$. Significa che ogni componente di $\vec u$ è $M$-lipschitziana, in particolare
ogni componente è uniformemente continua. Dunque la funzione $\vec u$ può essere estesa con continuità ad una funzione $\vec v \colon [a,b] \to \RR^n$ definita anche agli estremi dell'intervallo.
Visto che $(x,\vec u(x)) \in K$ per ogni $x\in(a,b)$ e visto che $\vec v(b) = \lim_{x\to b^-}\vec u(x)$ essendo $K$ chiuso possiamo affermare che $\vec v(b) \in K$. E lo stesso vale per $\vec v(a)$.  particolare $(x,\vec v(x))\in K \subset \Omega$ per ogni $x\in [a,b]$. Vogliamo ora verificare che $\vec v$ è soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno}.
Chiaramente $\vec v$ soddisfa l'equazione per ogni $x\in (a,b)$
in quanto su $(a,b)$ coincide con $\vec u$ che è soluzione.
Consideriamo l'estremo $b$.
Visto che $\vec v$ è continua in $b$ e $\vec f$ è continua in $(b,\vec v(b))$ si ha
\[
  \lim_{x\to b^-} \vec v'(x)
  = \lim_{x\to b^-} \vec f(x,\vec  v(x))
  = \vec f(b,\vec v(b)) \in \RR^n
\]
Sappiamo però che se il limite della derivata di una funzione continua esiste ed è finito, allora la funzione è derivabile nel punto limite e la derivata è continua in quel punto (si applichi il teorema dell'Hospital al limite del rapporto incrementale).
Dunque $\vec v$ è derivabile in $b$ e anche in quel punto soddisfa l'equazione differenziale. Lo stesso vale nel punto $a$. Dunque siamo riusciti a trovare una estensione $\vec v$ di $\vec u$ contraddicendo l'ipotesi che $\vec u$ fosse una soluzione massimale.

\emph{Seconda implicazione}. Supponiamo ora $\vec u\colon I\to \RR^n$ sia una soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno} definita su un intervallo $I$ e tale che la curva $(x,\vec u(x))$ esca da ogni compatto $K$ al variare di $x\in I$. Vogliamo dimostrare che $\vec u$ è soluzione massimale. Innanzitutto $I$ non può essere compatto, altrimenti anche $K=\{(x,\vec u(x))\colon x \in I\}$ sarebbe un compatto di $\Omega$ e ovviamente la curva non esce da $K$.
Sia $a=\inf I$ e $b=\sup I$. Siccome $I$ non è chiuso esso non contiene uno dei due estremi: supponiamo sia $b$. Allora se $\vec u$ fosse estendibile a destra esisterebbe una estensione continua $\vec v\colon (a,b]\to \RR^n$ che coincide con $\vec u$ su $(a,b)$ e che è continua nel punto $x=b$ e che soddisfa l'equazione differenziale quindi, in particolare, $(b,\vec v(b))\in \Omega$.
Ma allora scelto $x_0 \in (a,b)$
prendiamo $K=\{(x,\vec v(x))\colon x\in [x_0,b]\}$. Visto che $\vec v\colon[x_0,b]\to \RR^n$ è continua è facile verificare che $K$ è chiuso è limitato, $K\subset \Omega$. Ma ovviamente $(x,\vec v(x))\in K$ per ogni $x\ge x_0$, contro le ipotesi.
\end{proof}

\begin{proposition}[separazione delle soluzioni]
\label{prop:separazione_soluzioni}
Se $\vec f\colon \Omega\subset \RR \times \RR^n\to \RR^n$ soddisfa le ipotesi del teorema di esistenza e unicità e se $\vec u\colon I \to \RR^n$ e $\vec v\colon J\to\RR^n$ sono due soluzioni
dell'equazione differenziale
$\vec u'(x) = \vec f(x,\vec u(x))$
definite su due intervalli $I,J \subset \RR$  e se esiste $x_0\in I\cap J$ tale che $\vec u(x_0) = \vec v(x_0)$ allora $\vec u(x) = \vec v(x)$ per ogni $x\in I\cap J$.
Detto in altri termini: nelle ipotesi del teorema di esistenza e unicità i grafici di due soluzioni diverse definite in un intervallo non possono toccarsi.
\end{proposition}
%
\begin{proof}
Sia $x_0 \in I\cap J$ un punto in cui $\vec u(x_0) = \vec v(x_0)$
e supponiamo per assurdo che esista $x_2 \in I\cap J$ tale che $\vec u(x_2)\neq \vec v(x_2)$. In tal caso possiamo considerare il punto
\[
   x_1 = \sup \{x \in [x_0,x_2] \colon \vec u(x) = \vec v(x)\}.
\]
Su tutto l'intervallo $[x_0,x_1)$ si ha quindi $\vec u(x) = \vec v(x)$ e per continuità dovrà dunque anche essere $\vec u(x_1) =  \vec v(x_1)$.
In pratica $x_1$ è l'ultimo punto di contatto tra le due soluzioni.
Ponendo allora il punto $(x_1,\vec u(x_1))$ come dato iniziale del problema di Cauchy scopriamo che $\vec u$ e $\vec v$ sono localmente due soluzioni di tale problema. Per l'unicità locale le due soluzioni devono coincidere in un piccolo intorno del punto $x_1$, diciamo in particolare che devono coincidere su $[x_1,x_1+\eps]$ ma questo è in contraddizione con la definizione di $x_1$.
\end{proof}

\begin{theorem}[esistenza di soluzioni massimali]
\label{th:edo_esistenza_massimali}
Sia $\vec u\colon I\to \RR^n$ una soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno}
definita su un intervallo non vuoto $I\subset \RR$
e supponiamo che tale equazioni soddisfi il teorema di esistenza e unicità locale (questo succede ad esempio se $f\in C^1$).
Se $\vec u$ non è essa stessa una soluzione massimale, esiste
sempre una estensione massimale $\vec v\colon J\to \RR^n$, $J\supset I$.
\end{theorem}
%
\begin{proof}
Supponiamo che $\vec u$ non sia massimale, dunque $\vec u$ ammette estensioni. In particolare ammette estensioni definite su intervalli aperti, in quanto se una soluzione è definita su un intervallo che non è aperto posso sempre estenderla in un intorno aperto dei punti di frontiera dell'intervallo mediante il teorema di esistenza locale ottenendo quindi una estensione definita su un intervallo aperto.

Sia $\mathcal F$ l'insieme di tutte le estensioni di $\vec u$ definite su intervalli aperti. Più precisamente ogni $\vec w\in \mathcal F$ è una funzione $\vec w\colon J_{\vec w}\to \RR^n$ definita su un intervallo aperto $J_{\vec w}\supset I$ che soddisfa l'equazione differenziale~\eqref{eq:edo_normale_ordine_uno} e che coincide con $\vec u$ su $I$. Definiamo $\vec v\colon J\to\RR^n$ come segue:
\begin{align*}
J &= \bigcup_{\vec w \in \mathcal F} J_{\vec w}\\
\vec v(x) &= \vec w(x) \qquad\text{se $x\in J_{\vec w}$}.
\end{align*}
Si osservi che dato $x\in I$ se $\vec w_1, \vec w_2\in\mathcal F$ sono due estensioni entrambe definite su un punto $x$, allora esse coincidono in $x$ per la Proposizione~\ref{prop:separazione_soluzioni}, dunque $\vec v(x)$ è univocamente definita.

Essendo ogni $J_{\vec w}$ aperto è chiaro che $J$ è aperto. E' anche facile convincersi che $J$ deve essere un intervallo, in quanto tutti i $J_{\vec w}$ hanno in comune i punti di $I$.
Verifichiamo ora che $\vec v$ soddisfa l'equazione differenziale. Preso un punto $x\in J$ deve esistere $\vec w$ tale che $x\in J_{\vec w}$. Sappiamo che $\vec w'(x) = f(x,\vec w(x))$ e visto che $\vec u$ coincide con $\vec w$ su $J_{\vec w}$ possiamo dedurre che anche le derivate, in $x$, coincidono:
\[
\vec u'(x) = \vec w'(x) = f(x,\vec w(x)) = f(x,\vec u(x)).
\]
Dunque anche $\vec u$ è soluzione di~\eqref{eq:edo_normale_ordine_uno}.
In pratica abbiamo verificato che $\vec v\in \mathcal F$ ed è quindi una estensione di $\vec u$. Vogliamo ora dimostrare che è una estensione massimale. Supponiamo per assurdo che esista $\vec w$ estensione di $\vec v$ definita su un intervallo $J_{\vec w}\supset J$, $J_{\vec w}\neq J$.
Posso supporre che $J_{\vec w}$ sia aperto, perché in caso contrario potrei estendere la soluzione $\vec w$ in un intorno aperto dei punti di frontiera dell'intervallo, mediante il teorema di esistenza locale, ottenendo una estensione definita su un intervallo più grande ed aperto.
Dunque $\vec w\in \mathcal F$ ma allora, per definizione di $J$, dovrà essere $J\supset J_{\vec w}$: assurdo.
\end{proof}

\begin{theorem}[esistenza globale]
\label{th:edo_esistenza_globale}
Sia $I = (a,b) \subset \RR$ un intervallo aperto, $\Omega=I\times \RR^n$ e sia
$\vec f\colon \Omega \to \RR^n$, $(x,\vec y)\mapsto \vec f(x,\vec y)$ una funzione che
soddisfa le ipotesi di esistenza e unicità locale (dunque continua nella coppia $(x,\vec y)$
e lipschitziana nella variabile $\vec y$ uniformemente rispetto alla variabile $x$) nell'intorno di ogni punto di $\Omega$ (ad esempio $\vec f \in C^1(\Omega,\RR^n)$ è condizione sufficiente)
e che soddisfi
inoltre l'ipotesi aggiuntiva (sublinearità in $\vec y$ uniforme rispetto a $x$):
\[
  \abs{\vec f(x,\vec y)} \le m\abs{\vec y} + q,
  \qquad\forall (x,\vec y)\in \Omega.
\]
Allora per ogni $x_0\in I$ e per ogni $\vec y_0\in \RR^n$ esiste una funzione definita su tutto $I$:
$\vec u\colon I \to \RR^n$ soluzione del problema di Cauchy~\eqref{eq:problema_cauchy}.
\end{theorem}
%
La dimostrazione richiede un lemma preliminare.
\begin{lemma}[Gronwall]
Siano $a,b\in \RR$ con $a<b$ e siano $m,q\ge 0$.
Sia $\vec u \colon [a,b) \to \RR^n$ una funzione di classe $C^1$
tale che
\[
  \abs{\vec u'(t)} \le m \abs{\vec u(t)} + q
  \qquad \forall t\in [a,b).
\]
Allora $\vec u$ è limitata.
\end{lemma}
%
\begin{proof}
Ricordando che
\[
  (\abs{\vec u}^2)'
  = (\vec u\cdot \vec u)'
  = 2 \vec u \cdot \vec u'
\]
per ogni $x\in [a,b)$ possiamo fare la seguente stima:
\begin{align*}
\Enclose{\ln(1+\abs{\vec u(t)}^2)}_{a}^{x}
&= \int_{a}^{x}
  \enclose{\ln(1+\abs{\vec u(t)}^2)}' \, dt
= \int_{a}^{x}
\frac{2\vec u(t)\cdot \vec u'(t)}{1+\abs{\vec u(t)}^2}\, dt \\
&\le 2 \int_{a}^{x} \frac{\abs{\vec u(t)} \abs{\vec u'(t)}}{1+\abs{\vec u(t)}^2}\, dt
\le 2\int_{a}^{x} \frac{\abs{\vec u}\enclose{m\abs{\vec u(t)}+q}}{1+\abs{\vec u(t)}^2}\, dt\\
&\le 2\int_{a}^{x} \enclose{m + \frac{q\abs{\vec u(t)}}{1+\abs{\vec u(t)}^2}}\, dt\\
&\le (x-a)(2m + q) \le (b-a)(2m+q)
\end{align*}
avendo anche sfruttato la disuguaglianza $s/(1+s^2) \le 1/2$ con $s=\abs{\vec u(t)}$.
Dunque
\[
  \ln(1+\abs{\vec u(x)}^2) \le \ln(1+\abs{\vec u(a)}^2) + (b-a)(2m+q)
\]
è una funzione limitata e di conseguenza anche $\abs{\vec u(x)}$ è limitata.
\end{proof}

\begin{proof}[Dimostrazione teorema~\ref{th:edo_esistenza_globale}]
Sia $\vec u$ una soluzione massimale del problema di Cauchy e sia $J\subset I$ l'intervallo massimale di esistenza con $x_0\in J$.
Per la proposizione~\ref{prop:edo_massimale} sappiamo che $J$ è un intervallo aperto:
dobbiamo dimostrare che $J=I$.
Basterà dimostrare che $\sup J=\sup I$ e $\inf J = \inf
I$. Dimostreremo solamente che $\sup J = \sup I$ in quanto l'altra
uguaglianza si fa in maniera analoga (o si ottiene per simmetria).
Sia $b=\sup J$ e supponiamo per assurdo che sia $b<\sup I\le +\infty$.
Allora possiamo applicare il lemma precedente alla funzione $\vec u$
sull'intervallo $[x_0, b)$ ottenendo quindi che su tale intervallo
$\vec u$ è limitata. Sia $M$ il massimo di $\abs{\vec u}$ su $[x_0,b)$
e consideriamo l'insieme $K=[a,b]\times\overline{B_M(0)} = \{(x,\vec
y)\in \RR \times \RR^n\colon x\in [a,b], \abs{\vec y}\le
M\}$. Chiaramente $K\subset \Omega$ e $K$ è chiuso e limitato, quindi
per la proposizione\ref{prop:edo_massimale} il grafico di $\vec u$
dovrebbe uscire da $K$ da destra, cosa che invece non fa in quanto
$\abs{\vec u(t)}\le M$ per ogni $t\in [a,b]$.
\end{proof}

Il problema di Cauchy per le equazioni di ordine $n$ è il problema di determinare la soluzione di una equazione differenziale ordinaria di ordine $n$
in forma normale accoppiato ad una condizione iniziale per il valore della funzione e di tutte le sue derivate fino all'ordine $n$. Dato $\Omega\subset \RR\times \RR^n$ aperto, $f\in C^0(\Omega)$, $\vec y = (y_1, \dots, y_n) \in \RR^n$
si tratta quindi di trovare un intervallo $I\subset \RR$ e una funzione $u\in C^n(I)$ che soddisfi le seguenti condizioni:
\begin{equation}\label{eq:problema_cauchy_ordine_n}
  \begin{cases}
    u^{(n)}(x) = f(x,u(x), u'(x), \dots, u^{(n-1)}(x))\\
    u(x_0) = y_1 \\
    u'(x_0) = y_2 \\
    \ \vdots \\
    u^{(n-1)}(x_0) = y_n.
  \end{cases}
\end{equation}

\begin{theorem}[esistenza e unicità per le equazioni di ordine $n$]
\label{th:cauchy_lipschitz_ordine_n}
Se $f\colon \Omega\to \RR$ soddisfa le ipotesi del teorema di esistenza e unicità per i sistemi del primo ordine (cioè $f$ è continua e lipschitziana nelle ultime $n-1$ variabili uniformemente rispetto alla prima) allora il problema di Cauchy~\eqref{eq:problema_cauchy_ordine_n} ammette una unica soluzione locale. Esiste cioè un $\delta_0>0$ tale che per ogni $\delta < \delta_0$ posto $I_\delta = [x_0-\delta,x_0+\delta]$ esiste una unica $u\in C^n(I_\delta)$ che soddisfa~\eqref{eq:problema_cauchy_ordine_n}.

Se poi $\Omega$ è della forma $\Omega = I \times \RR^n$ con $I\subset \RR$ intervallo aperto e se
$f$ è anche sublineare (comme nelle ipotesi di esistenza globale) allora il problema di Cauchy~\ref{eq:problema_cauchy_ordine_n} ammette una unica soluzione definita su tutto $I$.
\end{theorem}
%
\begin{proof}
Se $u\in C^n$ è una funzione scalare possiamo considerare la funzione vettoriale $\vec u \in C^1$ le cui componenti sono $u$ e le sue prime $n-1$ derivate:
\[
\vec u(x) = (u(x), u'(x), \dots, u^{(n-1)}(x))
\]
ovvero $u_k(x) = u^{(k-1)}(x)$ per $k=1,\dots ,n$ essendo $\vec u(x) = (u_1(x), \dots, u_n(x))$.

Con questa trasformazione il problema~\eqref{eq:problema_cauchy_ordine_n} si può scrivere nella forma:
\[
 \begin{cases}
   u_1'(x) = u_2 \\
   u_2'(x) = u_3 \\
   \ \vdots \\
   u_{n-1}'(x) = u_n \\
   u_n'(x) = f(x, u_1(x), u_2(x), \dots, u_n(x))\\\\
   u_1(x_0) = y_1\\
   \ \vdots \\
   u_{n}(x_0) = y_n
 \end{cases}
\]
ovvero posto $\vec f(x,\vec y) = (y_2, y_3, \dots, y_n, f(x, \vec y))$ abbiamo una funzione vettoriale $\vec f\colon \Omega \to \RR^n$ e il problema~\eqref{eq:problema_cauchy_ordine_n} risulta equivalente a
\begin{equation}\label{eq:437583}
  \begin{cases}
   \vec u'(x) = \vec f(x,\vec u(x))\\
   \vec u(x_0) = \vec y.
  \end{cases}
\end{equation}
Visto che $f$ è continua, anche $\vec f$ risulta continua.
Verifichiamo se $\vec f$ soddisfa la condizione di Lipschitz.
Per ipotesi $f$ la soddisfa, cioè
esiste $L>0$ tale che:
\[
  \abs{f(x,\vec y)-f(x,\vec z)} \le L\abs{\vec y - \vec z}.
\]
Ma allora si ha
\begin{align*}
\abs{\vec f(x,\vec y) - \vec f(x,\vec z)}
  &= \sqrt{\sum_{k=2}^n \abs{y_k-z_k}^2 + \abs{f(x,\vec y)-f(x,\vec z)}^2} \\
  &\le \sqrt{\abs{\vec y - \vec z}^2 + L^2 \abs{\vec y - \vec z}^2}
  = \sqrt{1+L^2}\cdot \abs{\vec y - \vec z}.
\end{align*}
Dunque la funzione $\vec f$ verifica le ipotesi del teorema di Cauchy\hyp{}Lipschitz: esiste dunque una soluzione $\vec u$ di tale problema in un opportuno intervallo centrato nel punto $x_0$.
Ponendo $u=u_1$ (la prima componente di $\vec u$)
si osserva che $u$ è di classe $C^n$.
Infatti sappiamo che $\vec u$ è di classe $C^1$
ed essendo
$u_1' = u_2$,
$u_2' = u_3$, \dots,
$u_{n-1}'=u_n$
ed essendo $u_n\in C^1$,
si scopre che $u=u_1$ è di classe $C^n$
ed è una soluzione del problema \eqref{eq:problema_cauchy_ordine_n}.
Anche l'unicità segue direttamente dall'equivalenza delle due formulazioni.

L'esistenza globale segue in maniera analoga dal teorema per i sistemi del primo ordine. Basti osservare che se la funzione $f$ soddisfa l'ipotesi di sublinearità anche $\vec f$ la soddisfa.
\end{proof}

\section{metodi risolutivi}

Una tipologia di equazioni differenziali che abbiamo già trattato
è data dalle equazioni della forma:
\[
   u'(x) = f(x).
\]
Banalmente l'insieme delle soluzioni è dato dalle primitive di $f$:
\[
  u(x) \in \int f(x)\, dx.
\]

Osserviamo che se $f$ è continua il problema di Cauchy associato
\[
  \begin{cases}
    u'(x) = f(x) \\
    u(x_0) = u_0
  \end{cases}
\]
ha una unica soluzione che si può scrivere nella forma:
\[
  u(x) = u_0 + \int_{x_0}^x f(t)\, dt
\]

\subsection{equazioni lineari del primo ordine}

Sono le equazioni del tipo:
\mymark{***}
\[
   u'(x) + a(x) u(x) = b(x).
\]
Per risolvere queste equazioni si cerca di ricondurre la somma al lato sinistro alla derivata di un prodotto.
Per fare ciò si considera una qualunque primitiva
$A(x) \in \int a(x)\, dx$ e si moltiplicano ambo i membri
per $e^{A(x)}$:
\[
  e^{A(x)} u'(x) + a(x) e^{A(x)} u(x) = b(x) e^{A(x)}
\]
essendo $A'(x) = a(x)$
si osserva che il lato sinistro è ora la derivata di un prodotto:
\[
  \enclose{e^{A(x)}u(x)}' = b(x) e^{A(x)}.
\]
Scelta una qualunque primitiva del lato destro
\[
  F(x) \in \int b(x) e^{A(x)}\, dx
\]
su ogni intervallo in cui $a(x)$ e $b(x)$ sono definite
si ha
\[
  e^{A(x)}u(x) = F(x) + c
\]
per qualche $c\in \RR$
in quanto $e^{A(x)}u(x)$ e $F(x)$ sono due primitive della stessa funzione.
Dunque
\[
  u(x) = e^{-A(x)}\enclose{F(x) + c}.
\]

Se $b(x)=0$ l'equazione è lineare omogenea, possiamo scegliere $F(x) = 0$ e quindi lo spazio delle soluzioni in questo caso
è dato da
\[
  u(x) = c e^{-A(x)}
\]
ed è quindi lo spazio vettoriale unidimensionale generato dalla funzione $e^{-A(x)}$.

Ogni soluzione della non omogenea si può scrivere come somma di una soluzione particolare
più una generica soluzione dell'equazione omogenea associata. Infatti:
\[
  u(x) = u_0(x) + c e^{-A(x)}.
\]
dove
\[
  u_0(x) = e^{-A(x)}F(x)
\]
è una particolare soluzione dell'equazione non omogenea.

Osserviamo che se $a(x)$ e $b(x)$ sono funzioni continue definite su uno stesso intervallo $I$, anche la soluzione è definita su tutto $I$. Si dirà quindi che la soluzione esiste \emph{globalmente}.

\begin{exercise}[autovettori dell'operatore derivata]
Fissato $\lambda \in \RR$ trovare tutte le soluzioni dell'equazione
\[
  u'(x) = \lambda u(x).
\]
\end{exercise}
%
\begin{proof}[Svolgimento]
Scriviamo l'equazione nella forma
\[
  u'(x) - \lambda u(x) = 0.
\]
Nelle notazioni precedenti abbiamo $a(x) = -\lambda$ e quindi possiamo scegliere $A(x) = -\lambda x \in \int a$.
Moltiplicando ambo i membri per $e^{-A(x)}$ si ottiene
\[
  e^{-\lambda x} u'(x) - \lambda e^{-\lambda x} u(x) = 0
\]
cioè
\[
 \enclose{e^{-\lambda x}\cdot u(x)}' = 0
\]
da cui su ogni intervallo in cui $u$ è definita esiste una costante $c$ tale che
\[
  e^{-\lambda x} u(x) = c
\]
ovvero
\[
  u(x) = c e^{\lambda x}.
\]
Abbiamo dunque trovato che le soluzioni sono definite su tutto $\RR$, una soluzione è $e^{\lambda x}$ e ogni altra soluzione è multiplo di questa.
\end{proof}

\begin{exercise}
\begin{enumerate}
\item
Trovare tutte le soluzioni dell'equazione differenziale
\[
  u'(x) - \frac{u(x)}{x} = x^2.
\]

\item
Trovare tutte le soluzioni dell'equazione differenziale
\[
  x u'(x) - u(x) = x^3.
\]
\end{enumerate}
\end{exercise}
\begin{proof}[Svolgimento]
La prima è una equazione lineare non omogenea del primo ordine in forma normale: $u'(x) + a(x) u(x) = b(x)$. Il fattore integrante è $e^{A(x)}$ con
\[
  A(x) \in \int a(x)\, dx = -\int \frac{1}{x}\, dx \ni - \ln \abs{x}.
\]
Dunque $e^{A(x)} = 1/\abs{x}$. Dovremmo dunque dividere ambo i membri dell'equazione per $\abs{x}$. Osserviamo che l'equazione non è definita per $x=0$ e possiamo dunque distinguere i casi $x>0$ e $x<0$. Decidiamo quindi, per semplicità, di cambiare segno all'equazione per $x<0$ cosicché possiamo dividere per $x$ invece che per $\abs{x}$. Si ottiene dunque:
\[
  \frac{u'}{x} - \frac{u}{x^2} = x
\]
cioè
\[
  \enclose{u\cdot \frac{1}{x}}'  = x
\]
da cui
\[
  \frac{u}{x} \in \int x\, dx \ni \frac{x^2}{2}.
\]
Dunque la funzione $u(x)/x$ differisce da $x^2/2$ per una costante su ognuno dei due intervalli $x>0$ e $x<0$. Su ognuno dei due intervalli si ha dunque:
\[
  \frac{u(x)}{x} = \frac{x^2}{2} + c
\]
da cui
\[
  u(x) = \frac{x^3}{2} + c x
\]
per qualche $c\in \RR$. Per come è stato posto il problema, la soluzione non deve essere definita per $x=0$ e la costante $c$ può essere quindi diversa se $x>0$ o $x<0$.

La seconda equazione è equivalente alla prima se $x\neq 0$. Ma non è in forma normale e le soluzioni
potranno essere definite anche per $x=0$.
Si avrà quindi
\[
  u(x) = \frac{x^3}{2} + c x
\]
come prima ma affinché la funzione sia derivabile in $x=0$ la costante $c$ dovrà essere uguale per $x>0$ e per $x<0$.

Si osservi che ogni soluzione soddisfa la condizione iniziale $u(0) = 0$ e che quindi nessuna soluzione soddisfa la condizione $u(0)= q$ se $q \neq 0$.
\end{proof}

\begin{exercise}
Risolvere l'equazione differenziale:
\[
 u'(x) + \frac{u(x)}{(1+x^2)\arctg x} = 1.
\]
\end{exercise}
%
\begin{proof}
Osserviamo che l'equazione è definita solo per $x\neq 0$. Cercheremo quindi le soluzioni sui due intervalli $x<0$ e $x>0$.
Moltiplicando ambo i membri dell'equazione per $\arctg x$ si ottiene
\[
  \arctg x \cdot u'(x) +\frac{1}{1+x^2} u(x) = \arctg x
\]
cioè:
\[
  \enclose{\arctg x \cdot u(x)}' = \arctg x
\]
da cui
\[
  \arctg x \cdot u(x) \in \int \arctg x\, dx \ni x \arctg x - \frac {1}{2}\ln(1+x^2).
\]
Dunque su ogni intervallo su cui la soluzione è definita esisterà
$c\in \RR$ tale che
\[
  \arctg x \cdot u(x) = x \arctg x - \frac{1}{2}\ln(1+x^2) + c
\]
da cui essendo $x\neq 0$ si può dividere per $\arctg x$ e ottenere
\[
  u(x) = x - \frac{\ln(1+x^2)}{2\arctg x} + \frac{c}{\arctg x}.
\]
Abbiamo quindi una famiglia di soluzioni definite per $x<0$ e una famiglia di soluzioni definite per $x>0$.
\end{proof}

\subsection{equazioni a variabili separabili}

Si chiamano equazioni a variabili separabili le
equazioni del tipo:
\mymark{***}
\begin{equation}\label{eq:edo_separabile}
  u'(x) = f(x) \cdot g(u(x)).
\end{equation}
Questa è una equazioni del primo ordine in forma normale:
\[
  u'(x) = F(x, u(x))
\]
dove nella funzione $F$ risulta possibile separare le variabili $x$ e $u$ in un prodotto:
\[
  F(x, u) = f(x)\cdot g(u).
\]

Se $u$ è una soluzione dell'equazione~\eqref{eq:edo_separabile}
e se $x$ è un punto in cui $g(u(x))\neq 0$, possiamo dividere ambo i membri dell'equazione per $g(u(x))$ per ottenere:
\[
  \frac{u'(x)}{g(u(x))} = f(x).
\]
Vogliamo ora scrivere il lato sinistro come la derivata della funzione composta. Se scegliamo una primitiva di $1/g$:
\[
  H(u) \in \int \frac{1}{g(u)}\, du
\]
si osserva che
\[
  \enclose{H(u(x))}' = H'(u(x))u'(x) = \frac{u'(x)}{g(u(x))} = f(x).
\]
dunque se $F\in \int f$, su ogni intervallo in cui $g(u(x))\neq 0$ dovrà esistere $c\in \RR$ tale che
\[
  H(u(x)) = F(x) + c.
\]
Se supponiamo inoltre che $H$ sia invertibile si avrà:
\[
  u(x) = H^{-1}(F(x)+ c).
\]

\begin{example}
Risolviamo l'equazione
\begin{equation}\label{eq:43856}
  u'(x) = x u^2(x) + x.
\end{equation}
E' una equazione del primo ordine in forma normale.
Raccogliendo $x$ al lato destro si ottiene una equazione a variabili separabili.
Dividendo ambo i membri per $u^2(x)+1$ (che è sempre diverso da zero) si ottiene l'equazione equivalente
\[
\frac{u'(x)}{1+u^2(x)} = x.
\]
Integrando il lato sinistro si ottiene:
\[
  \int \frac{u'(x)}{1+u^2(x)}\, dx
  = \Enclose{\int \frac{du}{1+u^2}}_{u=u(x)}
  \ni \arctg(u(x))
\]
mentre per il lato destro si ottiene
\[
  \int x\, dx \ni \frac{x^2}{2}.
\]
Dunque su ogni intervallo si deve avere
\[
  \arctg(u(x)) = \frac{x^2}{2} + c.
\]
Visto che l'arcotangente assume valori compresi tra $-\pi/2$ e $\pi/2$ anche il lato destro dovrà rimanere in tale intervallo. Dovrà quindi essere:
\begin{equation}\label{eq:4856}
  -\pi < x^2 + 2c < \pi.
\end{equation}
Con questa condizione possiamo invertire l'arcotangente ottenendo finalmente una espressione per la soluzione:
\[
  u(x) = \tg\enclose{\frac{x^2}{2} + c}.
\]

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{fig43856.png}
\label{fig:43856}
\caption{i grafici delle soluzioni dell'equazione differenziale~\eqref{eq:43856}}.
\end{figure}

Esplicitando la condizione~\eqref{eq:4856} si osserva che per $c>-\pi/2$ la soluzione è definita su un intervallo aperto centrato in $x=0$ mentre per $c\le -\pi/2$ la soluzione può essere definita su due intervalli simmetrici e l'ampiezza di tali intervalli si riduce tendendo a zero quando $c\to -\infty$.
Agli estremi di tali intervalli la soluzione ha degli asintoti verticali.

Significa dunque che anche le soluzioni massimali possono essere definite su intervalli arbitrariamente piccoli coerentemente con quanto affermato nel teorema di esistenza e unicità locale.
\end{example}

\begin{example}
Si voglia risolvere l'equazione
\[
  u'(x) = u^2(x).
\]
Si tratta di una equazione in forma normale, del primo ordine, autonoma. In particolare è a variabili separabili $u'(x) = f(u(x))\cdot g(x)$ con $f(u)=u^2$ e $g(x)=1$.
Osserviamo innanzitutto che $u(x) = 0$ è soluzione in quanto $u'(x) = u^2(x) = 0$. Se $u$ è una soluzione non identicamente nulla ci saranno dei punti in cui $u(x)\neq 0$. Nei punti in cui $u(x)\neq 0$ possiamo dividere ambo i membri dell'equazione per $u^2(x)$ ottenendo:
\[
  \frac{u'(x)}{u^2(x)} = 1.
\]
Osserviamo ora che, tramite cambio di variabile $u=u(x)$, $du=u'(x)\, dx$ si ottiene:
\[
  \int \frac{u'(x)}{u^2(x)}\, dx = \Enclose{\int \frac{du}{u^2}}_{u=u(x)} = \Enclose{-\frac{1}{u}}_{u=u(x)}
  = -\frac{1}{u(x)}
\]
mentre
\[
  \int 1 \, dx \ni x
\]
dunque su ogni intervallo in cui $u(x)\neq 0$ deve esistere una costante $c\in \RR$ tale che
\[
-\frac{1}{u(x)} = x + c
\]
ovvero, ponendo $x_0 = -c$
\begin{equation}\label{eq:5782196}
  u(x) = -\frac{1}{x+c} = \frac{1}{x_0-x}.
\end{equation}

Ci chiediamo ora se è possibile che una soluzione $u(x)$ possa avere sia dei punti in cui si annulla sia dei punti in cui non si annulla. La risposta è no, e può essere ottenuta in due modi diversi.

Giustificazione algebrica: sia $u(x)$ una soluzione qualunque, definita su un intervallo $I$ e supponiamo che ci sia un punto $x_0$ tale che $u(x_0)\neq 0$. Prendiamo il più grande intervallo $J\subset I$ contenente $x_0$ e tale che $u(x)\neq 0$ su $J$.
Tale intervallo deve essere aperto, perché se $u(x)\neq 0$ in un estremo dell'intervallo allora per continuità $u(x)$ sarebbe diverso da $0$ in un intorno di tale estremo e potrei quindi allargare l'intervallo. Dunque $J=(a,b)$ e se fosse $J\neq I$ si avrebbe che uno dei due estremi, diciamo $a$, è in $I$ e $u(a)=0$ (altrimenti potrei aggiungere l'estremo a $J$ che quindi non sarebbe massimale). Ma sempre per la continuità di $u$ si avrebbe $u(x) \to 0$ per $x\to a$ e $x\in J$ mentre nell'equazione~\eqref{eq:5782196} vediamo che $u(x)\to 0$ è possibile solamente se $x\to +\infty$ o $\to -\infty$ che non sono certamente punti di $I$.

Giustificazione analitica: soluzioni diverse non possono avere punti in comune in quanto la funzione $f(y)=y^2$ soddisfa le ipotesi del teorema di esistenza e unicità locale e quindi si applica la proposizione~\ref{prop:separazione_soluzioni} (separazione delle soluzioni).
\end{example}

\section{equazioni lineari di ordine $n$}

\index{equazione!differenziale!lineari di ordine $n$}
\mynote{equazioni lineari di ordine $n$}
Le equazioni differenziali ordinarie lineari di ordine $n$ in forma normale possono essere scritte nella forma:
\begin{equation}\label{eq:edo_lineare_ordine_n}
  u^{(n)}(x) + a_{n-1}(x) u^{(n-1)}(x) + \dots + a_1(x) u'(x) + a_0 u(x) = b(x)
\end{equation}
con $a_k\colon A \to \RR$, $b\colon A\to \RR$ funzioni continue definite su uno stesso dominio $A\subset \RR$.
Nel caso $b(x) = 0$ l'equazione dice essere \myemph{omogenea}
e si può scrivere come:
\begin{equation}\label{eq:edo_lineare_omogenea_ordine_n}
  u^{(n)}(x) + a_{n-1}(x) u^{(n-1)}(x) + \dots + a_1(x) u'(x) + a_0 u(x) = 0.
\end{equation}
In generale l'equazione \eqref{eq:edo_lineare_ordine_n}
viene chiamata \emph{equazione non omogenea}
\mynote{equazione non omogenea}
\index{equazione!differenziale!non omogenea}
e la corrispondente equazione~\eqref{eq:edo_lineare_omogenea_ordine_n}
viene chiamata \emph{equazione omogenea associata}.
\mynote{equazione omogenea associata}
\index{equazione!differenziale!omogenea associata}

\begin{theorem}[struttura delle soluzioni di una equazione lineare]
\label{th:edo_lineare_ordine_n}
\mymark{***}
Siano $a_k\in C^0(I)$ con $I\subset \RR$
un intervallo aperto.

\begin{enumerate}
\item
L'insieme $V$ delle soluzioni dell'equazione lineare omogenea~\eqref{eq:edo_lineare_omogenea_ordine_n}
è un sottospazio vettoriale di $C^n(I)$ di dimensione $n$.
Inoltre, fissato un punto qualunque $x_0\in I$ l'operatore $J\colon V \to \RR^n$ (chiamato \myemph{Jet}) definito da
\[
  J(u) = (u(x_0), u'(x_0), u''(x_0), \dots, u^{(n-1)}(x_0))
\]
è un operatore lineare bigettivo (cioè un isomorfismo di spazi vettoriali).

\item
L'insieme delle soluzioni dell'equazione non omogenea~\eqref{eq:edo_lineare_ordine_n} è un sottospazio affine di $C^n(A)$ di dimensione $n$, parallelo al sottospazio delle soluzioni dell'equazione omogenea associata~\eqref{eq:edo_lineare_omogenea_ordine_n}. In particolare se $u_0$ è una soluzione particolare dell'equazione non omogenea~\eqref{eq:edo_lineare_ordine_n} ogni altra soluzione $u$ di \eqref{eq:edo_lineare_ordine_n} si scrive nella forma
\[
  u = u_0 + v
\]
con $v$ soluzione dell'equazione omogenea associata.
\end{enumerate}
\end{theorem}
%
\begin{proof}
\mymark{***}
Innanzitutto il teorema~\ref{th:edo_esistenza_globale} di esistenza globale garantisce che
le soluzioni delle equazioni~\eqref{eq:edo_lineare_ordine_n}
e~\eqref{eq:edo_lineare_omogenea_ordine_n} esistono e sono funzioni in $C^n(I)$.

Possiamo riscrivere l'equazione~\eqref{eq:edo_lineare_ordine_n} nella forma
\[
  Lu = b
\]
con
\[
  Lu = u^{(n)} + \sum_{k=0}^{n-1} a_k u^{(k)}
\]

L'equazione omogenea~\eqref{eq:edo_lineare_omogenea_ordine_n}
risulta quindi essere
\[
  Lu = 0.
\]
Si osservi che $L\colon C^n(I) \to C^0(I)$ è un operatore lineare in quanto la somma, la derivata e la moltiplicazione per una funzione sono operatori lineari sullo spazio vettoriale delle funzioni.
Dunque l'insieme $V$ delle soluzioni dell'equazione omogenea non è altro che $\ker L$ che notoriamente è uno spazio vettoriale. Cerchiamo ora di determinare la dimensione di tale spazio, mettendo in corrispondenza le soluzioni dell'equazione con un loro dato iniziale.

Fissato un punto $x_0\in I$ consideriamo l'applicazione $J\colon V \to \RR^n$ che ad ogni $u\in V = \ker L$ associa il vettore (chiamato \emph{jet})
\[
  J(u) = (u(x_0), u'(x_0), \dots, u^{(n-1)}(x_0)) \in \RR^n.
\]
Chiaramente $J$ è lineare perché l'operatore derivata e la valutazione in un punto sono operatori lineari. Osserviamo che $J$ è suriettivo perché dato un qualunque $\vec y\in \RR^n$ per il teorema \ref{th:cauchy_lipschitz_ordine_n} di esistenza (globale) di soluzioni per il problema di Cauchy di ordine $n$ sappiamo esistere una soluzione $u\in V$ tale che $J(u)=\vec y$. Ma $J$ è anche iniettivo perché se $u,v\in V$ sono due soluzioni con $J(u)=J(v)$ significa che $u$ e $v$ verificano lo stesso problema di Cauchy. Per l'unicità della soluzione risulta quindi $u=v$. Abbiamo quindi mostrato che $J\colon V \to \RR^n$ è un isomorfismo di spazi vettoriali, quindi $\dim V=n$.

Per quanto riguarda l'equazione non omogenea
sia $W = \{ v\in C^n(A)\colon L v = b\}$ l'insieme di tutte le soluzioni. Se consideriamo una soluzione particolare $v_0\in W$ e se $v\in W$ è una qualunque altra soluzione, si osserva che
\[
  L(v-v_0) = L(v) - L(v_0) = b-b = 0.
\]
Significa che $u=v-v_0$ è soluzione dell'equazione omogenea associata: $u\in V=\ker L$. Dunque ogni soluzione $v$ dell'equazione non omogenea si può scrivere nella forma $v = v_0 + u$ con $v_0$ soluzione particolare della non omogenea e $u$ soluzione generale dell'equazione omogenea associata ovvero
\[
  W = v_0 + V.
\]
\end{proof}

\begin{theorem}[maggiore regolarità delle soluzioni]
Se $u(x)$ è una soluzione dell'equazione differenziale lineare~\eqref{eq:edo_lineare_ordine_n} e se i coefficienti $a_1, \dots, a_{n-1}, b$ sono funzioni di classe $C^m$ per un certo $m\in \NN$ allora la soluzione è di classe $C^{m+n}$. In particolare se i coefficienti sono di classe $C^\infty$ le soluzioni sono anch'esse di classe $C^\infty$.
\end{theorem}
%
\begin{proof}
Se $u$ è soluzione di~\eqref{eq:edo_lineare_ordine_n} possiamo scrivere l'equazione in forma normale per ottenere:
\[
  u^{(n)}(x) = b(x) - \sum_{k=0}^{n-1}a_k(x) u^{(k)}(x).
\]
Essendo $a_k \in C^0$ sappiamo che per lo meno $u$ è di classe $C^n$. Supponiamo sia $u\in C^j$ per qualche $j\ge n$. I coefficienti dell'equazione sono di classe $C^m$ e vengono moltiplicati per le derivate di $u$ che sono almeno di classe $C^{j-n+1}$. Se $j-n+1 \le m$ allora il lato destro della precedente equazione è di classe $j-n+1$. Dunque $u^{(n)}\in C^{j-n+1}$ da cui $u\in C^{j+1}$. Un passo alla volta è quindi possibile incrementare la regolarità di $u\in C^j$ finché $j-n+1\le m$ cioè finché $j\le m+n-1$. A quel punto otteniamo $u\in C^{j+1} = C^{m+n}$.

Se i coefficienti sono di classe $C^\infty$ il procedimento non termina mai e si ottiene dunque che anche $u$ è di classe $C^\infty$.
\end{proof}

\section{equazioni lineari di ordine $n$ a coefficienti costanti}

Una equazione differenziale ordinaria di ordine $n$ a coefficienti costanti è una equazione del tipo:
\index{equazione!differenziale!lineare!a coefficienti costanti}
\mymark{***}
\begin{equation}\label{eq:edo_lineare_omogenea_coeff_costanti}
   \sum_{k=0}^n a_k u^{(k)}(x) = 0
\end{equation}
con $a_k\in \RR$.
Senza perdita di generalità possiamo supporre che sia $a_n\neq 0$ cosicchè tale equazione può essere scritta in forma normale e rientra nella casistica generale che abbiamo considerato nel capitolo precedente.
In particolare sappiamo che lo spazio delle soluzioni è uno spazio vettoriale di dimensione $n$.
Osserviamo che il teorema di esistenza e unicità globale garantisce che le soluzioni dell'equazione differenziale lineare a coefficienti costanti siano funzioni di classe $C^n$ definite su tutto $\RR$. Ma i coefficienti costanti
sono di classe $C^\infty$ dunque la maggiore regolarità delle soluzioni ci dice, in questo caso, che le soluzioni dovranno essere funzioni di classe $C^\infty$.

Per motivi puramente algebrici sarà utile considerare le funzioni a valori complessi.
Ricordiamo che se $u(x)$ è una funzione a valori complessi,
allora si può scrivere $u(x) = f(x) + i g(x)$ con $f$ e $g$ funzioni a valori reali.
Si definisce allora $u'(x) = f'(x) + ig'(x)$.
Denotiamo con $D \colon C^\infty(\RR, \CC)\to C^\infty(\RR, \CC)$ l'operatore derivata: $D u = u'$.

Se $P$ è un polinomio di grado $n$:
\[
  P(z) = \sum_{k=0}^n a_k z^k
\]
possiamo definire l'operatore
$P(D)\colon C^\infty(\RR,\CC) \to C^\infty(\RR,\CC)$
come
\[
  P(D)u = \sum_{k=0}^n a_k D^k u = \sum_{k=0}^n a_k u^{(k)}.
\]

In particolare l'equazione lineare omogenea a coefficienti costanti~\eqref{eq:edo_lineare_omogenea_coeff_costanti}
si può scrivere più espressivamente nella forma
\[
  P(D)u = 0
\]
Se l'equazione è in forma normale si avrà $\deg P = n$ (in quanto il cofficiente $a_n$ del termine di grado massimo è $1$ per le equazioni in forma normale).

Vogliamo ora mostrare come una decomposizione del polinomio porta ad una decomposizione delle soluzioni dell'equazione differenziale.

\begin{theorem}[isomorfismo tra polinomi e operatori differenziali a coefficienti costanti]
\label{th:6822095}
Siano $P$ e $Q$ due polinomi e sia $\lambda \in \RR$.
Allora si ha
\[
  (P\cdot Q)(D) = P(D) \circ Q(D)
\]
cioè: l'operatore differenziale associato al prodotto dei polinomi è la composizione degli operatori associati ai singoli fattori.
\end{theorem}
\begin{proof}
La dimostrazione si basa sul fatto che la composizione degli operatori (in questo caso l'operatore derivazione $D$) soddisfa le stesse regole formali del prodotto.

Sia
\[
  P(z) = \sum_{k=0}^n a_k z^k, \qquad Q(z) = \sum_{j=0}^m b_j z^j.
\]
Allora si ha
\begin{align*}
 (P\cdot Q)(z)
  &= \enclose{\sum_{k=0}^n a_k z^k Q(t)} \cdot \enclose {\sum_{j=0}^m b_j z^j} \\
  &= \sum_{k=0}^n \sum_{j=0}^m a_k b_j z^{k+j} \\
  &= \sum_{s=0}^{n+m} \enclose{\sum_{k=\max\{0,s-m\}}^{\min\{s,n\}} a_k b_{s-k}} z^s.
\end{align*}
Ma, d'altro canto, se $u\in C^\infty(\RR,\CC)$ si ha
\begin{align*}
  (P(D) \circ Q(D)) u &= P(D)Q(D)u \\
  &= \sum_{k=0}^n a_k D^k\enclose{\sum_{j=0}^m b_j D^j u}
  = \sum_{k=0}^n \sum_{j=0}^m a_k b_j D^{k+j} u \\
  &= \sum_{s=0}^{m+n} \enclose{\sum_{k=\max\{0,s-m\}}^{\min\{s,n\}} a_k b_{s-k}} D^s u \\
  &= (P\cdot Q)(D)u.
\end{align*}
\end{proof}

\begin{theorem}
\mymark{***}
Sia $P$ un polinomio e sia $\lambda\in \CC$ una radice di $P$ con molteplicità $m$. Allora se $p(x)$ è un polinomio (a coefficienti complessi) di grado inferiore a $m$, la funzione $u(x) = p(x) e^{\lambda x}$ è soluzione (complessa) dell'equazione differenziale
\[
   P(D) u = 0.
\]
\end{theorem}
%
\begin{proof}
\mymark{***}
Se il polinomio $P(t)$ ha una radice $\lambda$ con molteplicità $m$ significa che $P(t)$ è divisibile per $(t-\lambda)^m$ cioè esiste un polinomio $R(t)$ tale che
\[
  P(t) = R(t)\cdot (t-\lambda)^m.
\]

Ma allora, in base al teorema~\ref{th:6822095} possiamo decomporre anche l'equazione differenziale:
\[
 P(D) u = R(D) (D - \lambda)^m u.
\]
Se $u(x) = p(x) e^{\lambda x}$ si ha
\begin{align*}
  (D-\lambda) u(x) &= Du(x) -\lambda u(x) =
  p'(x) e^{\lambda x} + p(x) \lambda e^{\lambda x} - \lambda p(x) e^{\lambda x} \\
  &= p'(x) e^{\lambda x},
\end{align*}
da cui
\[
  (D-\lambda)^m u(x) = p^{(m)}(x) e^{\lambda x}.
\]
Visto che la derivata di un polinomio è un polinomio di grado inferiore, se il polinomio $p$ ha grado inferiore a $m$ risulta che $p^{(m)}=0$. Dunque, come volevamo dimostrare,
\[
 P(D) u = R(D) (D-\lambda)^m u = R(D) 0 = 0.
\]
\end{proof}

\begin{theorem}[indipendenza delle soluzioni fondamentali complesse]
\mymark{***}
Il sottoinsieme $\B$ di $\C^\infty(\RR, \CC)$ dato dalle funzioni $u(x)$ della forma
\[
   u(x) = x^m e^{\lambda x}
\]
con $m\in \NN$, $\lambda \in \CC$ è un insieme linearmente indipendente nello spazio vettoriale $C^\infty(\RR,\CC)$ sul campo $\CC$.
\end{theorem}
%
\begin{proof}
Si tratta di mostrare che se una combinazione lineare finita di tali funzioni è identicamente nulla, allora tutti i coefficienti sono nulli.

Supponiamo per assurdo che esistano
$u_1, \dots, u_N\in \B$
\[
  u_k(x) = x^{m_k} e^{\lambda_k x}, \qquad k=1, \dots, N
\]
tali che
\[
\sum_{k=1}^N c_k x^{m_k} e^{\lambda_k x} = 0
  \qquad \forall x \in \RR
\]
per una qualche scelta di coefficienti $c_k \in \CC$ non tutti nulli.
Sommando assieme i monomi che moltiplicano gli esponenziali con lo stesso coefficiente, potremo riscrivere la relazione precedente nella forma:
\begin{equation}\label{eq:4656978}
  \sum_{j=1}^M P_j(x) e^{\lambda_j x} = 0 \qquad \forall x\in \RR
\end{equation}
con i coefficienti $\lambda_j$ tutti diversi tra loro e con $P_j$ polinomi non nulli.

Abbiamo già osservato che $(D-\lambda)P(x)e^{\lambda x}= P'(x)e^{\lambda x}$ mentre se $\lambda\neq \mu$ si ha $(D-\lambda)P(x) e^{\mu x}= Q(x) e^{\mu x}$ con $Q$ polinomio dello stesso grado di $P$.
Posto $m_j = \deg P_j$ applichiamo all'equazione~\eqref{eq:4656978} l'operatore $(D-\lambda_1)^{m_1}$.
Quello che si ottiene è:
\begin{equation}\label{eq:4656979}
  k_1 e^{\lambda_1 x} + \sum_{j=2}^M Q_j(x) e^{\lambda_j x} = 0 \qquad \forall x\in \RR
\end{equation}
dove $k_1$ è la derivata $m_1$-esima del polinomio $P_1$. Visto che $P_1$ è un polinomio di grado $m_1$ la sua derivata $m_1$-esima è un polinomio di grado $0$ non nullo, dunque è una costante $k_1 \neq 0$. I polinomi $Q_j$ hanno invece lo stesso grado dei $P_j$ perché abbiamo visto che se $\lambda\neq \mu$ il grado del polinomio non cambia.

Ora applichiamo in sequenza all'equazione~\eqref{eq:4656979} gli operatori $(D-\lambda_j)^{m_j+1}$ per $j=2,\dots, M$. Applicando tali operatori il coefficiente $k_1$ cambierà, ma rimarrà comunque diverso da zero (chiamiamolo $k\neq 0$) mentre tutti gli altri polinomi verranno annullati, perché i rispettivi polinomi vengono derivati una volta in più del loro grado. Si otterrà quindi:
\[
  k e^{\lambda_1 x} = 0 \qquad \forall x\in \RR.
\]
Ma questo è assurdo perché per $x=0$ si ottiene $k=0$, che abbiamo escluso.
\end{proof}

I risultati precedenti ci permettono di determinare tutte le soluzioni reali delle equazioni lineari omogenee a coefficienti costanti come nel seguente esempio.

\begin{example}
Si determinino tutte le soluzioni dell'equazione differenziale
\[
  u^{(5)}(x) + 2 u'''(x) + u'(x) = 0.
\]
\end{example}
%
\begin{proof}[Svolgimento]
L'equazione può essere scritta nella forma
\[
  P(D) u = 0
\]
con $P(t) = t^5 + 2 t^3+t$. Possiamo fattorizzare il polinomio $P$ nel campo complesso:
\[
t^5 + 2t^3 +t = t(t^2+1)^2 = t (t+i)^2(t-i)^2.
\]
Il polinomio ha una radice $\lambda_0=0$ con molteplicità uno e due radici complesse coniugate $\lambda_1 = i$, $\lambda_2=-i$ con molteplicità due.
Risulta quindi che le seguenti funzioni devono essere soluzione complesse dell'equazione:
\[
  1, \qquad
  e^{ix}, \qquad
  x e^{ix}, \qquad
  e^{-ix}, \qquad
  x e^{-ix}.
\]
Per il teorema precedente sappiamo che queste funzioni sono indipendenti.
Tramite la formula di Eulero possiamo scrivere:
\[
  \cos x = \frac{e^{ix}+e^{-ix}}{2},
  \qquad
  \sin x = \frac{e^{ix}-e^{-ix}}{2i}
\]
da cui si ottiene che le funzioni
\[
 1, \qquad
 \cos x, \qquad
 x \cos x, \qquad
 \sin x, \qquad
 x \sin x
\]
sono combinazioni lineari delle precedenti, e quindi anch'esse devono essere soluzioni dell'equazione.
Inoltre anch'esse sono funzioni indipendenti in quanto il cambio di base dato dalle formule di Eulero è invertibile.
Visto che queste funzioni sono $5$ funzioni linearmente indipendenti, lo spazio generato ha dimensione $5$, come l'ordine dell'equazione.
Sappiamo però che anche lo spazio di tutte le soluzioni ha dimensione pari all'ordine dell'equazione, dunque lo spazio che abbiamo determinato esaurisce tutte le soluzioni. Ogni soluzione reale si potrà dunque scrivere nella forma:
\[
  u(x) = c_1 + c_2 \cos x + c_3 \sin x + c_4 x \cos x + c_5 x \sin x
\]
con $c_1, c_2, \dots, c_5 \in \RR$ costanti arbitrarie.
\end{proof}

Possiamo in effetti dare un enunciato generale.

\begin{theorem}[soluzioni dell'equazione lineare omogenea a coefficienti costanti]
\mymark{**}
Se $P(t)$ è un polinomio a coefficienti reali.
Ogni soluzione $u\colon \RR\to \RR$ dell'equazione differenziale
\[
  P(D) u = 0
\]
si scrive nella forma
\begin{equation}
\label{eq:4725549774}
  u(x) = \sum_{k=1}^N \sum_{j=1}^{n_k} c_{kj} x^j e^{\lambda_k x}
        + \sum_{k=1}^M \sum_{j=1}^{m_l} x^j e^{\alpha_k x} (a_{kj} \cos(\beta_k x) + b_{kj}\sin(\beta_k x))
\end{equation}
dove $\lambda_1, \dots, \lambda_N$ sono le radici reali del polinomio $P(t)$, $n_1, \dots, n_N$ sono le rispettive molteplicità,
$\alpha_k \pm i \beta_k$ sono le radici complesse coniugate (non reali) del polinomio $P$ con $k=1,\dots, M$ ognuna con molteplicità $m_k$ e infine $c_{kj}, a_{kj}, b_{kj}\in \RR$ sono costanti arbitrarie.
\end{theorem}
%
\begin{proof}
Sappiamo che le funzioni complesse
\begin{equation}\label{eq:739647}
  x^j e^{\lambda_k x}, \qquad
  x^j e^{(\alpha_k+i\beta_k)x}
\end{equation}
sono soluzioni dell'equazione $j$ è inferiore alla molteplicità della corrispondente radice reale $\lambda_k$ o complessa $\alpha_k + i \beta_k$.
Dunque
\[
  u_{kj}(x) = x^j e^{\lambda_k x}
\]
è soluzione se $j<n_k$.
Per le radici complesse applichiamo la formula di Eulero:
\begin{align}\label{eq:49544467}
x^j e^{\alpha_k x}\cos (\beta_k x) &=
  \frac 1 2 x^j e^{(\alpha_k +i \beta_k)x}
  + \frac 1 2 x^j e^{(\alpha_k - i \beta_k)x}\\
x^j e^{\alpha_k x}\sin (\beta_k x) &=
    \frac 1 {2i} x^j e^{(\alpha_k +i \beta_k)x}
    - \frac 1 {2i} x^j e^{(\alpha_k - i \beta_k)x}
\end{align}
dunque anche le funzioni
\[
  v_{kj}(x) = x^j e^{\alpha_k x}\cos x, \qquad
  w_{kj}(x) = x^j e^{\alpha_k x}\sin x
\]
essendo combinazione lineare di soluzioni, sono anch'esse
soluzioni.
Queste soluzioni sono inoltre indipendenti in quanto
le funzioni in \eqref{eq:739647} lo sono (per il teorema precedente) e possono essere ricondotte alle \eqref{eq:49544467}
tramite la formula di Eulero.

Abbiamo dunque trovato un insieme formato da
\[
  n = \sum_{k=1}^N n_k + 2 \sum_{k=1}^M m_k
\]
soluzioni reali indipendenti. Il numero $n$ è pari alla somma delle molteplicità delle radici (reali e complesse) del polinomio $P$ e dunque, per il teorema fondamentale dell'algebra, coincide con il grado di $P$. Sappiamo allora che lo spazio delle soluzioni di $P(D)u=0$ ha dimensione $n$ e quindi abbiamo trovato una base di tutte le soluzioni reali. Significa che ogni soluzione si scrive nella forma~\eqref{eq:4725549774}.
\end{proof}


\begin{theorem}[metodo di similarità]
\mymark{***}
Consideriamo l'equazione non omogenea complessa
\begin{equation}\label{eq:3976734956}
 P(D) u(x) = q(x) e^{\lambda x}.
\end{equation}
con $P$ e $q$ polinomi a coefficienti complessi.
Sia $m$ la molteplicità di $\lambda$ come radice di $P$ ($m=0$ se $\lambda$ non è radice di $P$).
Allora una soluzione particolare di~\eqref{eq:3976734956} può essere scritta nella forma
\begin{equation}
\label{eq:945396}
u(x) = p(x) x^m e^{\lambda x}
\end{equation}
con $p$ un polinomio (incognito) di grado uguale al grado di $q$.
\end{theorem}
%
\begin{proof}
Se $u(x) = x^m p(x) e^{\lambda x}$ vogliamo capire come agisce l'operatore $P(D)$ su $u$, in particolare
dato un qualunque polinomio $q$ vogliamo capire
se esiste un polinomio $p$, dello stesso grado di $q$, tale che applicando $P(D)$ ad $u(x)$ si ottiene $q(x) e^{\lambda x}$.

Fattorizziamo il polinomio $P(z)$:
\[
  P(z) = (z-\lambda_1)^{m_1}\dots (z-\lambda_n)^{m_n}
\]
dove $\lambda_1, \dots, \lambda_n\in \CC$ sono le radici distinte di $P(z)$ e $m_1,\dots,m_n\in \NN$ sono le relative molteplicità. Dunque l'operatore $P(D)$ può essere fattorizzato nella forma corrispondente:
\[
   P(D) = (D-\lambda_1)^{m_1} \dots (D-\lambda_n)^{m_n}.
\]

Chiamiamo $V^n_m$ lo spazio vettoriale dei polinomi della forma $x^m p(x)$ con $\deg p<n$. Una base di $V^n_m$ sono i monomi $x^m, x^{m+1}, \dots, x^{m+n}$ e dunque $V^n_m$ è uno spazio vettoriale di dimensione $n$.

La dimostrazione del teorema si basa sull'ossevazione di come agisce l'operatore $(D-\lambda_k)^{m_k}$ sulle funzioni del tipo $u(x) = p(x) e^{\lambda x}$ con $p\in V^n_m$. In generale se $p$ è un polinomio si ha
\[
 (D-\lambda_k)(p(x)e^{\lambda x})
 = \Enclose{(\lambda -\lambda_k)p(x) + p'(x)} e^{\lambda x}.
\]
Dobbiamo allora distinguere due casi. Se $\lambda\neq \lambda_k$ allora la funzione $p(x) e^{\lambda x}$ viene trasformata nella funzione $q(x) e^{\lambda x}$ dove $q$ è un polinomio dello stesso grado di $p$, in quanto il termine di grado massimo di $p$ viene moltiplicato per $\lambda-\lambda_k$ mentre il polinomio $p'$ ha il grado inferiore al grado di $p$ e quindi non influenza il termine di grado massimo. Se chiamiamo $T_k\colon V^n_0 \to V^n_0$ l'operatore lineare che manda il polinomio $p(x)$ nel polinomio $q(x)=(\lambda-\lambda_k) p(x) + p'(x)$ osserviamo che $T_k$ è iniettivo in quanto, visto che $T_k$ preserva il grado del polinomio, l'unico polinomio che può andare a zero è il polinomio nullo. Dunque, per il teorema del rango, $T_k$ è bigettivo e di conseguenza $T_k^{m_k} \colon V^n_0 \to V^n_0$ è bigettivo.

Se invece $\lambda = \lambda_k$ osserviamo ora che l'operatore $T_k$ non è altro che la derivata $q(x) = p'(x)$. E se $m=m_k$ è la molteplicità di $\lambda$ come radice di $P$, l'operatore $T_k^{m_k}$ non è altro che la derivata $m$-esima. Tale operatore non è invertibile su $V^n_0$ in quanto manda a zero tutti i polinomi di grado inferiore a $m$. Ma se partiamo da un polinomio in $V^n_m$ allora l'operatore $T_k^m\colon V^n_m \to V^n_0$ diventa invertibile: infatti in $V^n_m$ ci sono polinomi della forma $a_m x^m + a_{m+1}x^{m+1} + \dots + a_{n+m}x^{n+m}$
e facendone la derivata $m$-esima si può ottenere il polinomio nullo solamente se tutti i coefficienti sono nulli.
Risulta quindi che, se $\lambda = \lambda_k$, l'operatore $T_k^m\colon V^n_m \to V^n_0$ è invertibile.

Siamo arrivati alla conclusione. Se prendiamo un polinomio $p(x)$ in $V^n_m$ e applichiamo $P(D)$ alla corrispondente funzione $u(x) =p(x) e^{\lambda x}$
possiamo applicare sequenzialmente gli operatori $(D-\lambda_k)^{m_k}$. Questi agiscono sulla funzione $u$ modificando il polinomio $p$.
Se $\lambda$ è radice di $P$ al primo passaggio applichiamo l'operatore $(D-\lambda_k)^{m_k}$ con $\lambda_k=\lambda$ e $m_k=m$ cosicché il polinomio $p\in V^n_m$ viene trasformato, in maniera biunivoca, in un polinomio di $V^n_0$. Dopodiché applico tutti gli altri operatori $(D-\lambda_k)^{m_k}$ con $\lambda_k \neq \lambda$ trasformando il polinomio di $V^n_0$ in un altro polinomio di $V^n_0$ ma sempre in maniera biunivoca. Ne risulta che alla fine ottengo una mappa biunivoca tra $V^n_m \to V^n_0$ e qualunque sia $q\in V^n_0$ sappiamo esister un $u\in V^n_m$ che viene mandato in $q$.
\end{proof}

\begin{remark}
Si noti che nella dimostrazione precedente abbiamo investigato la
struttura degli operatori differenziali $P(D)$ a coefficienti
costanti. Quello che abbiamo fatto è in realtà il processo di
decomposizione di Jordan di un operatore lineare. L'operatore $P(D)$
ha come autovettori le funzioni $e^{\lambda_k x}$ con $\lambda_k$
radici del polinomio $P$. Si osserva però che le funzioni della forma
$x^j e^{\lambda_k x}$ sono \emph{autovettori generalizzati}
in quanto iterando l'operatore $P(D)$ vengono mandati in un autovettore
e sono quindi una base dell'autospazio relativo all'autovettore
$\lambda_k$.

La base formata dalle funzione della forma $x^j e^{\lambda_j x}/j!$ è
una \emph{base a ventaglio} e la matrice che rappresenta l'operatore
$P(D)$ risulta essere una matrice a blocchi nella forma di Jordan: gli
autovalori sulla diagonale e una fila di $1$ al di sopra della
diagonale.
\end{remark}

\begin{definition}[wronksiano]
Siano $u_1, \dots, u_n$ funzioni di classe $C^{n-1}$.
La matrice
\begin{equation}\label{eq:wronksiano}
  W(x) =
    \begin{pmatrix}
    u_1(x) & u_2(x) & \dots & u_n(x) \\
    u_1'(x) & u_2'(x) & \dots & u_n'(x) \\
    \vdots & & & \vdots\\
    u_1^{(n-1)}(x) & u_2^{(n-1)}(x) & \dots & u_n^{(n-1)}(x)
    \end{pmatrix}
\end{equation}
è chiamata \myemph{matrice wronksiana}. Il determinante di tale matrice $w(x) = \det W(x)$ è chiamato \myemph{determinante wronksiano}.
\end{definition}

\begin{theorem}
\label{th:wronksiano}
Sia $I\subset \RR$ un intervallo aperto non vuoto e siano $a_j\in C^0(I,\CC)$.
Siano $u_1, \dots, u_n$ funzioni di classe $C^n$ soluzioni
dell'equazione lineare omogenea in forma normale:
\begin{equation}\label{eq:092784}
  u^{(n)} = \sum_{j=1}^n a_j(x) u^{(j-1)}(x).
\end{equation}
Sia $W(x)$ la matrice wronksiana~\eqref{eq:wronksiano}.
Allora le seguenti proprietà sono equivalenti:
\begin{enumerate}
\item \label{item:4736201} le funzioni $u_1,\dots, u_n$ sono linearmente indipendenti;
\item \label{item:4736202} per ogni $x\in I$ si ha $\det W(x)\neq 0$;
\item \label{item:4736203} esiste $x\in I$ tale che $\det W(x)\neq 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
Ovviamente \ref{item:4736202} implica \ref{item:4736203}.

Dimostriamo che \ref{item:4736203} implica \ref{item:4736201}.
Passando alla implicazione contropositiva dobbiamo mostrare che
se $u_1,\dots, u_n$ sono linearmente dipendenti allora per ogni $x\in I$ si ha $\det W(x)=0$. Ma se le funzioni sono dipendenti significa che esiste $\vec \lambda \neq 0$ tale che
\begin{equation}\label{eq:3675893}
\sum_{k=1}^n \lambda_k u_k(x) = 0\qquad \forall x \in I.
\end{equation}
Derivando si ottiene, per ogni $j=0,\dots, n-1$:
\[
\sum_{k=1}^n \lambda_k u_k^{(j)}(x) = 0\qquad \forall x \in I
\]
e quindi
\[
  W(x) \, \vec \lambda = 0 \qquad \forall x\in I.
\]
Ma allora la matrice $W(x)$ non è invertibile e quindi $\det W(x)=0$, come volevamo dimostrare.

Dimostriamo infine che \ref{item:4736201} implica \ref{item:4736202}
cioè che se $v_1, \dots, v_n$ sono indipendenti allora $\det W(x) \neq 0$ per ogni $x\in I$.
Fissato $x\in I$ consideriamo il funzionale $J(v) = (v(x),v'(x), \dots, v^{(n-1)}(x))$ cosicché si ha
\[
  W(x) = \enclose{J(u_1), \dots, J(u_n)}.
\]
In base al teorema~\ref{th:edo_lineare_ordine_n} ci ricordiamo che $J$ risulta essere un isomorfismo di spazi vettoriali, dunque se $v_1,\dots,v_n$ sono indipendenti anche $J(v_1),\dots, J(v_n)$ dovranno essere indipendenti.
Ma visto che le colonne della matrice $W(x)$ sono indipendenti significa che $\det W(x)\neq 0$, come volevamo dimostrare.
\end{proof}

\begin{theorem}[metodo della variazione delle costanti arbitrarie]
\mymark{***}
Si consideri una generica equazione differenziale lineare non omogenea di ordine $n$ in forma normale:
\begin{equation}\label{eq:395467435}
  u^{(n)}(x) = \sum_{k=0}^{n-1} a_k(x) u^{(k)}(x) + b(x)
\end{equation}
dove $a_0, \dots, a_{n-1}, b\in C^0(A,\CC)$ sono funzioni definite su un aperto $A\subset \RR$
e si cerca una soluzione $u\in C^n(A,\CC)$.

Se $v_1, \dots, v_n\in C^n(A,\CC)$ sono soluzioni indipendenti dell'equazione omogenea associata:
\begin{equation*}
  u^{(n)}(x) = \sum_{k=0}^{n-1} a_k(x) u^{(k)}(x)
\end{equation*}
allora
esistono delle funzioni $c_k\in C^1(A,\CC)$
per $k=0,1, \dots, n-1$
tali che
\begin{equation}\label{eq:02154676}
  \begin{cases}
    \sum_{k=1}^n c'_k(x) v_k(x) = 0 \\
    \sum_{k=1}^n c'_k(x) v_k'(x) = 0 \\
    \vdots\\
    \sum_{k=1}^n c'_k(x) v_k^{(n-2)}(x) = 0 \\
    \sum_{k=1}^n c'_k(x) v_k^{(n-1)}(x) = b(x).
  \end{cases}
\end{equation}
E in tal caso la funzione
\[
 u(x) = \sum_{k=1}^n c_k(x) v_k(x)
\]
risolve l'equazione non omogenea~\eqref{eq:395467435}.
\end{theorem}
%
\begin{proof}
Osserviamo innanzitutto che se esistono le funzioni $c_k$ che
soddisfano il sistema~\eqref{eq:02154676} allora la funzione $u = c_1
\cdot v_1 + \dots + c_n \cdot v_n$ risolve l'equazione non
omogenea. Infatti dalla formula di derivazione del prodotto si ha:
\[
  u' = \sum_{k=1}^n c_k v_k' + \sum_{k=1}^n c_k' v_k
     = \sum_{k=1}^n c_k v_k'
\]
essendo il secondo addendo nullo per ipotesi~\eqref{eq:02154676}.
Ma allora si può proseguire con le derivate:
\[
  u'' = \sum_{k=1}^n c_k v_k'' + \sum_{k=1}^n c_k' v_k
      = \sum_{k=1}^n c_k v_k''
\]
fino alla derivata $(n-1)$-esima:
\[
  u^{(n-1)} = \sum_{k=1}^n c_k v_k^{(n-1)}.
\]
Per l'ultima derivata si ha infine:
\[
  u^{(n)} = \sum_{k=1}^n c_k v_k^{(n)} + \sum_{k=1}^n c_k' v_k^{(n-1)}
          = \sum_{k=1}^n c_k v_k^{(n)} + b.
\]
Ma allora si osserva che si ha
\begin{align*}
 u^{(n)} + \sum_{j=0}^{n-1} a_j u^{(j)}
 &= \sum_{k=1}^n c_k v_k^{(n)} + b + \sum_{j=0}^{n-1} a_j \sum_{k=1}^n c_k v_k^{(j)} \\
 &= \sum_{k=1}^n c_k \Enclose{v_k^{(n)} + \sum_{j=0}^{n-1} a_j v_k^{(j)}} + b = b
\end{align*}
 in quanto le funzioni $v_j$, per ipotesi, risolvono l'equazione omogenea.

 Rimane quindi solo da verificare che esistono funzioni $c_k\in
 C^1(A,\CC)$ che soddisfano il sistema~\eqref{eq:02154676}.
 Si osserva che l'equazione~\eqref{eq:02154676} si scrive nella forma
 \[
   W(x) \vec c'(x) = \vec b(x)
 \]
 dove $W(x)$ è la matrice wronksiana $W(x)$ associata alle
 alle soluzioni $v_1, \dots, v_n$, $\vec c(x) = (c_1(x), \dots,
 c_n(x))$ e $\vec b(x) = (0, \dots, 0, b(x))$.
 Siccome $v_1, \dots, v_n$ per ipotesi sono indipendenti,
 per il teorema~\ref{th:wronksiano}
 sappiamo che
 $\det W(x)\neq 0$ per ogni $x$ e quindi il sistema $W(x) \vec d(x) =
 \vec b(x)$
 ammette una unica soluzione $d(x)$ per ogni $x\in I$.
 Visto che $W(x)$ e $\vec b(x)$ hanno coefficienti continui,
 anche la soluzione $\vec d(x) = W(x)^{-1} \vec b(x)$ dovrà essere continua (che i coefficienti della matrice $W(x)^{-1}$ siano continui lo si vede ad esempio scrivendo $W(x)^{-1}$ tramite la matrice dei cofattori e sfruttando il fatto che il determinante è una funzione continua)
 e quindi, fissato un punto $x_0\in I$,
 potremo definire
 \[
   c_k(x) = \int_{x_0}^x d_k(t)\, dt
 \]
 per determinare le funzioni $c_k$.
\end{proof}
