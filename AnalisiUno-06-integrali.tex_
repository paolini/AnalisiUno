suddivision\chapter{calcolo integrale}

\begin{definition}[integrale di Riemann]
\mymark{***}
Siano $a,b\in \RR$, $a \le b$.

Un insieme $P\subset [a,b]$ si dice essere una \myemph{suddivisione di Riemann}
\index{Riemann!suddivisione di}
\index{partizione di Riemann}
\index{Riemann!partizione di}
dell'intervallo $[a,b]$ se $P$ è un insieme finito tale che $a,b\in P$.
In particolare $P$ si
potrà scrivere come
\[
 P = \{ x_0, x_1, \dots, x_N\}
\]
con
\[
  a = x_0 < x_1 < \dots < x_{N-1} < x_N = b.
\]

Sia $f\colon [a,b] \to \RR$ una funzione limitata.
Data una qualunque suddivisione $P$ di $[a,b]$ definiamo
rispettivamente le \emph{somme superiori} e le \emph{somme inferiori}
\mymargin{somme superiori/inferiori}
\index{Riemann!somme superiori}
\index{Riemann!somme inferiori}
come
\begin{align*}
S^*(f,P)
&= \sum_{k=1}^N (x_k - x_{k-1}) \cdot \sup f([x_{k-1},x_k]) \\
S_*(f,P)
&= \sum_{k=1}^N (x_k - x_{k-1}) \cdot \inf f([x_{k-1},x_k]).
\end{align*}
Definiamo infine
\begin{align*}
  I^*(f) &= \inf \{S^*(f,P) \colon \text{$P$ suddivisione di $[a,b]$}\}
  \\
  I_*(f) &= \sup \{S_*(f,P) \colon \text{$P$ suddivisione di $[a,b]$}\}.
\end{align*}

Se $I^*(f) = I_*(f)$ diremo che $f$ è
\emph{Riemann-integrabile}
\mymargin{integrale di Riemann}%
\index{Riemann!integrale di}%
\index{integrabilità}%
\index{integrale}%
\index{integrale!definizione}%
e diremo che l'\emph{integrale} di $f$ su $[a,b]$ è
il valore comune $I^*(f)=I_*(f)$ che verrà denotato con
\[
  \int_a^b f
  \qquad{\text{oppure con}} \qquad
  \int_a^b f(x)\, dx.
\]

Se $b<a$ e se $f$ è Riemann integrabile su $[b,a]$
definiamo per convenzione:
\[
  \int_a^b f = -\int_b^a f.
\]
\end{definition}

\begin{theorem}[criteri di integrabilità]
\label{th:criteri_integrabilita}
\mymark{*}
\mynote{criteri di integrabilità}
\index{criterio!di integrabilità}
Sia $f\colon[a,b]\to \RR$ una funzione limitata.
\begin{enumerate}
\item
Se $P$ e $Q$ sono due suddivisioni qualunque dell'intervallo
$[a,b]$ si ha
\[
  S_*(f,P) \le S^*(f,Q).
\]
Di conseguenza $I_*(f) \le I^*(f)$.

\item
La funzione $f$ è Riemann-integrabile se e solo se
per ogni $\eps>0$ esiste una suddivisione $P$
tale che
\[
  S^*(f,P) - S_*(f,P) < \eps.
\]

\item
Se $f$ è Riemann-integrabile su $[a,b]$ allora
esiste una successione $P_n$ di suddivisioni tali che
\begin{equation}\label{eq:93765}
  \lim_{n\to +\infty} S^*(f,P_n)
  = \lim_{n\to+\infty} S_*(f,P_n)
  = \int_a^b f.
\end{equation}
Viceversa se esiste una successione $P_n$ di suddivisioni di $[a,b]$
per cui si ha
\begin{equation*}
  \lim_{n\to +\infty} \enclose{S^*(f,P_n) - S_*(f,P_n)} = 0
\end{equation*}
allora la funzione $f$ è Riemann-integrabile e si ha
\[
  \int_a^b f(x)\, dx = \lim_{n\to+\infty} S^*(f,P_n) = \lim_{n\to+\infty} S_*(f,P_n).
\]
\end{enumerate}
\end{theorem}
%
\begin{proof}
Sia $P$ una qualunque suddivisione di $[a,b]$ e sia $y\in [a,b]$ un punto qualunque. Posto $P' = P \cup \{y\}$ vogliamo mostrare
che si ha
\begin{equation}\label{eq:39543}
  S_*(f,P) \le S_*(f,P') \le S^*(f,P') \le S^*(f,P).
\end{equation}
Se $y\in P$ non c'è niente da dimostrare in quanto
risulterebbe $P'=P$ e la disuguaglianza $S_*(f,P') \le S^*(f,P')$ è sempre verificata in quanto ogni estremo superiore che compare nella definizione di $S^*$ è maggiore o uguale al corrispondente
estremo inferiore che compare nella definizione di $S_*$.
Supponiamo allora che $y \not \in P$ e dunque che $y$ sia compreso tra due punti consecutivi $x_{k-1}, x_k$ della suddivisione $P$:
\[
  a= x_0 < x_1 < \dots < x_{k-1} < y < x_k < \dots < x_N=b.
\]
Allora le somme che definiscono $S_*(f,P)$ e $S_*(f,P')$ differiscono solo sull'intervallo $[x_{k-1},x_k]$ e si ha
\begin{align*}
  S_*(f,P') - S_*(f,P)
  &= (y-x_{k-1})\cdot \!\!\inf_{[x_{k-1},y]}\!\!\! f
  + (x_k - y)\cdot\! \inf_{[y,x_k]}\! f\\
  &\quad - (x_k - x_{k-1})\cdot \!\!\inf_{[x_{k-1}, x_k]}\!\!\!f
\end{align*}
ma osservando che
\[
\inf_{[x_{k-1}, x_k]}\!\!f
\le\inf_{[x_{k-1},y]}\!\! f
\qquad \text{e} \qquad
\inf_{[x_{k-1}, x_k]}\!\!f
\le\inf_{[y,x_k]}\!\! f
\]
si ottiene $S_*(f,P) \le S_*(f,P')$.
In maniera analoga si ottiene $S^*(f,P) \ge S^*(f,P')$.
Dunque \eqref{eq:39543} è dimostrata.
Ma allora se $P$ e $Q$ sono suddivisioni qualunque osserviamo che $P\cup Q$ si può ottenere da $P$ aggiungendo uno alla volta i punti di $Q$. Iterando la \eqref{eq:39543} si può dunque concludere che
\[
 S_*(f,P) \le S_*(f,P\cup Q) \le S^*(f,P\cup Q) \le S^*(f,Q)
\]
da cui discende il primo punto del teorema: $S_*(f,P) \le S^*(f,Q)$.
Facendo l'estremo inferiore al variare di $Q$
si ottiene $S_*(f,P) \le I^*(f)$ e facendo l'estremo superiore al variare di $P$ si ottiene $I_*(f) \le I^*(f)$.

Dimostriamo il secondo punto.
Se esiste una suddivisione $P$ tale che $S^*(f,P)-S_*(f,P) < \eps$ possiamo immediatamente concludere che
\[
I^*(f) - I_*(f) \le S^*(f,P) - S_*(f,P) < \eps.
\]
Se questo è vero per ogni $\eps >0$ deduciamo che $I^*(f) - I_*(f) = 0$ e dunque che $f$ è Riemann-integrabile.

Viceversa qualunque sia $f$, per le proprietà di
di $\sup$ e $\inf$
esistono $Q$ e $R$ suddivisioni tali che
\[
  I^*(f) \ge S^*(f,Q) - \frac\eps 2
  \qquad\text{e}\qquad
  I_*(f) \le S_*(f,R) + \frac\eps 2
\]
da cui, per il punto precedente, ponendo $P=Q\cup R$
se $f$ è Riemann integrabile
si ottiene
\begin{align*}
S^*(f,P)-S_*(f,P) &\le S^*(f,Q) - S_*(f,R) \\
&\le I^*(f) + \frac\eps 2 - \enclose{I_*(f) - \frac \eps 2} = \eps.
\end{align*}

Per il terzo punto del teorema
supponiamo dapprima che $f$ sia Riemann-integrabile su $[a,b]$.
Allora per il punto precedente per ogni $n\in \NN$ ponendo $\eps=1/n$ possiamo trovare una suddivisione $P_n$ tale che
\[
  S^*(f,P_n) - S_*(f,P_n) < \frac 1 n
\]
da cui
\[
  I^*(f) \le S^*(f,P_n) \le S_*(f,P_n) + \frac 1 n
   \le I_*(f) + \frac 1 n
\]
perciò passando al limite per $n\to +\infty$,
essendo $I^*(f) = I_*(f) = \int_a^b f$ deve valere
\[
  \lim S^*(f,P_n) = \lim S_*(f,P_n) = \int_a^b f.
\]

Viceversa se
\[
 \lim_{n\to +\infty} S^*(f,P_n) - S_*(f,P_n) = 0
\]
per ogni $\eps>0$ esiste $n$ tale che
\[
  S^*(f,P_n) - S_*(f,P_n) < \eps.
\]
Per il punto precedente concludiamo che $f$ è Riemann-integrabile.
D'altra parte sappiamo che
\[
  S_*(f,P_n) \le I_*(f) = \int_a^b f = I^*(f) \le S^*(f,P_n)
\]
dunque se $S^*(f,P_n) - S_*(f,P_n) \to 0$ necessariamente
l'integrale coincide con i limiti di $S^*(f,P_n)$ e di
$S_*(f,P_n)$.
\end{proof}

\begin{example}[calcolo dell'integrale tramite le suddivisioni]
\label{ex:integrale_quadrato}
Mostriamo che per ogni $b>0$ la funzione $f(x)=x^2$ è Riemann-integrabile sull'intervallo $[0,b]$ e si ha
\[
 \int_0^b x^2\, dx = \frac{b^3}{3}.
\]
\end{example}
\begin{proof}
Consideriamo le suddivisioni \emph{equispaziate} dell'intervallo $[0,b]$, cioè dividiamo $[0,b]$ in $N$ intervalli ognuno di ampiezza $b/N$:
\[
P_N = \left\{\frac{kb}{N}\colon k \in 0, 1, \dots, N\right\}.
\]
Si ha
\[
  S^*(f,P_N) = \sum_{k=1}^N \sup_{[(k-1)b/N,kb/N]}f \cdot \frac b N
   = \frac{b}{N} \sum_{k=1}^N \frac{k^2b^2}{N^2}
   = \frac{b^3}{N^3} \sum_{k=1^N} k^2.
\]
Ricordiamo ora che vale
\[
  \sum_{k=1}^n k^2 = \frac{n(n+1)(2n+1)}{6} = \frac{2n^3+3n^2+n}{6}
\]
(tale formula può essere facilmente verificata per induzione). Dunque si ha
\[
  S^*(f,P_N) = \frac{b^3}{N^3} \frac{2N^3+3N^2+N}{6}
       = \frac{b^3}{6}\enclose{2 + \frac{3}{N}+\frac 1 N^2}
       \to \frac{b^3}{3}
\]
per $N\to +\infty$.
Analogamente si trova
\[
  S_*(f,P_N) = \sum_{k=1}^N \inf_{[(k-1)b/N,kb/N]} f \cdot \frac{b}{N}
  = \frac{b}{N}\sum_{k=1}^N \frac{(k-1)^2b^2}{N^2}
  = \frac{b^3}{N^3} \sum_{k=0}^{N-1} k^2
\]
e osservando che si ha
\[
 \sum_{k=0}^{N-1} k^2 = \sum_{k=1}^{N-1} k^2 = \frac{2(N-1)^3+3(N-2)^2+(N-1)}{6}
\]
otteniamo
\[
 S_*\ge \sup_N S_*(f,P_N) \ge \lim_{N\to+\infty} S_*(f,P_N) = \frac{b^3}{3} \to \frac{b^3}{3}.
\]
La dimostrazione si conclude quindi applicando
il criterio \eqref{eq:93765} del teorema precedente.
\end{proof}

\begin{theorem}[integrale della costante]
\label{th:integrale_costante}
Se $f\colon[a,b]\to \RR$ è costante: $f(x) = c$ allora
$f$ è Riemann-integrabile e si ha
\[
  \int_a^b f = c\cdot (b-a).
\]
\end{theorem}
%
\begin{proof}
Visto che su ogni $A\subset [a,b]$ si ha
\[
  \sup_A f = \inf_A f = c
\]
è facile verificare che si ha
\[
  S^*(f,P) = S_*(f,P) = c\cdot (b-a)
\]
qualunque sia la suddivisione $P$ di $[a,b]$. Il risultato segue immediatamente.
\end{proof}

Non tutte le funzioni sono Riemann-integrabili come ci mostra il seguente esempio.
\begin{example}[funzione di Dirichlet]
\mymark{**}
\mynote{funzione di Dirichlet}
\index{funzione!di Dirichlet}
\index{integrabilità!controesempio}
Sia $a<b$ e sia $f\colon[a,b]\to \RR$ la funzione definita da
\[
 f(x) =
 \begin{cases}
   1 & \text{se $x\in \QQ$}\\
   0 & \text{se $x\not \in \QQ$}.
 \end{cases}
\]
Allora $f$ non è Riemann-integrabile.
\end{example}
%
\begin{proof}
\mymark{*}
Sia $P=\{x_0, x_1, \dots, x_N\}$ con $a=x_0 < x_1 < \dots < x_N = b$
una qualunque suddivisione di $[a,b]$.
Allora basta osservare che, per la densità dei razionali,
in qualunque intervallino $I=[x_{k-1}, x_k]$ sono presenti infiniti punti
razionali e infiniti punti irrazionali. Dunque $\sup f(I)=1$ e $\inf f(I)=0$ e di conseguenza
\begin{align*}
  S^*(f,P) &= \sum_{k=1}^N (x_k - x_{k-1})\cdot 1 = b-a \\
  S_*(f,P) &= \sum_{k=1}^N (x_k - x_{k-1})\cdot 0 = 0
\end{align*}
da cui $I^*(f) = b-a \neq 0 = I_*(f)$.
\end{proof}

\begin{theorem}[monotonia dell'integrale]
\mymark{*}
Sia $a\le b$ e siano
$f,g\colon [a,b]\to \RR$ due funzioni Riemann-integrabili.
Se per ogni $x\in [a,b]$ si ha $f(x) \le g(x)$ allora
\[
  \int_a^b f(x) \le \int_a^b g(x).
\]

In particolare se $f\ge 0$ allora $\int_a^b f \ge 0$.
\end{theorem}
%
\begin{proof}
Chiaramente se $f \le g$ si avrà che il $\sup$ di $f$ su qualunque intervallo sarà minore o uguale al $\sup$ di $g$ sullo stesso intervallo. Dunque su ogni suddivisione $P$ di $[a,b]$ si avrà:
\[
  S^*(f,P) \le S^*(g,P)
\]
da cui si ottiene immediatamente $I^*(f) \le I^*(g)$ e il risultato segue.
\end{proof}

\begin{theorem}[linearità dell'integrale]
\mymark{*}
\label{th:integrale_lineare}
Siano $f,g\colon [a,b]\to \RR$ due funzioni Riemann-integrabili
e siano $\lambda, \mu \in \RR$. Allora $\lambda f + \mu g$
è Riemann integrabile e si ha
\[
  \int_a^b (\lambda f + \mu g) = \lambda \int_a^b f + \mu \int_a^b g.
\]

In particolare l'insieme delle funzioni Riemann-integrabili su $[a,b]$ risulta
essere uno spazio vettoriale reale e l'integrale è una
applicazione lineare su tale spazio, a valori in $\RR$.
\end{theorem}
%
\begin{proof}
\mymark{*}
Mostriamo innanzitutto che
\begin{equation}\label{eq:20043}
  \int_a^b (-f) = -\int_a^b f.
\end{equation}
Questo deriva dal fatto che su qualunque insieme $A$ si ha
$\sup_A (-f) = -\inf_A f$ e dunque per una qualunque suddivisione $P$
si ha
\[
  S^*(-f,P) = -S_*(f,P).
\]
Se ne deduce che $I^*(-f) = -I_*(f)$ e, analogamente, $I_*(-f) = -I^*(f)$.
Dunque se $f$ è Riemann-integrabile anche $-f$ lo è e vale la proprietà \eqref{eq:20043}.

Ora se $\lambda \ge 0$ vogliamo mostrare che vale
\begin{equation}\label{eq:10032}
  \int_a^b \lambda f = \lambda \int_a^b f.
\end{equation}
Semplicemente si osserva che $\sup_I \lambda f = \lambda \sup_I f$ e dunque
$S^*(\lambda f,P) = \lambda S^*(f,P)$ per ogni suddivisione $P$.
Ne consegue che $I^*(\lambda f) = \lambda I^*(f)$. In maniera analoga si può
mostrare che $I_*(\lambda f) = \lambda I_*(f)$. Dunque se $f$ è
Riemann-integrabile anche $\lambda f$ (con $\lambda \ge 0$) lo è e vale \eqref{eq:10032}.

Mettendo assieme \eqref{eq:20043} e $\eqref{eq:10032}$ si ottiene
che $\eqref{eq:10032}$ vale per ogni $\lambda \in \RR$.
Lo stesso sarà vero se mettiamo $g$ al posto di $f$ e $\mu$ al posto di $\lambda$.
Per concludere la dimostrazione sarà dunque sufficiente
mostrare che vale anche
\begin{equation*}\label{eq:80003}
\int_a^b (f+g) = \int_a^b f + \int_a^b g.
\end{equation*}
Osserviamo che su qualunque insieme $A$ si ha
\[
  \sup_A (f+g) \le \sup_A f + \sup_A g.
\]
Infatti per le proprietà dell'estremo superiore per ogni $\eps>0$ esiste $x\in A$ tale che
\[
  \sup_A (f+g) \le f(x) + g(x) + \eps.
\]
Ma chiaramente $f(x) \le \sup_A f$ e $g(x)\le \sup_A g$ dunque si ottiene
\[
  \sup_A (f+g) \le \sup_A f + \sup_A g + \eps.
\]
Passando al limite per $\eps \to 0^+$ si ottiene la disuguaglianza voluta.
Questo significa che
\[
  S^*(f+g) \le S^*(f) + S^*(g).
\]
analogamente si potrà dimostrare che
\[
  S_*(f+g) \ge S_*(f) + S_*(g).
\]
Si ottiene dunque
\[
  I^*(f+g) \le I^*f(f) + I^*(g)
  \qquad\text{e}\qquad
  I_*(f+g) \ge I_*(f) + I_*(g)
\]
e dunque se $f$ e $g$ sono integrabili anche $f+g$ risulta integrabile
e vale la \eqref{eq:80003}.

Per concludere che l'insieme delle funzioni integrali sia uno spazio vettoriale
è sufficiente osservare che, grazie al teorema~\ref{th:integrale_costante},
la funzione $0$ risulta integrabile.
\end{proof}

\begin{theorem}[proprietà di reticolo]
\label{th:reticolo}
Se $f$ e $g$ sono funzioni a valori reali definiamo le
funzioni $f\wedge g$ (massimo), $f \vee g$ (minimo),
$f^+$ (parte positiva) e $f^-$ (parte negativa) come segue:
\begin{gather*}
  (f \wedge g)(x) = \min\{f(x), g(x)\}  \qquad
  (f \vee g)(x) = \max\{f(x), g(x)\} \\
  f^+ (x) =
      \begin{cases}
      f(x) & \text{se $f(x)> 0$}\\
      0 & \text{altrimenti}
      \end{cases} \qquad
  f^- (x) =
      \begin{cases}
      -f(x) & \text{se $f(x)< 0$}\\
      0 & \text{altrimenti.}
      \end{cases}
\end{gather*}
Risulta $f= f^+ - f^-$, $\abs{f}=f^+ + f^-$.

Se $f$ è una funzione Riemann-integrabile sull'intervallo $[a,b]$ allora
anche $\abs{f}$, $f^+$ e $f^-$ sono integrabili e se
anche $g$ è Riemann-integrabile su $[a,b]$ allora anche $f\vee g$ e $f\wedge g$
sono integrabili sullo stesso intervallo.

Viceversa se $f^+$ e $f^-$ sono Riemann-integrabili su $[a,b]$ anche $f$
è Riemann-integrabile su $[a,b]$.
\end{theorem}
%
\begin{proof}
Dimostriamo innanzitutto che se $f$ è integrabile anche $\abs{f}$ lo è.
Basta osservare che in generale se $x,y\in [a,b]$ si ha
\[
  \abs{f(x)}-\abs{f(y)} \le \abs{f(x) - f(y)}
\]
da cui per ogni $A\subset [a,b]$
\[
  \sup_{x\in A} \abs{f(x)} - \inf_{y\in A} \abs{f(y)} \le
  \sup_{x\in A} f(x) - \inf_{y\in A}f(y)
\]
e quindi per ogni suddivisione $P$ si avrà
\[
  S^*(\abs{f},P) - S_*(\abs{f}, P) \le S^*(f,P) - S_*(f,P).
\]
Ma se $f$ è integrabile allora il lato destro può essere reso
arbitrariamente piccolo (teorema~\ref{th:criteri_integrabilita})
e di conseguenza anche il lato sinistro.
Dunque la funzione $\abs{f}$ è integrabile (se $f$ lo è).

Ma allora basta osservare che
\begin{gather*}
  f^+ = \frac{\abs{f} + f}{2}, \qquad
  f^- = \frac{\abs{f} - f}{2}, \\
  f\wedge g = \frac{f + g + \abs{f-g}}{2}, \qquad
  f\vee g = \frac{f+g - \abs{f-g}}{2}
\end{gather*}
per dedurre che anche $f^+$, $f^-$, $f\wedge g$ e $f\vee g$
sono integrabili,
 grazie
alla linearità dell'integrale (teorema~\ref{th:integrale_lineare}).
\end{proof}

\begin{theorem}[additività dell'integrale]
\mymark{*}
\mymargin{additività dell'integrale}
\index{integrale!additività}
\label{th:additivita_integrale}
Sia $f\colon [a,b]\to \RR$ una funzione limitata e sia $c\in [a,b]$.
Allora $f$ è Riemann-integrabile su $[a,b]$ se e solo se
$f$ è Riemann-integrabile su $[a,c]$ e su $[c,b]$.
E in tal caso risulta
\begin{equation}\label{eq:36645}
 \int_a^b f = \int_a^c f + \int_c^b f.
\end{equation}

In base alla convenzione
\[
   \int_b^a f = -\int_a^b f
\]
la formula \eqref{eq:36645} è valida non solo se $a\le c\le b$ ma anche
se $a,b,c$ sono in qualunque ordine, purché la funzione $f$ sia integrabile
sull'intervallo che contiene tutti e tre i punti $a,b,c$.
\end{theorem}
%
\begin{proof}
\mymark{*}
Supponiamo che $f$ sia integrabile su $[a,c]$ e su $[c,b]$.
Allora, in base ai criteri di integrabilità, per ogni $\eps>0$ esisteranno una
suddivisione $P$ di $[a,c]$ e una suddivisione $Q$ di $[c,b]$ tali che
\[
  S^*(f,P) - S_*(f,P) < \frac \eps 2,
  \qquad
  S^*(f,Q) - S_*(f,Q) < \frac \eps 2.
\]
L'insieme $R=P\cup Q$ risulta essere una suddivisione di $[a,b]$ su cui si avrà
\begin{equation}\label{eq:56632}
S^*(f,R) = S^*(f,P) + S^*(f,Q), \qquad
S_*(f,R) = S_*(f,P) + S_*(f,Q)
\end{equation}
e dunque
\[
S^*(f,R) - S_*(f,R) \le \frac \eps 2 + \frac \eps 2 = \eps.
\]
Applicando nuovamente il criterio di integrabilità in senso invertito otteniamo
unque l'integrabilità di $f$ su $[a,b]$ e le equazioni
\eqref{eq:56632} garantiscono l'additività dell'integrale rispetto al dominio.

Viceversa se $f$ è integrabile su $[a,b]$ il criterio di integrabilità
ci garantisce che per ogni $\eps>0$ esiste una suddivisione $R$ di $[a,b]$ tale che
\[
S^*(f,R) - S_*(f,R) < \eps.
\]
Se ora consideriamo $R' = R \cup \{c\}$ sappiamo che $S^*(f,R') \le S^*(f,R)$
e $S_*(f,R') \ge S_*(f,R)$ dunque anche $R'$ soddisfa la proprietà
\[
S^*(f,R') - S_*(f,R') < \eps.
\]
Ma ora è chiaro che posto $P=R \cap[a,c]$ e $Q=R\cap[c,b]$ risulta che $P$
e $Q$ siano suddivisioni di $[a,c]$ e $[c,b]$ rispettivamente e che
\begin{align*}
  S^*(f,R') &= S^*(f,P) + S^*(f,Q), \\
  S_*(f,R') &= S_*(f,P) + S_*(f,Q).
\end{align*}
Dunque si ha
\begin{align*}
(S^*(f,P) - S_*(f,P)) + (S^*(f,Q) - S_*(f,Q))
&= S^*(f,R') - S_*(f,R) \\
&< \eps.
\end{align*}
Visto che entrambi gli addendi $S^*-S_*$ sono non negativi
risulta che valgono separatamente le disuguaglianze
\[
S^*(f,P) - S_*(f,P) < \eps, \qquad
S^*(f,Q) - S_*(f,Q) < \eps.
\]
Dunque $f$ è integrabile sia su $[a,c]$ che su $[c,b]$.
E nuovamente possiamo osservare che l'integrale è additivo sul dominio.
\end{proof}

\begin{theorem}[integrabilità delle funzioni continue]
\mymark{***}
\label{th:integrabilita_continue}
\mynote{integrabilità delle funzioni continue}
\index{integrabilità!funzioni continue}
Sia $f\colon [a,b]\to \RR$ una funzione continua.
Allora $f$ è limitata e Riemann-integrabile.
\end{theorem}
%
\begin{proof}
\mymark{***}
Per il teorema di Weierstrass sappiamo che $f$ è limitata.
Per il teorema di Heine-Cantor sappiamo che $f$ è uniformemente continua,
dunque per ogni $\eps>0$ esiste un $\delta>0$ tale che
\[
 \abs{x-y} < \delta \implies \abs{f(x)-f(y)} < \eps.
\]
Possiamo allora considerare una suddivisione $P_\delta$ con la proprietà che
gli intervalli individuati dalla suddivisione abbiano tutti ampiezza minore di
$\delta$ (ad esempio potremmo prendere la suddivisione formata da
$(b-a)/\delta+2$ punti equispaziati in $[a,b]$). Su ogni intervallo $I$ di tale
suddivisione si avrà che se $x,y\in I$ allora $\abs{f(x)-f(y)} < \eps$ da cui
si deduce $\sup_I f - \inf_I f \le \eps$.
In particolare, sommando su tutti gli intervalli, si avrà
\begin{align*}
  S^*(f,P_\delta) - S_*(f,P_\delta)
  &= \sum_{k=1}^N (x_k-x_{k-1})\enclose{\sup_{[x_{k-1},x_k]} f - \inf_{[x_{k-1},x_k]} f} \\
  &\le \eps \sum_{k=1}^N (x_k - x_{k-1})
   = \eps (b-a).
\end{align*}
Visto che questa quantità può essere resa arbitrariamente piccola per
$\eps \to 0$, in base ai criteri di integrabilità possiamo concludere che la
funzione $f$ è integrabile.
\end{proof}

\begin{theorem}[integrabilità delle funzioni monotone]
\label{th:integrabilita_monotone}%
\mynote{integrabilità delle funzioni monotone}%
\index{integrabilità!funzioni monotone}%
Sia $f\colon [a,b]\to \RR$ una funzione monotona. Allora $f$ è limitata e
Riemann-integrabile.
\end{theorem}
%
\begin{proof}
Supponiamo, per fissare le idee, che $f$ sia crescente.

Chiaramente $f$ è limitata in quanto $f(a) \le f(x) \le f(b)$ per ogni
$x\in [a,b]$.

Per avere l'integrabilità e sufficiente mostrare
che esiste una successione di suddivisioni $P_n$
tale che $S^*(f,P_n) - S_*(f,P_n) \to 0$.
Consideriamo la suddivisione equispaziata
$P_n=\{x_k \colon k=0,1, \dots, n\}$ con $x_k=a+k(b-a)/N$.
In tal caso su ogni intervallino $[x_{k-1},x_k]$ si ha
\[
  \sup f([x_{k-1}, x_k]) = f(x_k),
  \qquad
  \inf f([x_{k-1}, x_k]) = f(x_{k-1}).
\]
Dunque la differenza tra le somme superiori
e le somme inferiori è telescopica
e si ha, per $n\to +\infty$
\begin{align*}
S^*(f,P) - S_*(f,P)
&= \sum_{k=1}^n \frac{b-a}{n} f(x_k)
  - \sum_{k=1}^n \frac{b-a}{n} f(x_{k-1}) \\
&= \frac{b-a}{n}(f(b)-f(a)) \to 0.
\end{align*}
E' quanto volevamo dimostrare.
\end{proof}

\begin{example}[funzione di Heaviside]
\label{ex:heaviside}
\mynote{funzione di Heaviside}
\index{funzione!di Heaviside}
Sia $a<0<b$.
La funzione $H\colon [a,b] \to \RR$ definita da
\[
H(x) =
\begin{cases}
1 & \text{se $x\ge 0$}\\
0 & \text{se $x< 0$}
\end{cases}
\]
è crescente quindi integrabile.
\end{example}

\begin{theorem}[continuità dell'integrale]
\label{th:integrale_continuo}
Sia $f\colon [a,b]\to \RR$ una funzione limitata e tale
che per ogni $c\in (a,b]$ risulta che $f$ sia Riemann-integrabile
su $[c,b]$. Allora $f$ è Riemann-integrabile su $[a,c]$ e risulta
\[
  \int_a^b f = \lim_{c\to a^+} \int_c^b f.
\]

Analogamente se $f$ è integrabile su ogni intervallo $[a,c]$
con $c\in [a,b)$ allora $f$ è integrabile su $[a,b]$ e vale
\[
  \int_a^b f = \lim_{c\to b^-} \int_a^c f.
\]
\end{theorem}
%
\begin{proof}
Dimostriamo solamente la prima parte, visto che la seconda si tratta in
maniera del tutto analoga. Supponiamo quindi che $f$ sia integrabile su ogni
intervallo $[c,b]$ con $c\in (a,b]$. Sappiamo inoltre che $f$ è limitata su
tutto $[a,b]$ e quindi esiste $M>0$ tale che $\abs{f(x)}\le M$ per ogni
$x\in [a,b]$. Fissato $\eps>0$ qualunque,
scegliamo $c = a + \eps/(4M)$ e, sapendo che $f$ è integrabile su $[c,b]$,
consideriamo una suddivisione $Q_\eps$ di $[c,b]$ tale che
\[
  S^*(f, Q_\eps) - S_*(f, Q_\eps) < \frac \eps 2.
\]
Ma allora posto $P_\eps = \{a\} \cup Q_\eps$ otteniamo:
\begin{align*}
  S^*(f,P_\eps) &= S^*(f,Q_\eps) + (c-a)\sup_{[a,c]} f
    \le S^*(f,Q_\eps) + M \cdot (c-a)\\
  S_*(f,P_\eps) &= S_*(f,Q_\eps) + (c-a)\inf_{[a,c]} f
    \ge S_*(f,Q_\eps) - M \cdot (c-a)
\end{align*}
da cui
\begin{align*}
 S^*(f,P_\eps)-S_*(f,P_\eps)
 &\le S^*(f,Q_\eps) - S_*(f,Q\eps) + 2 M \cdot (c-a)\\
 &< \frac{\eps}{2} + 2 M \frac{\eps}{4M}
 = \eps.
\end{align*}
Dunque, la funzione $f$ è integrabile. Ma si ha
\begin{align*}
\int_a^b f \le S^*(f,P_\eps) \le S^*(f,Q_\eps) + \frac{\eps}{2}
\le S_*(f,Q_\eps) + \frac 3 2 \eps
\le \int_{a+\frac {\eps}{4M}}^b f + \frac {3}{2} \eps \\
\int_a^b f \ge S_*(f,P_\eps) \ge S_*(f,P_\eps) - \frac{\eps}{2}
\ge S^*(f,P_\eps) - \frac 3 2 \eps
\ge \int_{a+\frac {\eps}{4M}}^b f - \frac {3}{2} \eps
\end{align*}
da cui, passando al limite per $\eps \to 0^+$, si ottiene
\[
  \lim_{c\to a^+} \int_c^b f \le \int_a^b f \le \lim_{c\to a^+} \int_c^b f
\]
e quindi l'uguaglianza.
\end{proof}

\begin{example}
La funzione
\[
  f(x) = \begin{cases}
  \sin\frac 1 x & \text{se $x\neq 0$,}\\
  0 & \text{se $x=0$}
  \end{cases}
\]
è integrabile su ogni intervallo chiuso e limitato $[a,b]$.
\end{example}
\begin{proof}
Su ogni intervallo $[\eps,b]$ con $\eps>0$ e $b>\eps$ la funzione
è integrabile in quanto su tali intervalli è continua (la funzione è continua su
tutto $\RR\setminus\{0\}$). Inoltre la funzione è limitata, quindi per il teorema
precedente possiamo concludere che è integrabile su $[0,b]$ per ogni $b>0$.

In maniera analoga (per simmetria) la funzione è integrabile su $[a,0]$
per ogni $a<0$.
Dunque, per additività, la funzione è integrabile su ogni $[a,b]$ con $a<0$ e $b>0$
e di conseguenza (sempre grazie al teorema~\ref{th:additivita_integrale})
è integrabile su ogni intervallo $[a,b]$.
\end{proof}

\section{teorema fondamentale del calcolo integrale}

\begin{theorem}[del valor medio]
\mymark{***}
Siano $a,b\in \RR$, $a<b$ e sia
$f\colon [a,b] \to \RR$ una funzione continua.
Allora esiste un punto $y \in (a,b)$
tale che
\[
\frac{\int_a^b f}{b-a} = f(y).
\]
\end{theorem}
%
La quantità
\[
  \frac{\int_a^b}{b-a} f
\]
si chiama \emph{valor medio integrale} di $f$ su $[a,b]$ e spesso
si indica con il simbolo
\[
  -\!\!\!\!\!\!\int_a^b f.
\]
%
\begin{proof}
\mymark{***}
Per il teorema di Weierstrass la funzione $f$ ha massimo $M$ e minimo $m$
sull'intervallo $[a,b]$ cosicché
per ogni $x\in [a,b]$ si avrà:
\[
  m \le f(x) \le M.
\]
Risulta quindi, per la monotonia dell'integrale:
\[
  (b-a) m = \int_a^b m \le \int_a^b f \le \int_a^b M = (b-a) M
\]
ovvero
\[
  m \le \frac{\int_a^b f}{b-a} \le M.
\]
Dunque la media integrale è un valore intermedio tra il minimo e il massimo
della funzione e quindi, per il teorema dei valori intermedi,
dovrà esistere un punto $y\in [a,b]$ dove la funzione assume tale valore.
\end{proof}


\mynote{teor. fondamentale}
\begin{theorem}[Torricelli-Barrow: teorema fondamentale del calcolo integrale]
\mymark{***}
\index{teorema!fondamentale del calcolo integrale}
\index{teorema!di Torricelli-Barrow}
\index{Torricelli-Barrow!teorema di}
Sia $I\subset \RR$ un intervallo, sia $x_0 \in I$
 e sia $f\colon I\to \RR$ una funzione continua.
Allora la \emph{funzione integrale}
\index{funzione!integrale}
$F\colon I \to \RR$
\mynote{funzione integrale}
\[
  F(x) = \int_{x_0}^x f
\]
è ben definita, è derivabile e si ha per ogni $x\in I$
\[
  F'(x) = f(x).
\]
In particolare essendo $f\in C^0(I)$ si ha $F\in C^1(I)$.

Inoltre se $G\colon I \to \RR$ è una qualunque funzione tale che
$G'(x) = f(x)$ per ogni $x\in I$, allora per ogni $a,b \in I$ si ha
\mynote{formula fondamentale del calcolo integrale}
\index{formula!fondamentale del calcolo integrale}%
\[
  \int_a^b f = G(b) - G(a).
\]
\end{theorem}
%
\begin{proof}
\mymark{***}
Osserviamo innanzitutto che la funzione $f$, essendo continua, è integrabile
su ogni intervallo chiuso e limitato contenuto in $I$. Dunque l'integrale
$\int_{x_0}^x f$ è ben definito.

Per ogni $h\neq 0$, se $x+h \in I$ per l'additività dell'integrale
si ha
\[
\frac{F(x+h) - F(x)}{h} = \frac{\int_{x_0}^{x+h} f - \int_{x_0}^x f}{h}
 = \frac{\int_x^{x+h} f}{h}.
\]
Applicando ora il teorema del valor medio possiamo
affermare che esiste un punto $\xi(h)$ nell'intervallo di estremi $x$ e $x+h$
tale che
\[
  \frac{\int_x^{x+h} f}{h} = f(\xi(h)).
\]
Per $h\to 0$, si ha $\xi(h) \to x$ e, per continuità di $f$,
$f(\xi(h)) \to f(x)$.
Dunque abbiamo mostrato che $F$ è derivabile in $x$:
\[
 \lim_{h\to 0}\frac{F(x+h)-F(x)}{h} = f(x)
\]
e $F'(x) = f(x)$.

Dunque se $a,b\in I$ sono punti qualunque si ha:
\[
\int_a^b f = \int_{x_0}^b f - \int_{x_0}^a f = F(b) - F(a).
\]
E se $G\colon I \to \RR$ è una qualunque  funzione tale che $G'(x)=f(x)$ si
avrà $G'(x) = F'(x)$ per ogni $x\in I$ e dunque $(G-F)' = 0$ su $I$.
Per i criteri di monotonia possiamo concludere che $G-F$ è costante su $I$:
$G-F = c$. Dunque si ha
\[
 \int_a^b f = F(b) - F(a) = (G(b) - c) - (G(a) - c) = G(b) - G(a).
\]
\end{proof}

\begin{example}
Si voglia calcolare
\[
  \int_0^b x^2\, dx.
\]
Basterà osservare che posto $G(x)=\frac{x^3}{3}$ si ha $G'(x)=x^2$
e quindi, grazie alla formula fondamentale del calcolo integrale si
ha
\[
  \int_0^b x^2\, dx = G(b) - G(0) = \frac{b^3}{3}.
\]
E' evidente quanto questo metodo risolutivo sia molto più semplice
e potente di quello utilizzato nell'esempio~\ref{ex:integrale_quadrato}.
\end{example}

\begin{definition}[primitiva]
\mymark{***}
Sia $A \subset \RR$ e sia $f\colon A \to \RR$ una funzione qualunque.
Una funzione $F\colon A \to \RR$ si dice essere una \myemph{primitiva}
(o \emph{antiderivata})
\index{antiderivata}
di $f$ se $F$ è derivabile e $F'(x)=f(x)$ per ogni $x\in A$.
\end{definition}

Il teorema fondamentale del calcolo integrale può dunque essere espresso nel
modo seguente: ogni funzione $f$ continua, definita su un intervallo,
ammette almeno una primitiva e se $F$ è una qualunque primitiva di $f$ si ha
\[
  \int_a^b f = F(b) - F(a).
\]
Per indicare la differenza $F(b)-F(a)$ si usano
talvolta le seguenti notazioni:
\[
  \Enclose{F(x)}_{x=a}^b = \Enclose F_a^b
  = F(x) \vert_{x=a}^b
  = F \vert_a^b = F(b) - F(a).
\]

Il calcolo degli integrali si riduce quindi alla determinazione delle primitive
ovvero ad invertire l'operatore di derivata.
Risulterà quindi importante avere degli strumenti per determinare le primitive
di una funzione.

\begin{definition}[integrale indefinito]
\mymark{***}
L'insieme di tutte le primitive di una funzione $f\colon A \to \RR$
si indica con il simbolo
\[
  \int f
  \qquad\text{oppure}\qquad
  \int f(x) \, dx
\]
e si chiama \emph{integrale indefinito}.
Il motivo di questa notazione (e del nome) deriva dal teorema fondamentale del
calcolo integrale, in
base al quale se $f\colon[a,b]\to \RR$
è continua si ha
\[
  \int_a^b f = \Enclose{\int f}_a^b.
\]
\end{definition}

\begin{remark}
Si faccia attenzione però che nel caso di funzioni non continue è possibile
che le funzioni integrali non siano primitive. Ad esempio
se si prende la funzione di Heaviside $H(x)$ definita nell'esempio~\ref{ex:heaviside}
si può osservare che $\int_0^x H(t)\, dt = \abs{x}$
è una funzione integrale ma non è una primitiva (e l'insieme delle primitive,
in questo caso, è vuoto).
\end{remark}

Se pensiamo all'operatore lineare $D$ definito sull'insieme delle funzioni
derivabili $Df = f'$
si può pensare a $\int f$ come all'insieme delle controimmagini di $f$
tramite $D$ ovvero:
\[
  \int f = D^{-1}(\{f\}) = \{F \colon DF =f \}.
\]

\begin{theorem}[proprietà delle primitive]
\mymark{***}
Sia $f\colon I \to \RR$
una funzione continua definita su un intervallo non vuoto $I\subset \RR$. Allora
\begin{enumerate}
\item esiste almeno una primitiva $F$ di $f$;
\item data una primitiva $F$ di $f$ ogni altra
primitiva $G$ differisce da $F$ per una costante: $\exists c\in \RR\colon G= F+c$.
\end{enumerate}

Detto in altri termini $\int f$ non è vuoto e se il dominio di $f$ è un
intervallo e
$F\in \int f$ è una primitiva, allora
\[
  \int f = \{F+c \colon c \in \RR\}.
\]
\end{theorem}

Osserviamo che l'insieme delle funzioni costanti
su un intervallo
non è altro che $\ker D$ ovvero lo spazio di annullamento dell'operatore derivata.
Stiamo dunque semplicemente osservando che le controimmagini di un operatore
lineare sono spazi affini paralleli al nucleo dell'operatore.

\begin{proof}
\mymark{***}
Scelto un punto $x_0\in I$ possiamo
considerare la funzione integrale
\[
  F(x) = \int_{x_0}^x f(t)\, dt.
\]
Il teorema fondamentale del calcolo integrale
ci assicura che $F$ è una primitiva di $f$.

Viceversa se $F$ e $G$ sono due primitive di $f$ allora si ha:
\[
  F' = G' = f.
\]
Posto $H=G-F$ avremo quindi $H'=0$ sull'intervallo $I$. Per i criteri di
monotonia sappiamo quindi che $H$ è costante, ovvero esiste $c\in \RR$ tale
che $H(x)=c$ per ogni $x\in I$. Dunque si ottiene, come voluto: $G=F+H=F+c$.
\end{proof}

\section{calcolo delle primitive}

In generale quello che ci interessa è trovare una singola primitiva in quanto
in genere tutte le altre si otterranno di conseguenza molto facilmente.
In base alle proprietà delle primitive, infatti, sappiamo che su ogni
intervallo le primitive differiscono per una costante. Osserviamo però
che se la funzione è definita sull'unione di più intervalli allora ogni
intervallo può avere una costante diversa, come si vede nel seguente.

\begin{example}[primitive sugli insiemi non connessi]
\mymark{*}
Consideriamo la funzione $f(x) = 1/x$. Osserviamo che
$f\colon (-\infty, 0) \cup (0,+\infty)\to \RR$ è definita sull'unione di due
intervalli. Per verifica diretta possiamo osservare che la funzione
$F(x) = \ln \abs{x}$ è una primitiva di $f$. Per ottenere l'insieme di tutte
le primitive possiamo aggiungere ad $F$ una qualunque funzione con derivata
nulla sul dominio di $f$. Le funzioni con derivata nulla sono costanti su ogni
intervallo e quindi troviamo che per ogni $c_1, c_2\in \RR$ la funzione
\[
G(x) =
\begin{cases}
  \ln (x) + c_1 &\text{se $x>0$},\\
  \ln (-x) + c_2 & \text{se $x<0$}
\end{cases}
\]
è una primitiva di $f$ e non ci sono altre primitive.

In questo caso lo spazio delle primitive ha dimensione $2$ in quanto il nucleo
dell'operatore derivata sullo spazio delle funzioni definite sull'unione di
due intervalli ha dimensione $2$.
Questo è l'esempio più semplice di un fenomeno piuttosto generale per cui gli
operatori differenziali su uno spazio risultano strettamente legati alla
topologia dello spazio stesso. In questo caso la dimensione del nucleo
dell'operatore differenziale $D$ è uguale al numero di componenti connesse del
dominio delle funzioni nel dominio di $D$.
\end{example}

Come già detto utilizzeremo la notazione $\int f$ per indicare le primitive
della funzione $f$.
Ma invece di scrivere $F\in \int f$
per indicare che $F$ è una primitiva di $f$
potremo scrivere
più semplicemente ma con abuso di notazione $\int f = F$
ricordando (come facevamo con la notazione degli $o$-piccolo) che tale relazione
non è affatto simmetrica.
Ad esempio potremo scrivere:
\[
  \int \cos x\, dx = \sin x.
\]
Si faccia attenzione però che per alcuni la scrittura precedente
è considerata sbagliata, in quanto $\sin x$ non è l'unica primitiva e si dovrebbe
quindi scrivere
\[
  \int \cos x\, dx = \sin x + c.
\]
Ma abbiamo visto che se pretendiamo di scrivere tutte le primitive (e non solo una)
allora sarebbe complicato scrivere il risultato quando la funzione non è definita
su un singolo intervallo. Infatti se scrivessimo:
\[
  \int \frac 1 x\, dx = \ln \abs{x} + c
\]
avremmo comunque scritto solo una parte delle primitive (per quanto visto nell'esempio
precedente).

\begin{theorem}[integrali di alcune funzioni elementari]
\index{integrali!di alcune funzioni elementari}
Si ha
per ogni $\alpha \in \RR$, $\alpha\neq -1$
\begin{gather*}
\int x^\alpha\, dx \ni \frac{x^{\alpha+1}}{\alpha+1},
\qquad
\int \frac{1}{x}\, dx \ni \ln\abs{x}
\\
\int e^x \, dx \ni e^x,
\qquad
\int \cos x\, dx \ni \sin x,
\qquad
\int \sin x\, dx \ni -\cos x
\\
\int \cosh x \, dx \ni \sinh x,\qquad
\int \sinh x \, dx \ni \cosh x, \\
\int \frac{1}{1+x^2}\, dx \ni \arctg x, \qquad
\int \frac{1}{\sqrt{1-x^2}}\, dx \ni \arcsin x, \\
\int \frac{1}{\sqrt{x^2+1}}\, dx \ni \settsinh x = \ln\enclose{x+\sqrt{x^2+1}}\\
\int \frac{1}{\sqrt{x^2-1}}\, dx \ni \settcosh x = \ln\enclose{x+\sqrt{x^2-1}}.
\end{gather*}
\end{theorem}
%
\begin{proof}
E' sufficiente fare riferimento alla corrispondente tabella
delle derivate delle funzioni elementari.
\end{proof}

\begin{theorem}[linearità dell'integrale indefinito]
Per ogni $\lambda,\mu \in \RR$ e se $f$, $g$ sono funzioni qualunque si ha:
\[
  \int \enclose{\lambda f + \mu g} \supset \lambda \int f + \mu \int g
\]
\end{theorem}
%
\begin{proof}
Ogni elemento dell'insieme che si trova sul lato destro
si scrive nella forma $\lambda F + \mu G$ con $F\in \int f$ e $G\in \int g$.
Dunque si ha $F'=f$ e $G'=g$ da cui
\[
  (\lambda F + \mu G)' = \lambda f + \mu g
\]
e quindi
\[
  \lambda F + \mu G \in \int (\lambda f + \mu g)
\]
come dovevamo dimostrare.
\end{proof}

\begin{theorem}[cambio di variabile negli integrali]
Valgono le seguenti proprietà:
\begin{enumerate}
\item
se $g\colon A \to \RR$ è derivabile e $f\colon g(A) \to \RR$
allora
\mymargin{sostituzione diretta}
\index{integrali!sostituzione diretta}
\[
  \int f(g(x)) g'(x)\, dx \supset
  \Enclose{\int f(y) \, dy}_{y=g(x)}
\]
dove si intende
\[
 \Enclose{F(y)}_{y=g(x)} = F(g(x));
\]

\item
se $g\in C^1([a,b])$ e $f\in C^0(g([a,b]))$ allora
\mymargin{cambio di variabile}
\index{integrali!cambio di variabile}
\[
 \int_{g(a)}^{g(b)} f(x)\, dx = \int_a^b f(g(t))\, g'(t)\, dt;
\]

\item
se $g\in C^1([a,b])$ è iniettiva, $f\in C^0(g([a,b]))$
allora $g^{-1}$ è definita su $g([a,b])$
e si ha
\mymargin{sostituzione inversa}
\index{integrali!sostituzione inversa}
\[
  \int f(x)\, dx \supset \Enclose{\int f(g(t)) g'(t)\, dt}_{t=g^{-1}(x)}
\]
\end{enumerate}
\end{theorem}
%
\begin{proof}
Per la prima parte prendiamo una qualunque funzione
$H$ appartenente all'insieme sul lato destro.
Essa sarà della forma $H(x) = F(g(x))$ con $F\in \int f$ ovvero con $F'=f$.
Facciamo la derivata:
\[
  H'(x) = (F(g(x)))' = F'(g(x)) g'(x) = f(g(x)) g'(x).
\]
Abbiamo quindi mostrato che $H$ è una primitiva di $f(g(x))g'(x)$ e quindi è
elemento anche dell'insieme sul lato sinistro:
era quanto dovevamo dimostrare

Per la seconda parte sappiamo che $f$, essendo continua, ammette almeno una
primitiva $F(x)$. Per il punto precedente sappiamo che $F(g(t))$ è una
primitiva di $f(g(t))g'(t)$ (basta farne la derivata per verificarlo).
Dunque, utilizzando la formula fondamentale del calcolo, si ottiene:
\[
\int_{g(a)}^{g(b)} f(x) \, dx
= \Enclose{F(x)}_{g(a)}^{g(b)}
= F(g(b)) - F(g(a))
\]
e
\[
\int_a^b f(g(t))g'(t)\, dt
= \Enclose{F(g(t))}_a^b
= F(g(b)) - F(g(a)).
\]
Le due espressioni sono uguali, come volevamo dimostrare.

Per la terza parte
sia $F$ una qualunque funzione elemento dell'insieme sul lato destro della
relazione che vogliamo dimostrare.
Si avrà $F(x) = H(g^{-1}(x))$ con $H(t)$ primitiva
di $f(g(t))g'(t)$. Ma allora, per la formula fondamentale del calcolo integrale,
si ha
\[
  H(t)-H(a) = \int_a^t f(g(s)) g'(s)\, ds
\]
da cui
\[
  F(x) - F(g(a))
  = H(g^{-1}(x)) - H(a)
  = \int_a^{g^{-1}(x)} f(g(t)) g'(t)\, dt
\]
utilizzando il punto precedente sappiamo però che vale
\[
F(x) - F(g(a)) =
\int_a^{g^{-1}(x)} f(g(t)) g'(t)\, dt
= \int_{g(a)}^x f(s)\, ds.
\]
Dunque derivando ambo i membri, grazie ancora al teorema
fondamentale otteniamo:
\[
  F'(x) = f(x)
\]
cioè $F\in \int f$, come dovevamo dimostrare.
\end{proof}

Le formule del teorema precedente si scrivono usualmente nella forma
\[
  \int f(g(x)) g'(x)\, dx = \int f(y) \, dy
\]
dove si intende che le variabili $x$ e $y$ devono soddisfare la relazione
$y=g(x)$ (o, viceversa, $x=g^{-1}(y)$).
Per memorizzare tale formula si usa normalmente definire il
\emph{differenziale} di una funzione $g$ come $dg(x) = g'(x)\, dx$
(coerentemente con la notazione $g' = dg / dx$)
cosicché se $y=g(x)$ si ha $dy = g'(x)\, dx$.
Non daremo qui una definizione formale di cosa sia un differenziale
ma senz'altro utilizzeremo questa comoda notazione, pensandola
semplicemente come una facilitazione tipografica.

\begin{exercise}
Vogliamo calcolare
\[
  \int \cos^2(x)\, dx.
\]

Ricordando che $\cos(2t) = \cos^2 t - \sin^2 t = 2\cos^2 t - 1$ si
ha $\cos^2 t = (1+\cos(2t))/2$ (formula di bisezione).
Dunque
\[
\int \cos^2(t)\, dt
= \int\frac{1+\cos(2t)}{2}\, dt
= \int \frac 1 2 \, dt + \int \frac{\cos(2t)}{2}\, dt.
\]
Chiaramente $\int \frac 1 2 \, dt  = t/2$.
Nel secondo integrale
possiamo fare un cambio di variabile, ponendo
$2t=s$ da cui $2dt = ds$:
\begin{align*}
\int \frac{\cos(2t)}{2}\, dt
&= \frac 1 4 \int \cos(2t)\, 2dt
= \frac 1 4 \Enclose{\int \cos s \, ds}_{s=2t}
= \frac 1 4 \Enclose{\sin s}_{s=2t}\\
&= \frac 1 4 \sin(2t)
= \frac{1}{2}\sin t \cos t.
\end{align*}
In definitiva otteniamo
\[
  \int \cos^2(x)\, dx = \frac{t+\sin t \cos t}{2}.
\]
\end{exercise}


\begin{example}
Vogliamo calcolare
\[
 \int \sqrt{1-x^2}\, dx.
\]
La funzione integranda è definita per $x\in [-1,1]$.
Ci viene in mente di operare la sostituzione $x=\sin t$
con $t\in [-\pi/2, \pi/2]$.
Osserviamo che su $[-\pi/2,\pi/2]$ la funzione $\sin t$ è derivabile,
invertibile e la sua inversa è $t = \arcsin x$.
Informalmente si ha
\[
 x= \sin t, \qquad dx = \cos t \, dt
\]
da cui si ottiene la formula
\[
 \int \sqrt{1-x^2}\, dx = \Enclose{\int \sqrt{1-\sin^2(t)} \cos t\, dt}_{t=\arcsin x}.
\]
Osserviamo ora che per $t\in [-\pi/2, \pi/2]$ risulta $\sqrt{1-\sin^2(t)}=\cos t$
e dunque l'integrale
diventa
\[
 \int \sqrt{1-\sin^2(t)}\cos t\, dt = \int \cos^2(t)\, dt.
\]
Quest'ultimo integrale lo abbiamo calcolato nell'esercizio precedente.
Dunque otteniamo:
\begin{align*}
 \int \sqrt{1-x^2}\, dx
 &\stackrel{(x=\sin t)}= \int \cos^2(t)\, dt
 = \frac{t + \sin t \cos t}{2} \\
 &= \frac{t + (\sin t) \sqrt{1-\sin^2 t}}{2} \\
 &\stackrel{(t=\arcsin x)}= \frac{\arcsin x + x \sqrt{1-x^2}}{2}.
\end{align*}
\end{example}

\begin{theorem}[integrazione per parti]
\mymark{*}
\mymargin{integrazione!per parti}
Sia $f\colon A\subset \RR \to \RR$ una funzione qualunque, sia $g\colon A \to\RR$
una funzione derivabile
e sia $F \in \int f$.
Allora
\[
  \int f\cdot g \supset F \cdot g - \int F \cdot g'.
\]

In particolare se $f\in C^0([a,b])$ e $g\in C^1([a,b])$
e $F \in \int f$, si ha
\[
  \int_a^b f\cdot g = \Enclose{F\cdot g}_a^b - \int_a^b F \cdot g'.
\]
\end{theorem}
%
\begin{proof}
\mymark{*}
Ogni funzione dell'insieme di destra si scrive nella forma
$F\cdot g - H$ con $H \in \int F \cdot g'$.
Dunque $H' = F \cdot g'$ e, per ipotesi, $F'=f$ da cui
\[
(F\cdot g - H)' = F' \cdot g + F \cdot g' - H' = F' \cdot g
= f\cdot g
\]
che è quanto dovevamo dimostrare.

La seconda parte del teorema
deriva direttamente dalla formula fondamentale del calcolo integrale
(valida in quanto sia $f\cdot g$ che $F \cdot g'$ sono funzioni continue),
osservando che
\[
\Enclose{F\cdot g - \int F \cdot g'}_a^b
= \Enclose{F\cdot g}_a^b - \int_a^b F \cdot g'.
\]
\end{proof}

\begin{example}
Si voglia calcolare
\[
  \int x \cos x\, dx.
\]
Il metodo di integrazione per parti ci permette
di ricondurre l'integrale di un prodotto ad un integrale
di un prodotto in cui uno dei fattori viene integrato e l'altro derivato.
In questo caso sarà conveniente derivare il fattore $x$
e integrare il fattore $\cos x$ in modo da ricondursi all'integrale di
$1\cdot \sin x$, che sappiamo svolgere.
Precisamente si ha
\[
 \int x \cos x\, dx = x \sin x - \int 1 \cdot \sin x \, dx
  = x \sin x + \cos x.
\]
\end{example}

\begin{example}
Si voglia calcolare
\[
 \int e^x \cos x\, dx.
\]
In questo caso se utilizziamo l'integrazione per parti possiamo ricondurre
questo integrale a $\int e^x \sin x$. Integrando ancora per parti ci si
ricondurrà nuovamente ad $\int e^x \cos x$. Se però in questi passaggi si
riottiene la quantità originale con un segno cambiato, si potrà risolvere
l'equazione ottenuta per trovare il risultato cercato.

Precisamente:
\begin{align*}
\int e^x \cos x\, dx
&= e^x \sin x - \int e^x \sin x\, dx\\
 &= e^x \sin x - \Enclose{e^x(-\cos x) - \int e^x(-\cos x)\, dx} \\
 &= e^x \sin x + e^x \cos x - \int e^x \cos x \, dx
\end{align*}
da cui:
\[
 2 \cdot \int e^x \cos x\, dx  = e^x \sin x + e^x \cos x
\]
ovvero
\[
  \int e^x \cos x\, dx = \frac{e^x(\sin x + \cos x)}{2}.
\]
\end{example}

\begin{theorem}[ancora integrali di funzioni elementari]
\mymark{***}
Si ha
\begin{align*}
  \int \ln x\, dx  &= x \ln x - x, \\
  \int \arctg x\, dx &= x \arctg x - \ln \sqrt{1+x^2}.\\
\end{align*}
\end{theorem}
%
\begin{proof}
\mymark{***}
In entrambi i casi l'idea è che la derivata della funzione integranda trasforma
la funzione trascendente in una funzione
razionale. Può quindi risultare utile applicare l'integrazione
per parti nella forma:
\[
  \int f(x)\, dx = \int 1\cdot f(x)\, dx = x f(x) - \int x f'(x)\, dx.
\]
Nel primo caso si ha:
\[
\int \ln x\, dx = x \ln x - \int x \frac{1}{x}\, dx
 = x \ln x - \int 1 dx = x \ln x - x.
\]
Nel secondo caso:
\[
\int \arctg x\, dx = x \arctg x - \int \frac{x}{1+x^2}\, dx.
\]
Operiamo quindi un cambio di variabile $y=1+x^2$:
\begin{align*}
\int \frac{x}{1+x^2}\, dx
&= \frac{1}{2}\int \frac{1}{1+x^2} 2x \, dx
\stackrel{y=1+x^2} = \frac{1}{2}\int \frac 1 y\, dy \\
&= \frac{\ln y}{2} = \frac{1}{2}\ln\enclose{1+x^2}
\end{align*}
da cui, in conclusione:
\[
 \int \arctg x\, dx = x \arctg x - \frac 1 2 \ln\enclose{1+x^2}.
\]
\end{proof}

\begin{remark}
Se si applica il metodo di integrazione per parti nella ricerca della primitiva
della funzione $\frac{1}{x\ln x}$, integrando il fattore $\frac 1 x$ e
derivando il fattore $\frac 1 {\ln x}$ si ottiene un fenomeno a prima vista
sconcertante:
\begin{align*}
  \int \frac{1}{x\ln x}\,dx
  &= \ln x \cdot \frac{1}{\ln x} - \int \ln x \cdot \frac{\frac 1 x}{-\ln^2 x}\, dx\\
  &= 1 + \int \frac{1}{x\ln x}\, dx.
\end{align*}
Si potrebbe infatti pensare di poter semplificare gli integrali ai due lati
dell'uguaglianza per ottenere $0=1$.
Questo non si può fare perché, ricordiamolo, l'integrale indefinito è un insieme
di funzioni (le primitive della funzione $\frac{1}{x\ln x}$ in questo caso)
e sappiamo che sommando una costante ad una primitiva si ottiene un'altra primitiva.
Quindi non ci deve stupire il fatto che sommando $1$ all'insieme delle primitive
questo rimanga invariato.

Per la cronaca: l'integrale in questione può essere calcolato per sostituzione,
ponendo $y=\ln x$ trovando $F(x) = \ln\abs{\ln x}$ come una delle primitive.
\end{remark}
\section{integrale di una funzione razionale}

\begin{definition}[funzione razionale]
\mynote{funzione razionale}
\index{funzione!razionale}
Una funzione $f$ si dice essere \emph{razionale}
se si può scrivere
\[
  f(x) = \frac{P(x)}{Q(x)}
\]
con $P$ e $Q$ funzioni polinomiali a coefficienti reali.
\end{definition}

In questa sezione cercheremo di trovare un metodo per
calcolare esplicitamente l'integrale $\int P(x)/Q(x)\, dx$ di una qualunque funzione razionale.
Per fare ciò vogliamo scrivere il rapporto $P(x)/Q(x)$ come combinazione lineare di funzioni più semplici, di cui saremo in grado di calcolare l'integrale.

\begin{theorem}[decomposizione complessa in fratti semplici]
Siano $u_1, \dots, u_n$ funzioni complesse della forma
\begin{equation}
\label{eq:483818}
  u_k(z) = \frac{1}{(z-\lambda_k)^{p_k}}
\end{equation}
con $\lambda_k\in \CC$, $p_k\in \NN$, $p_k>0$.
Supponiamo che le $u_k$ siano tra loro distinte (cioè se $\lambda_k=\lambda_j$ allora $p_k \neq p_j$).
Sia $\Omega = \CC \setminus\{\lambda_1, \dots, \lambda_k\}$ cosicché tutte le funzioni $u_k$ sono elementi dello spazio vettoriale complesso $V=\CC^\Omega$ cioè sono funzioni complesse definite su tutto $\Omega$.

Allora le funzioni $u_k$ sono linearmente indipendenti come vettori di $V$ ovvero
se esistono dei coefficienti $\alpha_1,\dots, \alpha_n \in \CC$ tali che
\[
\forall z \in \Omega\colon  \sum_{k=1}^n \alpha_k u_k(z) = 0
\]
allora ogni $\alpha_k=0$ per $k=1,\dots,n$.

Di conseguenza se
$P$ e $Q$ sono due polinomi con $\deg P < \deg Q = N$
allora si può scrivere
\begin{equation}\label{eq:46772341}
  \frac{P(z)}{Q(z)} = \sum_{k=1}^n \sum_{j=1}^{p_k} \frac{A_{kj}}{(z-\lambda_k)^j}.
\end{equation}
dove $\lambda_1,\dots, \lambda_n$ sono le radici complesse distinte del polinomio $Q$,
$p_1,\dots, p_n$ sono le rispettive molteplicità con $p_1 + \dots + p_n = \deg Q = N$
e $A_{kj}\in \CC$ sono opportuni
coefficienti.
\end{theorem}
%
\begin{proof}
Procediamo per induzione su $n$. Se $n=1$ abbiamo una unica funzione che non
può essere identicamente nulla (anzi: non si annulla mai).

Supponiamo allora di avere un insieme di $n$ funzioni $u_1, \dots, u_n$
della forma~\eqref{eq:483818}.
Eventualmente riordinando le funzioni
possiamo supporre che $p_n$
(l'esponente associato alla funzione $u_n$)
sia il massimo degli esponenti: $p_n\ge p_k$ per $k=1,\dots,n$.
Supponiamo allora di avere
una combinazione lineare identicamente nulla:
$\alpha_1 u_1 + \dots + \alpha_n u_n = 0$
e consideriamo la funzione $f\colon \Omega \to \CC$
definita da
\begin{equation}
\label{eq:567384}
  f(z) = (z-\lambda)^{p_n}\sum_{k=1}^n \frac{\alpha_k}{(z-\lambda_k)^{p_k}}
    = \sum_{k=1}^{n-1} \alpha_k \frac{(z-\lambda_n)^{p_n}}{(z-\lambda_k)^{p_k}}
    + \alpha_n.
\end{equation}
Questa funzione è identicamente nulla
(in quanto ottenuta moltiplicando la combinazione lineare identicamente
nulla per il fattore $(z-\lambda)^p$).
Ma si osserva che si ha
\[
  \lim_{z\to \lambda_n} f(z) = \alpha_n
\]
in quanto
\[
  \lim_{z\to \lambda_n}\frac{(z-\lambda_n)^{p_n}}{(z-\lambda_k)^{p_k}} = 0
\]
visto che se essendo $u_k\neq u_n$ per $k<n$, deve essere $\lambda_k \neq \lambda_n$
oppure, se $\lambda_k=\lambda_n$ deve essere $p_k < p_n$. Dunque $\alpha_n=0$
e quindi $\alpha_1 u_1 + \dots + \alpha_{n-1}u_{n-1} = 0$. Ci siamo dunque
ricondotti ad una combinazione lineare nulla di $n-1$ funzioni. Per ipotesi
induttiva si ottiene quindi $\alpha_1 = \dots = \alpha_{n-1} = 0$, concludendo
la dimostrazione.

Per dimostrare la seconda parte osserviamo che l'insieme
dei polinomi
\[
  V_N = \{P\colon \text{$P$ polinomio, $\deg P < N$}\}
\]
è un sottospazio vettoriale di $V=\CC^\Omega$ ed ha dimensione
$N$ (una base è data dai monomi $1, z, z^2, \dots, z^{N-1}$).
Di conseguenza è facile verificare che anche lo spazio vettoriale
\[
  W = V_N \cdot \frac{1}{Q} = \left\{\frac{P}{Q}\colon \text{$\deg P < N$}\right\}
\]
delle funzioni razionali che stanno al lato sinistro di~\eqref{eq:46772341},
ha la stessa dimensione $N$.
D'altra parte il lato destro di~\eqref{eq:46772341}
è una combinazione lineare di funzioni $u_{kj} = 1/(z-\lambda_k)^j$ che sono
anch'esse elementi di $W$ in quanto $(z-\lambda_k)^j$ divide $Q(x)$.
Ma per quanto visto prima i vettori $u_{kj}$ sono $N$ vettori indipendenti e quindi
sono una base di $W$. Significa allora che ogni elemento di $W$ può
essere scritto come combinazione lineare degli $u_{kj}$ cioè, per ogni
polinomio $P$ con $\deg P < \deg Q$ si può ottenere
l'uguaglianza~\eqref{eq:46772341}.
\end{proof}

\begin{theorem}[fattorizzazione dei polinomi a coefficienti reali]
Sia $Q(x)$ un polinomio a coefficienti reali. Allora
\begin{equation}\label{eq:35549}
  Q(x) = a \cdot (x-x_1) \cdots (x-x_k)
  \cdot Q_1(x) \cdots Q_m(x)
\end{equation}
dove $a\in \RR$,
$x_1, \dots, x_k$ sono le radici (complesse) non reali di $Q$
(eventualmente ripetute con la loro molteplicità)
e
\[
  Q_j(x) = x^2 + \alpha_j x + \beta_j
\]
per $j=1,\dots, m$
sono polinomi monici di grado due con coefficienti $\alpha_j$ e $\beta_j$ reali e  con discriminante
$\alpha_j^2 - 4 \beta_j$ negativo i cui zeri (complessi coniugati) sono le radici non reali di $Q$.
\end{theorem}
%
\begin{proof}
Per il teorema fondamentale dell'algebra sappiamo che
\begin{equation}\label{eq:45549}
  Q(x) = a \cdot (x - x_1) \cdots (x- x_k)
  \cdot (x-\lambda_1) \cdots (x-\lambda_n)
\end{equation}
dove $x_1, \dots, x_k$ sono le radici reali
del polinomio $Q$ mentre $\lambda_1, \dots, \lambda_n$
sono le radici non reali.

Osserviamo che avendo $Q$ coefficienti reali, per ogni $z\in \CC$  si ha $\overline{Q(z)} = Q(\overline z)$ in quanto il coniugio lascia invariati i coefficienti del polinomio $Q$.
In particolare se $\lambda$ è una radice complessa di $Q$ allora $Q(\bar \lambda) = \overline{Q(\lambda)} = \overline 0 = 0$ cioè anche $\bar \lambda$ è radice di $Q$.
Dunque essendo $\lambda_1$ una radice non reale,
anche $\overline {\lambda_1} \neq \lambda_1$
deve essere una radice di $Q$ e senza perdita di generalità (riordinando le radici)
possiamo supporre che sia $\lambda_2 = \overline{\lambda_1}$.
Osserviamo allora che si ha
\begin{align*}
(x-\lambda_1)(x-\lambda_2)
&=(x-\lambda_1)(x-\overline{\lambda_1})\\
&= x^2 - (\lambda_1+ \overline {\lambda_1}) x + \lambda_1 \overline {\lambda_1} \\
&= x^2 - 2(\Re \lambda_1) x + \abs{\lambda_k}^2\\
&= Q_1(x)
\end{align*}
se poniamo $\alpha_1 = -2\Re \lambda_{k+1}$ e $\beta_1=\abs{\lambda_{k+1}}^2$.
Si osservi che $\alpha_1$ e $\beta_1$ sono reali.
Si itera quindi il procedimento finché non si esauriscono
tutte le radici e si completa quindi la decomposizione.
A posteriori dunque dovrà essere $n=2m$ cosicchè $\deg Q = k + 2m = k + n$.
\end{proof}

\begin{theorem}[decomposizione reale in fratti semplici]
Sia $Q$ un polinomio monico a coefficienti reali che
si fattorizzi nella forma:
\begin{align*}
  Q(x) &= (x-\lambda_1)^{p_1} \cdots (x-\lambda_n)^{p_n} \\
  &\quad \cdot
   (x^2+\alpha_1 x + \beta_1)^{q_1} \cdots (x^2+\alpha_m x + \beta_m)^{q_m}
\end{align*}
con $\lambda_1, \dots, \lambda_n$ le radici reali distinte di $Q$ di molteplicità rispettivamente
$p_1,\dots, p_n$ e $(x^2+\alpha_k x + \beta_k)$ polinomi monici distinti di grado $2$
con discriminante negativo $\alpha_k^2<4\beta_k$ con molteplicità $q_k$ per $k=1, \dots, m$.

Se $P$ è un polinomio a coefficienti reali
con $\deg P < \deg Q$
allora esistono dei coefficienti reali $A_{kj}$ con $k=1,\dots,n $ e $j=1,\dots, p_k$ e coefficienti reali $B_{kj}$, $C_{kj}$ con $k=1,\dots, m$ e $j=1,\dots, q_k$
tali che
\begin{align*}
\frac{P(x)}{Q(x)}
&= \sum_{k=1}^n \sum_{j=1}^{p_k} \frac{A_{kj}}{(x-\lambda_k)^j}
  + \sum_{k=1}^m \sum_{j=1}^{q_k} \frac{B_{kj} + C_{kj}x}{(x^2+\alpha_k x + \beta_k)^j}.
\end{align*}
\end{theorem}
%
\begin{proof}
Diamo un nome alle funzioni coinvolte nell'enunciato del teorema:
\begin{align*}
 u_\lambda^j(z) &= \frac{1}{(z-\lambda)^j},\\
 v_\lambda^j(z) &= \frac{1}{(z-\lambda)^j(z-\bar\lambda)^j},\\
 w_\lambda^j(z) &= z\cdot v_\lambda^j(z).
\end{align*}
Osserviamo che
\[
 (z-\lambda)(z-\bar \lambda)
 = z^2 - 2(\Re\lambda) z + \abs{\lambda}^2
\]
dunque si ha
\[
  \frac{B + Cx}{(x^2 + \alpha x + \beta)^j}
  = B \cdot v_\lambda^j(x) + C \cdot w_\lambda^j(x)
\]
per un opportuna scelta del numero complesso $\lambda$ (per la cronaca: $\lambda = \alpha/2 + i \sqrt{\beta-\alpha^2/4}$).

Nella versione complessa di questo teorema abbiamo già mostrato che le funzioni $u_\lambda^j$ sono indipendenti. Vogliamo ora mostrare che anche le funzioni $u_\lambda^j$, $v_\lambda^j$, $w_\lambda^j$ sono indipendenti e per fare ciò cercheremo di dimostrare che per ogni $\lambda\in \CC\setminus \RR$ fissato, lo spazio generato dalle funzioni
\begin{equation}\label{eq:43841}
u_\lambda^1, u_{\bar \lambda}^1, u_\lambda^2, u_{\bar \lambda}^2, \dots, u_\lambda^p, u_{\bar \lambda}^p
\end{equation}
coincide con lo spazio generato dalle funzioni
\begin{equation}\label{eq:43842}
v_\lambda^1, w_\lambda^1, v_\lambda^2, w_\lambda^2, \dots, v_\lambda^p, w_\lambda^p.
\end{equation}
Visto che le funzioni in \eqref{eq:43841} abbiamo già dimostrato essere indipendenti, sarà sufficiente mostrare che ogni combinazione lineare delle funzioni in \eqref{eq:43841}
si può esprimere come combinazione lineare delle funzioni in
\eqref{eq:43842}.

La dimostrazione si può fare per induzione su $p$.
Nel caso $p=1$
si osserva che
\begin{align*}
  a u_\lambda^1(z) + b u_{\bar \lambda}^1(z)
  &= \frac{a}{z-\lambda} + \frac{b}{z-\bar \lambda}
  = \frac{az - a \bar \lambda + b z - b \lambda}{z^2 -(\lambda + \bar \lambda)z  + \lambda \bar \lambda} \\
  &= \frac{(a+b) z - a \bar \lambda - b \lambda}{z^2 -(\lambda + \bar \lambda)z  + \lambda \bar \lambda}\\
  &= (a+b)\cdot w_\lambda^1(z) - (a \bar \lambda + b \lambda) v_\lambda^1(z).
\end{align*}

Supponendo ora di aver fatto la dimostrazione fino a $p-1$, dimostriamo il caso generico $p$. Una qualunque combinazione lineare delle funzioni \eqref{eq:43841}
si scrive nella forma
\begin{align*}
\sum_{j=1}^p \frac{a_j}{(z-\lambda)^j} + \sum_{j=1}^p \frac{b_j}{(z-\bar\lambda)^j}
&=
\frac{R(Z)}{(z-\lambda)^p(z-\bar \lambda)^p}
\end{align*}
dove $R(Z)$ è un polinomio di grado al più $2p-1$.
Facendo la divisione tra polinomi possiamo scrivere
\[
R(z) = S(z) (z-\lambda)(z-\bar \lambda) + \alpha z + \beta
\]
con $S$ opportuno polinomio di grado al massimo $2p-3$.
Dunque
\begin{align*}
\sum_{j=1}^p \frac{a_j}{(z-\lambda)^j} + \sum_{j=1}^p \frac{b_j}{(z-\bar\lambda)^j}
&=
\frac{S(z)}{(z-\lambda)^{p-1}(z-\bar \lambda)^{p-1}}\\
&\quad + \frac{\alpha z + \beta}{(z-\lambda)^p(z-\bar\lambda)^p}.
\end{align*}
Il primo addendo con numeratore $S(z)$ è una funzione razionale che può essere quindi espressa come combinazione lineare delle funzioni in \eqref{eq:43841} con $p-1$ al posto di $p$. Dunque per ipotesi induttiva tale addendo è combinazione lineare delle funzioni in \eqref{eq:43842} con $p-1$ al posto di $p$. Il secondo addendo non è altro che $\alpha w_\lambda^p + \beta v_\lambda^p$. Abbiamo quindi mostrato che qualunque combinazione lineare delle \eqref{eq:43841} si scrive come combinazione lineare delle \eqref{eq:43842}, come ci eravamo ripromessi di fare.

Ora, per il caso complesso che abbiamo già dimostrato,
sappiamo che la funzione razionale $P/Q$ ammette una decomposizione in fratti semplici complessi cioè può essere scritta come combinazione lineare a coefficienti complessi delle funzioni $u_\lambda^j$ facendo variare $\lambda$ su tutte le radici, reali e complesse, del polinomio $Q$ e facendo variare $j$ fino alla molteplicità di ogni radice.
Ma sappiamo ora che è possibile rimpiazzare le funzioni $u_\lambda^j$ e $u_{\bar \lambda}^j$ quando $\lambda$ non è reale con le funzioni $v_\lambda^j$ e $w_\lambda^j$ in quanto lo spazio generato da tali funzioni è lo stesso.

Questo ci permette di concludere che la decomposizione cercata esiste, se ammettiamo di avere coefficienti $A_{kj}$, $B_{kj}$ e $C_{kj}$ nel campo complesso.

Per concludere ci basta verificare che in realtà tali coefficienti non possono che essere reali.
Questo dipende da un semplice fatto generale.

Supponiamo che $u_1, \dots, u_n$ siano funzioni reali indipendenti e sia
\[
 u = \alpha_1 u_1 + \dots + \alpha_n u_n.
\]
una loro combinazione lineare a coefficienti complessi  $\alpha_k$.
Se la combinazione $u$ è anch'essa una funzione reale allora possiamo concludere che necessariamente tutti i coefficienti $\alpha_k$ sono reali. Infatti se prendiamo la parte immaginaria della combinazione lineare precedente si avrà
\[
   0 = (\Im \alpha_1) u_1 + \dots + (\Im \alpha_n) u_n.
\]
Ma essendo le $u_1,\dots,u_n$ funzioni indipendenti, una combinazione lineare è nulla solamente quando tutti i coefficienti sono nulli: significa che la parte immaginaria di ogni $\alpha_k$ è nulla, cioè che gli $\alpha_k$ sono reali.
\end{proof}

In base ai teoremi precedenti, se $P(x)/Q(x)$ è una qualunque funzione razionale reale,
possiamo innanzitutto
eseguire la divisione tra polinomi e trovare quindi un
quozionte $S(x)$ e un resto $R(x)$ con $\deg R < \deg Q$
cosicché
\[
  \frac{P(x)}{Q(x)} = S(x) + \frac{R(x)}{Q(x)}.
\]
Dopodiché possiamo decomporre $R(x)/Q(x)$
in fratti semplici. L'integrale di $P/Q$ si potrà quindi ricondurre (tramite combinazione lineare) agli integrali di $S$ e di ognuno dei fratti semplici. L'integrale di $S$ è banale, in quanto $S$ è un polinomio e quindi è combinazione lineare di potenze di $x$.

Non ci resta quindi che trovare l'integrale dei fratti semplici, cosa che faremo nel seguente teorema.

\begin{theorem}[integrale dei fratti semplici]
Se $\lambda\in \RR$, $p\in \NN$, $p>1$, $\alpha,\beta \in \RR$, $\alpha^2-4\beta < 0$, si ha:
\begin{align*}
  \int \frac{1}{x-\lambda}\, dx
  &= \ln \abs{x-\lambda} \\
  \int \frac{1}{(x-\lambda)^p}\, dx
  &=  -\frac{1}{(p-1)(x-\lambda)^{p-1}} \\
  \int \frac{1}{1+x^2}\, dx
  &= \arctg x \\
  \int \frac{1}{(1+x^2)^p}\, dx
   &= \frac{x}{2n(1+x^2)^{p-1}} + \frac{2p-3}{2p-2}\int \frac{1}{(1+x^2)^{p-1}}\, dx\\
  \int \frac{1}{x^2+\alpha x + \beta}\, dx
     &=
     \frac{2}{\sqrt{4\beta - \alpha^2}}
     \arctg \frac{2x+\alpha}{\sqrt{4\beta-\alpha^2}} \\
  \int \frac{1}{(x^2+\alpha x + \beta)^p}\, dx
    &=
    \enclose{\frac{4}{4\beta - \alpha^2}}^{p-\frac 1 2}
    \Enclose{\int \frac{1}{(1+y^2)^p}\, dy}_{y=\frac{2x+\alpha}{\sqrt{4\beta-\alpha^2}}}
 \\
  \int \frac{ax + b}{(x^2+\alpha x + \beta)^p}\, dx
   &= -\frac{a}{2(p-1)(x^2+\alpha x+ \beta)^{p-1}} \nonumber\\
   &\quad + \frac{2b-a\alpha}{2}\int \frac{1}{(x^2+\alpha x+\beta)^p}\, dx
\end{align*}

\end{theorem}
Osserviamo che non è rilevante ricordarsi le formule enunciate nel teorema,
converrà piuttosto ricordarsi i metodi di integrazione utilizzati nella dimostrazione.
%
\begin{proof}
I primi tre integrali sono immediati.
Per il quarto si ha:
\begin{align*}
\int \frac{1}{(1+x^2)^p}\, dx
&= \int \frac{1+x^2-x^2}{(1+x^2)^p}\, dx \\
&= \int \frac{1}{(1+x^2)^{p-1}}\, dx - \int\frac{x^2}{(1+x^2)^p}\, dx.
\end{align*}
Osservando che
\[
 \int \frac{2x}{(1+x^2)^p}\, dx
  = -\frac{1}{p-1}\frac{1}{(1+x^2)^{p-1}}.
\]
si ottiene, integrando per parti,
\begin{align*}
\int \frac{x^2}{(1+x^2)^p}\, dx
&=
\int \frac{2x}{(1+x^2)^p}\cdot \frac{x}{2}\, dx\\
&= -\frac{1}{p-1}\frac{1}{(1+x^2)^{p-1}} \frac{x}{2}
 + \frac{1}{2p-2}\int \frac{1}{(1+x^2)^{p-1}}\, dx
\end{align*}
da cui segue il risultato enunciato nel teorema.

Per quanto riguarda il quinto e il sesto integrale si opera il \emph{completamento del quadrato}:
\begin{align*}
  x^2 + \alpha x + \beta
  &= \enclose{x+\frac \alpha 2}^2 + \beta - \frac{\alpha^2}{4} \\
  &= \frac{4\beta-\alpha^2}{4}
  \enclose{\enclose{\frac{2x+\alpha}{\sqrt{4\beta-\alpha^2}}}^2+1}
\end{align*}
da cui, facendo il cambio di variabile
\begin{align*}
y  &=\frac{2x+\alpha}{\sqrt{4\beta -\alpha^2}}\\
dx &= \frac{\sqrt{4\beta-\alpha^2}}{2} dy
\end{align*}
si ottiene
\begin{align*}
\int \frac{1}{(x^2+\alpha x + \beta)^p}\, dx
&=
\frac{4^p}{\enclose{4\beta - \alpha^2}^p}
\int \frac{1}{\enclose{y^2 + 1}^p}\,  \frac{\sqrt{4\beta-\alpha^2}}{2} dy \\
&= \enclose{\frac{4}{4\beta - \alpha^2}}^{p-\frac 1 2}
\int \frac{1}{(1+y^2)^p}\, dy
\end{align*}
che per $p=1$ può essere calcolato immediatamente, e per $p>1$ si riconduce agli integrali già calcolati.

Nell'ultimo integrale dell'enunciato abbiamo semplicemente
utilizzato l'integrale immediato:
\[
  \int \frac{2x+\alpha}{(x^2+\alpha x+\beta)^p}\, dx
  = -\frac{1}{(p-1)(x^2+\alpha x + \beta)^{p-1}}
\]
utilizzandolo per eliminare il termine di grado uno al numeratore della funzione integranda.
\end{proof}

\section{integrali che si riconducono a funzioni razionali}

E' importante sapere che di qualunque funzione razionale è possibile
scriverne la primitiva utilizzando i metodi della sezione precedente.
Allo stesso modo è utile sapere che ci sono altre casistiche che si
riconducono all'integrazione di una funzione razionale.

\emph{Funzioni razionali in $e^x$.}
Se la funzione integranda $f(x)$ si scrive nella forma
\[
  f(x) = R(e^{\lambda x})
\]
con $R$ funzione razionale e $\lambda\neq 0$,
allora si può risolvere l'integrale tramite la
sostituzione $t = e^{\lambda x}$. Infatti si ha
\[
\begin{cases}
 e^{\lambda x} = t\\
 x = \frac{\ln t}{\lambda}\\
 dx = \frac{1}{\lambda t}
 \end{cases}
\]
e la funzione integranda diventa una funzione razionale:
\[
 \int R(e^{\lambda x})\, dx = \Enclose{\int \frac{R(t)}{\lambda t}\, dt}_{t=e^{\lambda x}}
\]

\begin{example}
Si voglia risolvere l'integrale
\[
  \int \frac{2\sqrt{e^x} + e^{2x}}{e^x-4}\, dx.
\]
\end{example}
\begin{proof}[Soluzione.]
Scriviamo la funzione integranda in funzione di $e^{\frac x 2}$:
\[
  \frac{2\sqrt{e^x} + e^{2x}}{e^x-4}
  =\frac{2e^{\frac x 2}+e^{4\frac x 2}}{e^{2\frac x 2}-4}.
\]
Facendo il cambio di variabile $t=e^{\frac x 2}$, $x=2\ln t$, $dx=\frac 2 t\, dt$
si ottiene una funzione razionale in $t$:
\[
  \int \frac{2\sqrt{e^x} + e^{2x}}{e^x-4}\, dx
  = \int \frac{2t + t^4}{t^2-4}\cdot \frac 2 t dt
  = 2\int \frac{2+t^3}{t^2-4}\, dt.
\]
Facendo la divisione tra i polinomi e la riduzione ai fratti semplici si ottiene
\[
 \int 2t\, dt + \int \frac{5}{t-2}\, dt + \int \frac{3}{t+2}\, dt
 = t^2 + 5\ln\abs{t-2} + 3\ln\abs{t+2}
\]
e quindi sostituendo $t=e^{\frac x 2}$ si ottiene il risultato
\[
 e^x + 5 \ln \abs{\sqrt{e^x}-2} + 3 \ln\enclose{\sqrt{e^x}+2}.
\]
\end{proof}

\emph{Funzioni razionali in $\sin^2 x$, $\cos^2 x$, $\sin x \cdot \cos x$.}
Se la funzione integranda $f(x)$ si scrive nella forma
\[
  f(x) = R(\sin^2 x, \cos^2 x, \sin x \cdot \cos x)
\]
con $R$ funzione razionale (cioè rapporto di polinomi nelle tre variabili indicate)
 allora si può risolvere l'integrale
tramite la sostituzione $t=\tg x$. Infatti
osservando che risulta
\[
  1 + \tg^2 x = 1 + \frac{\sin^2 x}{\cos^2 x} = \frac{1}{\cos^2 x}
\]
da cui
\begin{align*}
\cos^2 x &= \frac{1}{1+\tg^2 x}\\
\sin^2 x &= \tg^2 x \cdot\cos^2 x\\
\sin x \cdot \cos x &= \tg x \cdot \cos^2 x
\end{align*}
ponendo $t = \tg x$ si ha:
\begin{equation}\label{eq:466324}
\begin{cases}
  \cos^2 x = \frac{1}{1+t^2}\\
  \sin^2 x = \frac{t^2}{1+t^2}\\
  \sin x \cdot \cos x = \frac{t}{1+t^2}\\
  dx = \frac{1}{1+t^2}\, dt
\end{cases}
\end{equation}
e la funzione integranda diventa una funzione razionale.

\begin{example}
\label{ex:35663}
Si voglia calcolare
\[
  \int \frac{1}{\cos x \cdot ( \sin x + \cos x)}\, dx.
\]
\end{example}
\begin{proof}[Soluzione.]
La funzione integranda si può scrivere nella forma
\[
  \frac{1}{\sin x \cdot \cos x + \cos^2x}.
\]
Effettuando la sostituzione~\eqref{eq:466324}
si ottiene
\[
  \int \frac{1}{\frac t {1+t^2} + \frac{1}{1+t^2}}\cdot \frac{1}{1+t^2}\, dt
  = \int \frac{1}{t+1}\, dt = \ln \abs{t+1} = \ln \abs{\tg x + 1}.
\]
\end{proof}

\emph{Più in generale funzioni razionali di $\sin x$ e $\cos x$.}
Se la funzione integranda $f(x)$ si scrive nella forma
\[
  f(x) = R(\sin x, \cos x)
\]
con $R$ funzione razionale, allora si può risolvere l'integrale
tramite la sostituzione $t=\tg \frac{x}{2}$. Infatti con tale sostituzione si ha
(usando le formule di bisezione e riconducendosi al caso precedente)
\begin{equation}\label{eq:3675323}
  \begin{cases}
    \cos x = \cos^2 \frac x 2 - \sin^2 \frac x 2 = \frac{1-t^2}{1+t^2} \\
    \sin x = 2 \sin x \cos x = \frac{2t}{1+t^2}\\
    dx = \frac{2}{1+t^2}\, dt.
  \end{cases}
\end{equation}
Di nuovo con questa sostituzione la funzione integranda diventa razionale.

\begin{remark}
Si osservi che la sostituzione~\eqref{eq:3675323} potrebbe essere
sempre utilizzata al posto della~\eqref{eq:466324} in quanto più generale.
Ma, usualmente, se è possibile usare la sostituzione~\eqref{eq:466324}
l'integrale risulta
poi più semplice da calcolare. Si faccia la prova con l'integrale
dell'esempio~\ref{ex:35663}!
\end{remark}

\begin{example}
Si voglia calcolare
\[
 \int \frac{1}{\sin x}\, dx.
\]
\end{example}
\begin{proof}[Soluzione.]
Utilizzando la sostituzione~\eqref{eq:3675323}
si ottiene
\begin{align*}
  \int \frac{1}{\sin x}\, dx
  &= \int \frac{1}{\frac{2t}{1+t^2}}\frac{2}{1+t^2}\, dt \\
  &= \int \frac{1}{t}\, dt = \ln\abs t = \ln \abs{\tg \frac x 2}.
\end{align*}
\end{proof}

\emph{Funzioni razionali con radicali.}
Se la funzione $f(x)$ si scrive nella forma:
\[
  f(x) = R(\sqrt[n] x)
\]
con $R$ funzione razionale, allora si può risolvere l'integrale tramite
la sostituzione $t = x^n$. Infatti con tale sostituzione si ha
\begin{equation}\label{eq:4675821}
\begin{cases}
  \sqrt[n] x = t\\
  dx = n t^{n-1}\, dt.
\end{cases}
\end{equation}

\begin{example}
Si voglia calcolare
\[
  \int \frac{\sqrt[4]{x}}{\sqrt[2]{x} + \sqrt[3]{x}}\, dx.
\]
\end{example}
\begin{proof}[Soluzione.]
Si osservi che la funzione integranda può essere scritta
come funzione razionale di $t=\sqrt[12]{x}$ (abbiamo scelto
il minimo comune multiplo tra i radicandi in gioco: $12 = mcm(4,2,3)$)
\[
  \frac{\sqrt[4]{x}}{\sqrt{x} + \sqrt[3]{x}}
  = \frac{\sqrt[12]{x^3}}{\sqrt[12]{x^6} + \sqrt[12]{x^4}}.
\]
Dunque utilizzando la sostituzione \eqref{eq:4675821} con $n=12$ si ha
\begin{align*}
\int \frac{t^3}{t^6 + t^4}\cdot 12 t^{11}\, dt
&= 12 \int \frac{t^{14}}{t^4+t^4}\, dt
 = 12 \int \frac{t^{10}}{t^2+1}\, dt.
\end{align*}
Procedendo con la divisione tra polinomi si ottiene
\begin{align*}
\MoveEqLeft{12 \int \Enclose{t^8 - t^6 + t^4 - t^2 + 1 - \frac{1}{t^2+1}}\, dt} \\
&= 12 \Enclose{\frac{t^9}{9} - \frac{t^7}{7} + \frac{t^5}{t} - \frac{t^3}{3} + t - \arctg t} \\
&= \frac 4 3 \sqrt[4]{x^3} - \frac{12}{7}\sqrt[12]{x^7}
+ \frac{12}{5}\sqrt[12]{x^5} - 4 \sqrt[4]{x} + 12 \sqrt[12]{x} - 12 \arctg \sqrt[12]{x}.
\end{align*}
\end{proof}

\section{integrali impropri}

La definizione di integrale di Riemann è stata data solamente per funzioni
limitate definite su un intervallo
limitato.
Vogliamo ora estendere la definizione di integrale alle funzioni illimitate e
agli intervalli illimitati.
Lo faremo riconducendoci, tramite un limite, al caso già studiato.

\begin{definition}[integrabilità locale]
Sia $f\colon A \to \RR$ una funzione definita su un insieme $A\subset \RR$.
Diremo che $f$ è \emph{localmente Riemann-integrabile} se
per ogni intervallo chiuso e limitato $[\alpha,\beta]\subset A$ risulta
che $f$ sia limitata e Riemann-integrabile su $[\alpha,\beta]$.
\end{definition}

\begin{remark}
Osserviamo che se $f\colon A \to \RR$ è una funzione continua, allora su ogni
intervallo $[\alpha,\beta]\subset A$ la funzione $f$ risulta essere continua
e quindi Riemann-integrabile per il teorema~\ref{th:integrabilita_continue}.
Dunque ogni volta che ci viene richiesta l'integrabilità locale
sarà sufficiente verificare la continuità della funzione.
Lo stesso vale per le funzioni monotone: grazie al teorema~\ref{th:integrabilita_monotone}
sappiamoe che una funzione monotona e
Riemann-integrabile su ogni intervallo $[\alpha,\beta]$ dunque anch'essa
è localmente Riemann-integrabile.
\end{remark}

\begin{definition}[integrale improprio]
\label{def:integrale_improprio}
\index{integrale!improprio}
Se $f$ è una funzione localmente Riemann-integrabile sull'intervallo $[a,b)$
con $a\in \RR$, $b\in (a,+\infty]$ definiamo l'integrale improprio
di $f$ su $[a,b)$ come:
\[
  \int_a^b f(x)\, dx = \lim_{\beta \to b^-} \int_a^\beta f(x)\, dx
\]
se il limite a lato destro esiste (finito o infinito).

Se $f$ è una funzione localmente Riemann-integrabile sull'intervallo $(a,b]$
con $b\in \RR$, $a\in [-\infty,b)$ definiamo l'integrale improprio
di $f$ su $(a,b]$ come:
\[
  \int_a^b f(x)\, dx = \lim_{\alpha \to a^+} \int_\alpha^b f(x)\, dx
\]
se il limite esiste (finito o infinito).

Se $f$ è una funzione localmente Riemann-integrabile sull'intervallo $(a,b)$
con $a\in [-\infty,+\infty)$, $b\in(a,+\infty]$ preso un qualunque
punto $c\in (a,b)$
definiamo l'integrale improprio di $f$ su $(a,b)$ come:
\begin{align*}
  \int_a^b f(x)\, dx &=
  \int_a^c f(x)\, dx + \int_c^b f(x)\, dx \\
  &= \lim_{\alpha \to a^+}\int_\alpha^c f(x)\,dx
    + \lim_{\beta\to b^-}\int_c^\beta f(x)\, dx
\end{align*}
sempre che entrambi i limiti esitano (finiti o infiniti) e non siano infiniti
di segno opposto (cosicché la loro somma è ben definita). In base
all'osservazione~\ref{rem:4821341} l'integrale non dipende
dal punto $c$ scelto.

Se $I$ è un intervallo di estremi $a,b\in [-\infty, +\infty]$, $a<b$
ed esistono un numero finito di punti $x_0,\dots,x_n \in I$ tali che
$a = x_0 < x_1 < \dots < x_n = b$ e se
$f$ risulta essere localmente Riemann-integrabile sull'insieme
$A=I\setminus\{x_0, \dots, x_n\}$ allora definiamo
l'integrale improprio di $f$ su $A$ come:
\[
  \int_a^b f(x)\, dx = \sum_{k=1}^n \int_{x_{k-1}}^{x_k} f(x)\, dx
\]
se ogni integrale improprio $\int_{x_{k-1}}^{x_k} f(x)\, dx$ esiste (finito o infinito)
e se la somma è ben definita in quanto tutti gli integrali infiniti
hanno lo stesso segno.

Se l'integrale improprio $\int_a^b f(x)\, dx$ esiste ed è finito
(in base ad una delle definizioni precedenti), diremo che
la funzione $f$ è
\mynote{integrabile}
\emph{integrabile in senso improprio}
e diremo che l'integrale \emph{converge}.
\mynote{convergente}%
\index{congergenza!integrale}%
\index{integrale!convergente}%
\index{integrale!improprio!convergente}%
Se invece l'integrale esiste ma non è finito, diremo che l'integrale
\emph{diverge}.
\mynote{divergente}%
\index{divergenza!integrale}%
\index{integrale!divergente}%
\index{integrale!improprio!divergenza}%
Negli altri casi diremo che l'integrale è
\emph{indeterminato}.
\mynote{indeterminato}%
\index{integrale!improprio!indeterminato}%
\index{integrale!indeterminato}%

Determinare il \emph{carattere}
\mynote{carattere}%
\index{carattere!dell'integrale}%
\index{integrale!carattere}%
\index{integrale!improprio!carattere}%
dell'integrale $\int_a^b f(x)\, dx$
significa dire se tale integrale è convergente, divergente o indeterminato.

Se gli estremi di integrazione sono scambiati, $a>b$,
risulta utile utilizzare anche per gli integrali impropri
la convenzione
già introdotta per gli integrali di Riemann:
\[
  \int_a^b f(x) \, dx = - \int_b^a f(x)\, dx.
\]
Se gli estremi coincidono, $a=b$,
si intende che qualunque sia la funzione $f$ (anche non definita in $a$)
valga per convenzione:
\[
  \int_a^a f(x)\, dx = 0.
\]
\end{definition}

\begin{remark}\label{rem:4821341}
Nella definizione di integrale improprio sull'intervallo
aperto $(a,b)$ la scelta del punto $c\in(a,b)$ non influenza la definizione.
Se infatti scegliamo due diversi punti $c,c'\in(a,b)$ per ogni $\alpha,\beta \in (a,b)$
si ha, grazie alla additività dell'integrale di Riemann:
\[
  \int_\alpha^c f(x)\,dx + \int_c^\beta f(x)\,dx =
  \int_\alpha^{c'} f(x)\, dx + \int_{c'}^\beta f(x)\, dx.
\]
E quindi i limiti, se esistono, sono uguali.
\end{remark}

\begin{example}
La funzione $f(x)= \ln x$ può essere integrata in senso improprio sull'intervallo
$[1,+\infty)$ e si ha:
\begin{align*}
 \int_1^{+\infty} f(x)\,dx
 &= \lim_{\beta\to +\infty} \int_1^\beta \ln(x)\, dx
 = \lim_{\beta\to +\infty}\Enclose{x\ln x - x}_1^\beta \\
 &= \lim_{\beta\to +\infty} \enclose{\beta \ln \beta - \beta + 1}
 = +\infty.
\end{align*}
Anche sull'intervallo $(0,1]$ la funzione $\ln x$ ha integrale improprio
\[
  \int_0^1 \ln x\, dx = \lim_{\alpha \to 0^+}\Enclose{x \ln x -x}_\alpha^1
  = \lim_{\alpha\to 0^+} (-1-\alpha \ln \alpha + \alpha)  = -1.
\]
Dunque sull'intervallo $(0,+\infty)$ la funzione $\ln x$ ha integrale improprio
\[
  \int_0^{+\infty}\ln x\, dx = \int_0^1 \ln x\, dx + \int_1^{+\infty}\ln x\, dx
   = -1 + (+\infty) = +\infty.
\]
\end{example}

Per abbreviare le notazioni intenderemo:
\[
  \Enclose {F(x)}_a^b
  = \lim_{x\to b^-}F(x) - \lim_{x\to a^+}F(x)
\]
osservando che se $F$ è definita e continua agli estremi $a$ e $b$ questa notazione coincide con l'usuale
\[
\Enclose{F(x)}_a^b = F(b) - F(a).
\]

Potremo quindi scrivere:
\begin{align*}
  \int_0^1 \ln x\, dx
  &= \Enclose{x\ln x- x}_0^1
  = 1 \cdot\ln 1 -1 - \lim_{x\to 0^+} (x\ln x - x) \\
  &= -1  - 0 = -1.
\end{align*}

Osserviamo che se $f\colon(a,b) \to \RR$ è continua e $F\colon(a,b)\to \RR$
è una sua primitiva
allora si avrà
\begin{align*}
  \int_a^b f(x)\, dx
   &= \lim_{\alpha\to a^+}\int_{\alpha}^c f(x)\, dx
   + \lim_{\beta\to b^-}\int_c^{B}\, dx \\
   & = \lim_{x\to a} (F(c)-F(x)) + \lim_{x\to b} (F(x)-F(c)) \\
   & = \lim_{x\to b} F(x) - \lim_{x\to a} F(x)
   = \Enclose{F(x)}_a^b.
\end{align*}
Dunque la formula fondamentale del calcolo integrale rimane formalmente identica
per gli integrali impropri.

\begin{example}
Per calcolare l'integrale
\[
  \int_{-\infty}^{+\infty} \frac{1}{1+x^2}\, dx
\]
possiamo scegliere un qualunque punto $c\in \RR$ e calcolare
\begin{align*}
\int_{-\infty}^{+\infty}\frac{1}{1+x^2}\, dx
&=
\lim_{\alpha\to-\infty}\int_\alpha^c \frac{1}{1+x^2}\, dx
  + \lim_{\beta\to+\infty}\int_c^\beta \frac{1}{1+x^2}\, dx\\
  &= [\arctg x]_{-\infty}^c
  + [\arctg x]_c^{+\infty}  \\
  &= \arctg c - (-\frac\pi 2) + \frac \pi 2 - \arctg c
  = \pi.
\end{align*}

Ma più semplicemente possiamo scrivere:
\[
 \int_{-\infty}^{+\infty}\frac{1}{1+x^2}\, dx
 = \Enclose{\arctg x}_{-\infty}^{+\infty}
 = \frac \pi 2 - \enclose{-\frac \pi 2} = \pi.
 \]

\end{example}

\begin{remark}
Se $f\colon[a,b]\to\RR$ è limitata e Riemann-integrabile su
$[a,b]$ allora,
in base al teorema~\ref{th:integrale_continuo}
si ha:
\[
 \int_a^b f(x)\, dx
  = \lim_{\alpha \to a^+} \int_\alpha^b f(x)\, dx
  = \lim_{\beta\to b^-}\int_a^\beta f(x)\,dx.
\]
Dunque in questo caso gli integrali impropri su $[a,b)$ e su $(a,b]$ coincidono
con l'usuale integrale di Riemann (e sono quindi convergenti).
Allo stesso modo se $f$ è localmente integrabile su $[a,b)$
l'integrale improprio su $[a,b)$ e l'integrale improprio su
$(a,b)$ coincidono.
Lo stesso succede se la funzione è localmente integrabile
su $(a,b)$ e consideriamo l'insieme $A=(a,b)\setminus\{x_0,\dots,x_n\}$.
L'integrale su $(a,b)$ coincide con l'integrale su $A$ grazie all'additività
dell'integrale rispetto al dominio.

Questo giustifica l'aver utilizzato la stessa notazione $\int_a^b$ sia per l'integrale
di Riemann su $[a,b]$ sia per i diversi integrali impropri su $[a,b)$, $(a,b]$,
$(a,b)$ o $(a,b)\setminus\{x_0, \dots, x_n\}$.
\end{remark}

\begin{example}
La funzione $\sin x$ pur essendo integrabile su ogni intervallo chiuso
e limitato (in quanto funzione continua) non ammette integrale
improprio sull'intervallo $[0,+\infty)$ in quanto l'integrale:
\[
  \int_0^\beta \sin(x)\, dx
  = \Enclose{-\cos(x)}_0^\beta
  = -\cos(\beta)+\cos(0) = 1-\cos(\beta)
\]
non ammette limite per $\beta\to +\infty$.
\end{example}

\begin{example}
La funzione $\ln x$ ammette integrale improprio sull'intervallo $(0,+\infty)$
in quanto si ha:
\begin{align*}
 \int_0^{+\infty}\ln x \, dx
 &= \Enclose{x \ln x - x}_0^{+\infty}
 = +\infty.
\end{align*}
Ma visto che l'integrale diverge diremo che la funzione non è integrabile su
tale intervallo.
\end{example}

\begin{example}
La funzione $2x/(x^2+1)$ non è integrabile in senso improprio su $\RR$ in quanto si ha
\begin{align*}
\int_{0}^{+\infty} \frac {2x} {1+x^2}\, dx
 &= \Enclose{\ln (1+x^2)}_0^{+\infty} = +\infty \\
\int_{-\infty}^0 \frac{2x}{1+x^2}\, dx
 &= \Enclose{\ln(1+x^2)}_{-\infty}^0 = -\infty
\end{align*}
e $(+\infty)+ (-\infty)$ è indefinito.
\end{example}

\begin{example}
Si ha
\begin{align*}
  \int_{-1}^2 \frac{1}{\sqrt[3]{x}}\, dx
  &= \int_{-1}^0 \frac{1}{\sqrt[3] {x}}\, dx
   + \int_{0}^2 \frac{1}{\sqrt[3] x}\, dx \\
  &= \Enclose{\frac 3 2 \sqrt[3] {x^2}}_{-1}^0  + \Enclose{\frac 3 2 \sqrt[3] {x^2}}_0^2
  = 0 - \frac 3 2 + \frac 3 2 \sqrt[3]{4} - 0 \\
  &= \frac 3 2 (1+\sqrt[3] 4).
\end{align*}
\end{example}

\begin{example}
Si ha
\[
  \int_0^{+\infty} \frac{1}{x}\, dx
  = \Enclose{\ln x}_0^{+\infty}
  = +\infty - (-\infty) = +\infty
\]
e
\[
  \int_{-\infty}^{0} \frac{1}{x}\, dx
  = \Enclose{\ln(-x)}_{-\infty}^{0}
  = -\infty - (+\infty) = -\infty.
\]
Allora l'integrale
\[
  \int_{-\infty}^{+\infty} \frac{1}{x}\, dx
\]
non è definito in quanto somma di infiniti di segno opposto.
\end{example}

\begin{remark}
Attenzione a non dimenticare i punti ``cattivi'' all'interno dell'intervallo
di integrazione. Se vogliamo valutare:
\[
  \int_{-1}^1 \frac{1}{x}\, dx
\]
Potremmo essere portati a pensare che questo integrale sia uguale a:
\[
 \Enclose{\ln \abs{x}}_{-1}^1 = \ln \abs{1} - \ln \abs{-1} = 0.
\]
Invece questo integrale non è definito in quanto bisogna considerare
che la funzione integranda non è definita e comunque non è limita in un
intorno di $x=0$ e quindi l'intervallo $[-1,1]$ va spezzato nell'unione
dei due intervalli $[-1,0)$ e $(0,1]$ da cui:
\[
  \int_{-1}^1 \frac{1}{x}\, dx = \Enclose{\ln \abs{x}}_{-1}^0 + \Enclose{\ln\abs{x}}_0^{1}
   = -\infty + (+\infty)
\]
ed essendoci una somma di infiniti con segno opposto la somma non ha senso
e l'integrale improprio non è definito.
\end{remark}

\begin{theorem}[proprietà dell'integrale improprio]
\label{th:proprieta_integrale_improprio}
Se $f\le g$ e se entrambi gli integrali sono definiti, risulta:
\[
  \int_a^b f(x)\, dx \le \int_a^b g(x)\, dx.
\]

Siano $a,b,c \in \bar \RR$ con $a\le c \le b$.
Allora nella seguente uguaglianza
\[
  \int_a^b f(x)\, dx = \int_a^c f(x)\, dx + \int_c^b f(x)\, dx
\]
se almeno uno dei due lati è definito allora anche l'altro lato lo è e
l'uguaglianza è valida.

Siano $\lambda, \mu \in \RR$.
\[
  \int_a^b \enclose{\lambda f(x) + \mu g(x)}\, dx
  = \lambda \int_a^b f(x) + \mu \int_a^b g(x)
\]
se il lato destro è ben definito (esistono gli integrali impropi di $f$ e $g$
e le operazioni hanno senso).

Sia $g\colon (a,b)\to (c,d)$ una funzione $C^1$
e $f\colon (c,d) \to \RR$ una funzione continua
e integrabile in senso improprio su $(c,d)$.
Siano $-\infty \le a < b \le +\infty$ e
$-\infty \le c < d \le +\infty$ tali che
\[
c = \lim_{x\to a^+} g(x),
\qquad
d = \lim_{x\to b^-} g(x).
\]
Se $f$ è integrabile in senso improprio su $(c,d)$
allora $f\circ g$ è integrabile su $(a,b)$ e
si ha
\[
\int_a^b f(g(x)) g'(x)\, dx = \int_c^d f(y)\, dy.
\]

Se invece gli estremi sono scambiati:
\[
d = \lim_{x\to a^+} g(x),
\qquad
c = \lim_{x\to b^-} g(x)
\]
si avrà, nelle stesse ipotesi:
\[
  \int_a^b f(g(x))\, dx = \int_d^c f(y)\, dy.
\]
\end{theorem}
%
\begin{proof}
Tutte queste proprietà sono già state dimostrate per l'integrale delle funzioni
limitate su intervalli limitati. Si possono facilmente estendere all'integrale
improprio laterale osservando che il limite mantiene le proprietà richieste.
Di conseguenza le proprietà valgono per additività anche per l'integrale
improprio bilaterale, facendo attenzione che non si produca una somma
indeterminata $+\infty + (-\infty)$.
Infine, sempre per additività, le formule sono valide per gli integrali
impropri multilaterali.
\end{proof}

\begin{theorem}[integrabilità delle funzioni positive]
\label{th:integrabilita_positive}
\mymark{**}
Sia $f\colon (a,b)\to \RR$ una funzione localmente Riemann-integrabile
non negativa: $f\ge 0$. Allora l'integrale di $f$ esiste (finito o infinito)
ed è non negativo:
\[
  \int_a^b f(x)\, dx \ge 0.
\]
\end{theorem}
%
\begin{proof}
\mymark{**}
Sia $c\in (a,b)$ un punto fissato e sia
\[
  F(x) = \int_c^x f(t)\, dt.
\]
Essendo $f$ localmente Riemann-integrabile la funzione $F$ è ben definita
ed essendo $f\ge 0$ risulta che $F$ è crescente in quanto se $x_2>x_1$
essendo $f\ge 0$ si ha
\[
  F(x_2)-F(x_1) = \int_{x_1}^{x_2} f(t)\, dt \ge \int_{x_1}^{x_2} 0\, dt = 0.
\]
Dunque esistono i limiti:
\begin{align*}
  \int_c^b f(x)\, dx  &= \lim_{x\to b^-} F(\beta)\\
  \int_a^c f(x)\, dx  &= \lim_{x\to a^+} -F(\alpha)
\end{align*}
ed essendo $F$ crescente entrambi i limiti sono non negativi e quindi
la somma dei due limiti è ben definita.
\end{proof}

Anche se non sappiamo calcolare esplicitamente un integrale,
è spesso possibile determinarne la convergenza confrontandolo,
tramite il teorema seguente, con un integrale noto.

\begin{theorem}[criterio di confronto e confronto asintotico]
\mymark{**}
Siano $f, g \colon$ $ [a,b)\to \RR$ (con $a \le b \le +\infty$)
funzioni localmente Riemann-integrabili. Supponiamo inoltre che
per ogni $x\in [a,b)$ si abbia $f(x)\ge 0$ e $g(x)\ge 0$.
In queste ipotesi sappiamo che i due integrali:
\begin{equation}\label{eq:921754}
  \int_a^b f(x)\, dx, \qquad \int_a^b g(x)\, dx
\end{equation}
esistono entrambi (finiti o infiniti) e sono non negativi.
Inoltre abbiamo i seguenti criteri di confronto.

\begin{enumerate}
\item \emph{Confronto puntuale ``$\le$''.}
Supponiamo che per ogni $x\in [a,b)$ si abbia $f(x) \le g(x)$. Allora
si ha:
\begin{equation}\label{eq:467143}
  \int_a^b f(x)\, dx \le \int_a^b g(x)\, dx.
\end{equation}
In particolare se l'integrale di $g$ è convergente anche l'integrale di
$f$ è convergente mentre se l'integrale di $f$ è divergente anche l'integrale
di $g$ è divergente.

\item \emph{Confronto asintotico ``$\ll$''.}
Supponiamo che per $x\to b^-$ si abbia $f \ll g$ (cioè $f/g\to 0$).
Allora risulta che
se l'integrale di $g$ è convergente allora anche l'integrale di $f$ è convergente
mentre se l'integrale di $f$ è divergente anche l'integrale di $g$ è divergente.

\item \emph{Confronto asintotico ``$\sim$''.}
Suponiamo che per $x\to b^-$ si abbia $f\sim g$ (cioè $f/g\to 1$).
Allora i due integrali~\eqref{eq:921754}
hanno lo stesso carattere (entrambi convergenti oppure entrambi divergenti).
\end{enumerate}

Risultati analoghi valgono per funzioni definite su un intervallo aperto a
sinistra: $(a, b]$ con $-\infty \le a \le b$,
in tal caso nei criteri asontotici si faranno i limiti per $x\to a^+$.
\end{theorem}
%
\begin{proof}
\mymark{**}
Gli integrali di $f$ e $g$ esistono grazie al teorema~\ref{th:integrabilita_positive}.

Se $f\le g$ la disuguaglianza~\eqref{eq:467143} è garantità dalla proprietà di monotonia
(teorema~\ref{th:proprieta_integrale_improprio}).

Se $f(x)/g(x) \to 0$ per $x\to b^-$ significa che esiste un punto $c\in[a,b)$
tale che per ogni $x\in [c,b)$ si ha $f(x)/g(x)\le 1$ cioè $f(x)\le g(x)$.
Dunque, per l'additività e la monotonia dell'integrale, possiamo affermare che
\[
  \int_a^b f
  = \int_a^c f + \int_c^b f
  \le \int_a^c f + \int_c^b g
  = \int_a^c f + \int_a^b g - \int_a^c g.
\]
Sappiamo che gli integrali di $f$ e $g$ sull'intervallo
$[a,c]$ sono convergenti in quanto $f$ e $g$ sono
limitate e Riemann-integrabili su $[a,c]$.
Dunque se l'integrale $\int_a^b g$ è convergente anche l'integrale
$\int_a^b f$ è convergente.
Viceversa se $\int_a^b f$ è divergente allora anche $\int_a^b g$ è divergente.

Nel caso in cui $f(x)/g(x)\to 1$ per $x\to b^-$ possiamo trovare
un punto $c\in[a,b)$ tale per cui $\frac 1 2 \le f(x)/g(x) \le 2$
per ogni $x\in [c,b)$. Si avrà allora per ogni $x\in [c,b)$
\[
f(x) \le 2g(x), \qquad g(x) \le 2 f(x)
\]
e si potrà quindi procedere come nel caso precedente.
\end{proof}

Nei casi più frequenti il teorema precedente si applica confrontando la funzione con una potenza:
\[
  (x-x_0)^\alpha, \qquad \alpha \in \RR.
\]

Sarà quindi utile sapere, come termine di paragone,
per quali $\alpha$ queste funzioni hanno integrale convergente
come nel seguente esempio.

\begin{example}
\label{ex:416145}%
\mymark{***}%
Sia $a>0$ e $p\in \RR$. Gli integrali
\[
  \int_{a}^{+\infty}\frac{1}{x^p}\, dx,
  \qquad
  \int_{-\infty}^{-a}\frac{1}{x^p}\, dx
\]
sono convergenti se e solo se $p>1$.

Sia $x_0\in [a,b]$ e $p\in\RR$.
Gli integrali
\[
  \int_a^{x_0} \frac{1}{(x-x_0)^p},
  \qquad
  \int_{x_0}^b \frac{1}{(x-x_0)^p}
\]
sono convergenti se e solo se $p<1$.

La verifica si fa facilmente valutando il limite della primitiva
$F(x) = x^{1-p}/(1-p)$ negli estremi dell'intervallo.
\end{example}

\begin{example}
L'integrale
\[
 \int_{-\infty}^{+\infty}e^{-x^2}\, dx
\]
è convergente in quanto per $x\to +\infty$ risulta $e^{-x^2} \ll 1/x^2$
e l'integrale di $1/x^2$ è convergente (esempio~\ref{ex:416145})
in un intorno di $+\infty$ e di $-\infty$.

L'integrale
\[
  \int_{-\infty}^{+\infty}\frac{1}{1+x^2}\, dx
\]
è convergente in quanto per $x\to +\infty$ risulta $1/(1+x^2) \sim 1/x^2$.

L'integrale
\[
  \int_1^{+\infty} \frac{1}{\ln x}\, dx
\]
è divergente perché per $x\to +\infty$
si ha $1/\ln x \gg 1/x$ e per $x\to 1^+$ si ha
$1/\ln x \sim 1/(x-1)$. Per $p=1$ gli integrali
di $1/x$ a $+\infty$ e di $1/(x-1)$ in $1$
sono entrambi divergenti.

Se $f\colon [a,+\infty)$ è una funzione localmente Riemann-integrabile
e se $f(x)\to \ell \neq 0$ per $x\to +\infty$ allora l'integrale
di $f$ è divergente. Se $\ell>0$ esisterà $c>a$ tale che per $x>c$
si abbia $f(x)\ge 0$ (permanenza del segno). Visto che
$f(x)\sim \ell$ per $x\to +\infty$ possiamo quindi concludere
che $\int_c^{+\infty} f(x) = +\infty$ e
quindi anche
$\int_a^{+\infty} f(x) = +\infty$.
Se $\ell<0$ si può cambiare segno ad $f$ e ripetere l'argomento precedente
si scopre quindi che in questo caso l'integrale di $f$ è $-\infty$.
\end{example}

\begin{exercise}
Si mostri con un esempio che se una funzione $f$ ha integrale
convergente sull'intervallo $[0,+\infty)$ non
è detto che $f(x)\to 0$ per $x\to +\infty$.
\end{exercise}

\begin{comment}
\begin{exercise}[difficile]
Mostrare che l'integrale
\[
  \int_0^{+\infty} \exp\enclose{x^2 \ln \frac{2+\cos(2\pi x)}{3}} \, dx
\]
è finito. Ma la funzione integranda non tende a zero per $x\to +\infty$.
\end{exercise}
\end{comment}

\begin{theorem}[criterio di convergenza assoluta]
\label{th:convergenza_assoluta_integrale}
\mymark{**}%
Sia $f\colon (a,b)\to \RR$ una funzione localmente Riemann-integrabile.
Allora anche $\abs{f}$ è localmente Riemann-integrabile e se
\[
\int_a^b \abs{f(x)} < +\infty
\]
(cioè $\abs{f}$ è integrabile in senso improprio)
allora anche $f$ è integrabile in senso improprio e
\[
  \abs{\int_a^b f(x)\,dx}
  \le \int_a^b \abs{f(x)}\, dx < +\infty
\]
\end{theorem}
%
%
\begin{proof}
Poniamo $f = f^+ - f^-$ con $f^+\ge 0$ e $f^-\ge 0$.
Se $f$ è localmente Riemann-integrabile
$f^+$ e $f^-$ sono localmente Riemann-integrabili
(grazie al teorema~\ref{th:reticolo}).
Osserviamo che si ha $f = f^+ - f^-$
e $\abs{f} = f^+ + f^-$.
Visto che $0\le f^+ \le \abs{f}$ possiamo
applicare i criteri di confronto e
affermare che $f^+$ ha integrale convergente.
Lo stesso vale per $f^-$ e dunque per $f=f^+ - f^-$.
Inoltre
\[
  \abs{\int_a^b f }
  = \abs{\int_a^b f^+ - \int_a^\beta f^-}
  \le \int_a^b f^+ + \int_a^b f^-
  = \int_a^b \abs{f}.
\]
\end{proof}

\begin{definition}[convergenza assoluta]
\index{integrale!improprio!convergenza assoluta}%
\index{integrale!convergenza assoluta}%
\index{convergenza!assoluta!integrale}%
\index{integrabile!assolutamente}%
\index{assolutamente!integrabile}%
Quando
\[
  \int_a^b \abs{f(x)}\, dx <+\infty
\]
diremo che l'integrale di $f$ è \emph{assolutamente convergente}
o che $f$ è \emph{assolutamente integrabile} (in senso improprio).
Il teorema
precedente ci garantisce che una funzione assolutamente integrabile
è integrabile.
\end{definition}

\begin{example}
L'integrale
\[
  \int_0^{+\infty} \sin(x^2)\cdot e^{-x}\, dx
\]
è assolutamente convergente (e quindi convergente)
in quanto la funzione integranda è continua,
e si ha
\[
 \abs{\sin(x^2)\cdot e^{-x}} \le e^{-x}
\]
da cui, per confronto,
\[
 \int \abs{\sin(x^2)\cdot e^{-x}}\, dx  \le
  \int_0^{+\infty} e^{-x}\, dx < +\infty.
\]
\end{example}

\begin{example}[funzione integrabile ma non assolutamente]
\mymark{*}
Si consideri la funzione $f\colon (0,+\infty) \to \RR$
definita da
\[
  f(x) = \frac{\sin x }{x}.
\]
Visto che $f(x)\to 1$ per $x\to 0^+$ la funzione $f$
può essere estesa per continuità ad una funzione continua sull'intervallo
$[0,+\infty)$ ponendo $f(0)=1$.
Dunque l'integrale converge sull'intervallo $(0,1]$ e ci si
riduce a considerare l'intervallo $[1,+\infty)$.

Integrando per parti,
\begin{align*}
  \int_1^{+\infty} \frac{\sin x}{x}\, dx
  &= \Enclose{\frac{-\cos x}{x}}_1^{+\infty} -
  \int_1^{+\infty} \frac{\cos x}{x^2} \, dx \\
  &= \cos 1 - \int_1^{+\infty} \frac{\cos x}{x^2}\, dx.
\end{align*}
Osserviamo ora che la funzione $\frac{\cos x}{x^2}$ è integrabile
per il criterio della convergenza assoluta, in quanto:
\[
  \abs{\frac{\cos x}{x^2}} \le \frac{1}{x^2}
\]
che è integrabile su $[1,+\infty)$.
Dunque la nostra funzione ha integrale convergente anche in $[1,+\infty)$
e dunque ha integrale convergente su tutto $(0,+\infty)$.

Tale funzione non è però integrabile assolutamente.
Infatti si ha,
per ogni $k\in \NN$
\begin{align*}
  \int_{k\pi}^{(k+1)\pi} \abs{\frac{\sin x} x}\, dx
  &\ge \int_{k\pi}^{(k+1)\pi} \frac{\abs{\sin x}}{(k+1)\pi}\, dx\\
  &= \frac{\int_0^\pi \sin x \, dx}{(k+1)\pi}
  = \frac{2}{(k+1)\pi}
\end{align*}
da cui
\[
  \int_0^{+\infty} \abs{\frac{\sin x}{x}}\, dx
  \ge \sum_{k=0}^{+\infty} \frac{2}{(k+1)\pi} = +\infty.
\]
\end{example}

I criteri che abbiamo visto fin'ora mettono in evidenza una notevole
analogia tra serie e integrali. Determinare la convergenza
di un integrale è però, in generale, più semplice che determinare
la convergenza della corrispondente serie.
Nell'esercizio seguente, ad esempio, è sufficiente trovare una primitiva
delle funzioni integrande, per determinare il carattere dell'integrale.

\begin{exercise}
Si determini per quali $\alpha,\beta\in \RR$ risultano convergenti
gli integrali:
\[
  \int_0^{\frac 1 2} \frac{1}{x^\alpha \abs{\ln x}^\beta}\, dx,
  \qquad
  \int_2^{+\infty} \frac{1}{x^\alpha \ln^\beta x} \, dx.
\]
\end{exercise}

\begin{theorem}[collegamento tra serie ed integrali impropri]
\mymark{**}
Sia $f\colon$ $[1,+\infty)$ una funzione decrescente non negativa
e sia $a_k=f(k)$.
Allora la serie
\[
   \sum_{k=1}^{+\infty} a_k
\]
è convergente se e solo se è convergente l'integrale
\[
  \int_1^{+\infty} f(x)\, dx.
\]

E, più precisamente:
\begin{equation}\label{eq:39185}
  0
  \le \sum_{k=1}^{+\infty} a_k - \int_1^{+\infty}f(x)\, dx
  \le a_1.
\end{equation}

\end{theorem}
%
\begin{proof}
\mymark{**}
Visto che la funzione $f$, è non negativa e monotona,
l'integrale improprio di $f$ su $[0,+\infty)$ esiste (finito o infinito)
per il teorema~\ref{th:integrabilita_positive}.
Anche la serie $\sum f(k)$ essendo a termini non negativi risulta essere determinata.
Essendo $f$ decrescente, per ogni $x \in [k,k+1]$ si ha
\[
  f(k+1) \le f(x) \le f(k)
\]
integrando su $[k,k+1]$ si ottiene
\[
  f(k+1) \le \int_{k}^{k+1} f(x)\, dx \le f(k)
\]
e sommando per $k=1,\dots, N$ si ottiene:
\[
  \sum_{k=2}^{N+1} f(k) \le \int_{1}^{N+1} f(x)\, dx \le \sum_{k=1}^{N} f(k).
\]
Possiamo passare al limite per $N \to +\infty$ in quanto già sappiamo che
i limiti esistono:
\[
  \sum_{k=2}^{+\infty} f(k)
  \le \int_{1}^{+\infty} f(x)\, dx
  \le \sum_{k=1}^{+\infty} f(k).
\]
Queste ultime disuguaglianze sono equivalenti alle \eqref{eq:39185}
e ci dicono immediatamente che se l'integrale converge anche la serie
converge e viceversa.
\end{proof}

\begin{example}[stima asintotica di $\ln n!$]
\index{fattoriale!stima asintotica}
\mynote{stima asintotica $n"!$}
Osserviamo che
\[
  \ln (n!) = \ln \prod_{k=1}^n k = \sum_{k=1}^n \ln k.
\]
Applichiamo lo stesso procedimento utilizzato nel teorema precedente alla funzione $f(x) = \ln x$. Visto che il logaritmo è crescente (invece che decrescente) otterremo delle stime rovesciate, ma analoghe. Ripetendo con attenzione i conti, si ottiene:
\[
    \int_1^n \ln(x)\, dx \le  \sum_{k=2}^n \ln k = \sum_{k=1}^n \ln k \le \int_1^{n+1} \ln(x) \, dx
\]
e calcolando gli integrali: $\int \ln x = x \ln x -x$ si ottiene
\[
  n \ln n - n + 1 \le \ln(n!) \le (n+1) \ln (n+1) - n - \ln 2 +2
\]
da cui, dividendo ambo i membri per $n \ln n$ e facendo tendere $n\to +\infty$ si trova
\[
 \frac{\ln n!}{n \ln n}\to 1
\]
ovvero
\[
  \ln (n!) \sim n \ln n \qquad \text{per $n\to +\infty$.}
\]

Nella sezione seguente daremo la formula di Stirling: una stima asintotica più precisa (ma molto più complicata
da dimostrare) del fattoriale.
\end{example}

\section{alcune applicazioni del calcolo integrale}

\begin{example}[la funzione $\Gamma$ Eulero]
\index{$\Gamma$ funzione di Eulero}
\index{funzione!$\Gamma$ di Eulero}
Si definisce $\Gamma\colon (0,+\infty) \to \RR$ come
\[
  \Gamma(x) = \int_0^{+\infty} e^{-t} t^{x-1}\, dt.
\]
L'integrale converge per ogni $x>0$ in quanto posto
$f(t) = e^{-t} t^{x-1}$ per $t\to 0^+$ si ha $f(t) \sim t^{x-1}$ che ha integrale convergente in un intorno di $0$ mentre per $t\to +\infty$ si ha $f(t) \ll e^{-t/2}$ che ha integrale convergente in un intorno di $+\infty$.

Integrando per parti si ottiene una interessante proprietà della funzione $\Gamma$:
\begin{align*}
\int_0^{+\infty} e^{-t}t^{x}\, dt
&= \Enclose{-e^{-t}t^{x}}_0^{+\infty}
+ \int_0^{+\infty} e^{-t}xt^{x-1}\, dt\\
&= x\int_0^{+\infty} e^{-t}t^{x-1}\, dt
\end{align*}
cioè $\Gamma(x+1) = x\Gamma(x)$.
Osservando che $\Gamma(1)=0$ si può quindi dimostrare, per induzione,
che $\Gamma(n+1) = n!$ per ogni $n\in \NN$.
Abbiamo quindi trovato una funzione che estende il fattoriale
da $\NN$ a tutto l'intervallo di numeri reali $(-1,+\infty)$.
\end{example}

\begin{theorem}[irrazionalità di $\pi$]
\mynote{$\pi$ è irrazionale}
\index{$\pi$!irrazionalità}
\index{irrazionalità!di $\pi$}
Il numero $\pi$ è irrazionale.
\end{theorem}
%
\begin{proof}
Posto
\[
 f_n(x) = x^n(\pi-x)^n
\]
si ha
\[
f_n'(x) = n x^{n-1}(\pi-x)^n - n x^n (\pi-x)^{n-1}
\]
e
\begin{align*}
f_n''(x)
&= n(n-1) x^{n-2}(\pi-x)^n
  - 2n^2 x^{n-1}(\pi-x)^{n-1} \\
  &\quad + n(n-1) x^n(\pi-x)^{n-2} \\
&= (n^2-n) [(\pi-x)^2 +x^2] x^{n-2}(\pi-x)^{n-2} \\
  &\quad  - 2n^2 x^{n-1}(\pi-x)^{n-1}
\end{align*}
Osservando ora che
\[
 (\pi-x)^2 + x^2 = \pi^2 -2\pi x + 2x^2
  = \pi^2 - 2x(\pi-x)
\]
si ottiene
\begin{align*}
f_n''(x)
&= (n^2-n)\pi^2 x^{n-2}(\pi-x)^{n-2} \\
  &\quad - 2(n^2 - n + n^2) x^{n-1}(\pi-x)^{n-1} \\
&= (n^2-n)\pi^2 f_{n-2}(x)
-  (4n^2-2n) f_{n-1}(x).
\end{align*}

Poniamo ora
\[
  I_n = \int_0^\pi f_n(x) \sin(x)\, dx.
\]
Per $n>0$ osserviamo che $f_n(0) = f_n(\pi)=0$
e dunque integrando per parti due volte si ottiene:
\[
  I_n = \int_0^\pi f_n'(x) \cos x\, dx
   = -\int_0^\pi f_n''(x)\sin x\, dx
\]
da cui
\begin{align}\label{eq:9530978}
   I_n = -(n^2-n)\pi^2 I_{n-2} + (4n^2-2n)I_{n-1}
\end{align}
Supponiamo ora per assurdo che sia $\pi = p/q$ con $p,q\in \NN$ e consideriamo
la successione
\[
   a_n = \frac{q^{2n}}{n!} I_n.
\]
Possiamo calcolare i primi due termini della successione $a_n$:
\begin{align*}
  a_0 &= \frac{q^0}{0!}\int_0^\pi \sin x\, dx
    = 2 \in \ZZ, \\
  a_1 &= \frac{q^2}{1!}\int_0^\pi x(\pi -x)\sin x\, dx \\
   &= q^2 \int_0^\pi (2x - \pi)\cos x\, dx \\
   &= 2q^2 \int_0^\pi \sin x\, dx
   = 4q^2 \in \ZZ.
\end{align*}
Inoltre la relazione di ricorrenza~\eqref{eq:9530978}
si traduce in:
\begin{align*}
  a_n &= \frac{q^{2n}}{n!}\enclose{-(n^2-n)\frac{p^2}{q^2} \frac{(n-2)!}{q^{2n-4}}a_{n-2}
  + (4n^2-2n) \frac{(n-1)!}{q^{2n-2}}a_{n-1}} \\
  &= - q^2 p^2 a_{n-2} + (4n-2)q^2 a_{n-1}.
\end{align*}
Per induzione si trova quindi che $a_n \in \ZZ$ per ogni $n\in \NN$.

D'altra parte osservando che per $x\in [0,\pi]$ si ha
\[
  0 \le f_n(x) \sin(x) \le x^n (\pi-x)^n \le \pi^n \pi^n = \pi^{2n}
\]
dunque
\[
  0\le I_n = \int_0^\pi f_n(x) \sin x\, dx \le \pi^{2n+1}
\]
cioè
\[
  0 \le a_n \le \frac{q^{2n}}{n!}\pi^{2n+1} \to 0.
\]
Dunque abbiamo scoperto che $a_n\to 0$. D'altra parte abbiamo visto che
$a_n \in \ZZ$ e certamente $a_n > 0$ in quanto $I_n \neq 0$ visto che $f_n$ è
una funzione continua, non negativa e non identicamente nulla. Ma non è
possibile che una successione di numeri interi positivi converga a zero:
abbiamo quindi ottenuto l'assurdo.
\end{proof}

\begin{theorem}[prodotto di Wallis]
\mymark{*}
\mynote{prodotto di Wallis}
\index{$\pi$!prodotto di Wallis}
\index{Wallis!approssimazione di $\pi$}
\index{formula!di Wallis}
Si ha
\[
  \frac{\pi}{2}
  = \prod_{k=1}^{+\infty} \frac{(2k)^2}{(2k-1)(2k+1)}
  = \frac{2}{1}
  \cdot \frac{2}{3}
  \cdot \frac{4}{3}
  \cdot \frac{4}{5}
  \cdot \frac{6}{5}
  \cdot \frac{6}{7}
  \cdot \frac{8}{7}
  \cdot \frac{8}{9}
  \dots
\]
E si ottiene di conseguenza la seguente stima asintotica per
il coefficiente \myemph{binomiale centrale}
\[
 {2n \choose n} \sim \frac{4^n}{\sqrt{\pi n}}
 \qquad \text{per $n\to +\infty$.}
\]
\end{theorem}
%
\begin{proof}
Consideriamo la successione di integrali:
\[
  I_n = \int_0^\pi \sin^n(x)\, dx.
\]
Essendo $0\le \sin^n(x) \le 1$ per ogni $x\in [0,\pi]$ ed essendo
$\sin^{n+1}(x)\le \sin^n(x)$ per ogni $x\in[0,\pi]$ è chiaro che $I_n$ è una
successione decrescente di numeri positivi.

Da un calcolo diretto troviamo che
\[
  I_0 = \int_0^\pi 1\, dx = \pi, \qquad
  I_1 = \int_0^\pi \sin(x)\, dx = \Enclose {-\cos x}_0^\pi = 2.
\]
Se $n\ge 0$,
integrando per parti troviamo invece una relazione ricorsiva
\begin{align*}
 I_{n+2} &= \int_0^\pi \sin^{n+1}(x) \sin(x)\, dx \\
     &= \Enclose{-\sin^{n+1}(x)\cos(x)}_0^\pi + (n+1)\int_0^\pi \sin^{n} \cos^2(x)\, dx \\
     &= 0 + (n+1)\int_0^\pi\sin^{n}(1-\sin^2(x))\, dx \\
     &= (n+1) I_{n} - (n+1) I_{n+2}
\end{align*}
da cui
\[
  I_{n+2} = \frac{n+1}{n+2} I_{n}.
\]
Ricordando che $I_n$ è decrescente si ottiene (mettendo $2n$ al posto di $n$)
\[
  \frac{2n+1}{2n+2} \cdot I_{2n}
  = I_{2n+2}
  \le I_{2n+1}
  \le I_{2n}
\]
Cioè
\[
  1
  \le \frac{I_{2n}}{I_{2n+1}}
  \le \frac{2n+2}{2n+1} \to 1
\]
da cui ottieniamo che $I_{2n+1} / I_{2n} \to 1$.

Ma la formula ricorsiva ci permette di calcolare separatamente
i termini pari e dispari della successione:
\begin{align*}
  I_{2n} &= \frac{2n-1}{2n}\cdot \frac{2n-3}{2n-2} \cdots \frac {3}{4}\cdot \frac{1}{2} \cdot I_0 = \enclose{\prod_{k=1}^n \frac{2k-1}{2k}} \cdot \pi \\
  I_{2n+1} &= \frac{2n}{2n+1}\cdot \frac {2n-2}{2n-1} \cdots
  \frac{4}{5}\cdot \frac{2}{3} \cdot I_1
  = \enclose{\prod_{k=1}^n \frac{2k}{2k+1}}\cdot 2.
\end{align*}
In conclusione, visto che $\pi$ compare nella formula dei termini pari, ma
non in quella dei termini dispari, e visto che le due espressioni sono
asintotiche possiamo ottenere
la formula per il calcolo di $\pi$:
\begin{align*}
\frac{\pi}{2}
&= \frac{\pi}{2} \lim_{n\to +\infty} \frac{I_{2n+1}}{I_{2n}}
= \lim_{n\to +\infty} \frac{\displaystyle\prod_{k=1}^n \frac{2k}{2k+1}}{\displaystyle\prod_{k=1}^n \frac{2k-1}{2k}}\\
&= \lim_{n\to +\infty} \prod_{k=1}^{n} \frac{(2k)^2}{(2k-1)(2k+1)}.
\end{align*}

Per ottenere una stima sul binomiale centrale cerchiamo di riscrivere le
formule precedenti tramite il fattoriale.
Denotiamo con $n!!$ (doppio fattoriale) il prodotto dei numeri fino a $n$ e
con la stessa parità di $n$ (cioè un numero ogni due):
\[
\begin{cases}
  0!! = 1\\
  1!! = 1\\
  (n+2)!! = (n+2) \cdot n!!
\end{cases}
\]
Più esplicitamente possiamo scrivere il doppio fattoriale
distinguendo tra i pari i dispari:
\begin{align*}
  (2n)!! &= (2n) \cdot (2n-2) \cdot (2n-4) \dots 4 \cdot 2 = \prod_{k=1}^n 2k, \\
  (2n+1)!! &= (2n+1)\cdot(2n-1)\cdot(2n-3)\dots 4\cdot 3
   = \prod_{k=1}^n (2k+1).
\end{align*}
Osserviamo dunque che
\[
  (2n)!! = \prod_{k=1}^n (2k) = 2^n n!
\]
ma anche
\[
  (2n)! = (2n)!! (2n-1)!!
\]
Dunque la stima asintotica di Wallis si può scrivere come
\begin{align*}
 \sqrt{\frac{\pi}{2}} &\sim \sqrt{\frac{((2n)!!)^2}{(2n-1)!!(2n+1)!!}}
 = \frac{1}{\sqrt{2n+1}} \frac{(2n)!!}{(2n-1)!!} \\
 &= \frac{1}{\sqrt{2n+1}}\frac{(2n)!!(2n)!!}{(2n)!}
 = \frac{1}{\sqrt{2n+1}}\frac{(2^n)^2 (n!)^2}{(2n)!}
\end{align*}
da cui
\[
{2n \choose n} = \frac{(2n)!}{(n!)^2}
\sim \frac{4^n}{\sqrt{2n+1}}\sqrt{\frac 2 \pi}
\sim \frac{4^n}{\sqrt{2n}}\sqrt{\frac 2 \pi}
 = \frac{4^n}{\sqrt{\pi n}}.
\]
\end{proof}

\begin{theorem}[formula di Stirling]
\mymark{*}
\mynote{formula di Stirling}
\index{formula!di Stirling}
\index{Stirling formula di}
\index{fattoriale!formula di Stirling}
Si ha
\[
  n! \sim \sqrt{2\pi n}\cdot \frac{n^n}{e^n}
  \qquad \text{per $n\to +\infty$.}
\]
\end{theorem}
%
\begin{proof}
Osserviamo in generale che se $f:[a,b]\to \RR$ è una funzione concava allora il grafico di $f$ è compreso tra la retta passante per gli estremi $(a,f(a))$, $(b,f(b))$ (retta secante) e una qualunque retta tangente, ad esempio la retta tangente in $((a+b)/2,f((a+b)/2))$.
Di conseguenza l'area sotto il grafico, cioè $\int_a^b f(x)\, dx$ è compreso tra le aree dei due corrispondenti trapezi rettangoli. L'altezza di entrambi i trapezi è pari a $(b-a)$ e l'area si calcola moltiplicando l'altezza per la media delle basi. Nel caso del trapezio con lato obliquo la secante la media delle basi è $(f(a)+f(b))/2$, nel caso del trapezio con lato obliquo sulla retta tangente nel punto medio, la media delle basi è uguale alla sezione nel punto medio: $f((a+b)/2)$. Si ottiene dunque,
per ogni $f$ concava,
\[
   (b-a)\cdot  \frac{f(a) + f(b)}{2}
   \le \int_a^b f(x) \, dx
   \le (b-a) \cdot f\enclose{\frac{a+b}{2}}.
\]

Applicando queste stime alla funzione $f(x) = \ln x$ nell'intervallo $[k, k+1]$ si ottiene:
\[
\frac{\ln(k) + \ln(k+1)}{2}
\le \int_k^{k+1} \ln x\, dx
\le \ln \enclose{k+\frac 1 2}.
\]
Chiamiamo $a_k$ la differenza tra le prime due quantità. Si ha quindi:
\begin{align*}
a_k
 &= \int_k^{k+1} \ln x\, dx - \frac{\ln (k) + \ln(k+1)}{2}\\
 & \le \ln\enclose{k+\frac 1 2} - \frac{\ln(k) + \ln(k+1)}{2} \\
 & = \frac{1}{2}\Enclose{\enclose{\ln\enclose{k+\frac 1 2 } - \ln k}
 -\enclose{\ln(k+1)-\ln\enclose{k+\frac 1 2}}} \\
 &= \frac 1 2 \Enclose{\ln \enclose{1+\frac{1}{2k}}
 - \ln\enclose{1+\frac{1}{2k+1}}} \\
 &< \frac{1}{2}\Enclose{\ln \enclose{1+\frac{1}{2k}}
 - \ln \enclose{1+\frac{1}{2k+2}}}.
\end{align*}
Sommando per $k=1,\dots, n-1$ si osserva che l'ultima espressione che abbiamo scritto è telescopica. Dunque si ha:
\begin{align*}
 A_n = \sum_{k=1}^{n-1} a_k
 &<\frac 1 2 \sum_{k=1}^{n-1}\Enclose{\ln \enclose{1+\frac{1}{2k}}
 - \ln \enclose{1+\frac{1}{2k+2}}} \\
 & = \frac{1}{2}\Enclose{\ln \enclose{1+\frac{1}{2}}
 - \ln \enclose{1+\frac{1}{2n}}} \le \frac{1}{2}\ln\enclose{\frac 3 2}.
\end{align*}
Visto che $a_k > 0$ è chiaro che $A_n$ è una successione crescente a termini positivi. Dunque $A_n$ ammette limite $A_n\to \ell$ e visto che abbiamo appena mostrato che $A_n$ è limitata, sappiamo che $\ell<+\infty$.

Esplicitiamo ora $A_n$.
\begin{align*}
A_n
&= \sum_{k=1}^{n-1} \int_{k}^{k+1} \ln x\, dx -
\sum_{k=1}^{n-1} \frac{\ln k + \ln(k+1)}{2} \\
&= \int_{1}^{n} \ln x\, dx - \sum_{k=2}^{n-1} \ln k - \frac{\ln 1}{2}
-\frac{\ln(n)}{2} \\
&= \Enclose{ x \ln x -x}_1^n
- \sum_{k=1}^n \ln k + \frac{\ln n}{2} \\
&= n \ln n - n - 0 + 1 - \ln(n!) + \frac {\ln n}{2}
 \end{align*}
Da cui
\[
e^{A_n} = \frac{n^n\cdot e \sqrt{n}}{e^n \cdot n!} \to e^\ell
\]
ovvero, posto $c=e/e^\ell$,
\[
  n! \sim c\cdot\frac{n^n \sqrt{n}}{e^n}.
\]
Per determinare $c$ possiamo sfruttare la formula di Wallis
sul coefficiente binomiale centrale:
\begin{align*}
\frac{4^n}{\sqrt{\pi n}}
 &\sim
{2n \choose n}
= \frac{(2n)!}{(n!)^2}\\
&\sim \frac{c \frac{(2n)^{2n}\sqrt{2n}}{e^{2n}}}
{\enclose{c\frac{n^n\sqrt n}{e^n}}^2}
= \frac{2^{2n}\sqrt{2}}{c\sqrt{n}}
\end{align*}
da cui si ottiene
\[
  \frac{1}{\sqrt \pi} \sim \frac{\sqrt 2}{c}
\]
e quindi $c=\sqrt{2\pi}$.
\end{proof}
